<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AIGC on 基于LLM的系统设计与实现</title>
    <link>https://www6v.github.io/www6vAIGC/categories/AIGC/</link>
    <description>Recent content in AIGC on 基于LLM的系统设计与实现</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Tue, 16 Apr 2024 17:17:32 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vAIGC/categories/AIGC/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(原理&amp;实战)Deep Research</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Deep-Research/%E5%AE%9E%E7%8E%B0/DeepResearch/</link>
      <pubDate>Tue, 26 Mar 2024 12:13:50 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Deep-Research/%E5%AE%9E%E7%8E%B0/DeepResearch/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;deep-research&#34;&gt;&#xA;  Deep Research&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#deep-research&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/ebd/1c2bfe21108480ab8b80f0638d4ec2af&#34;&gt;(原理&amp;amp;实战)Deep Research&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(最佳实践)SFT  &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/%E5%AE%9E%E8%B7%B5/FineTuningBestPractice/</link>
      <pubDate>Sat, 23 Dec 2023 11:11:20 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/%E5%AE%9E%E8%B7%B5/FineTuningBestPractice/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;sft-bestpractice&#34;&gt;&#xA;  SFT BestPractice&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#sft-bestpractice&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/SFT-158bfe2110848004a0d7dc9be4e2e0ac?pvs=4&#34;&gt;SFT BestPractice&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理) RAG Pattern *</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/RAGPattern/</link>
      <pubDate>Fri, 22 Dec 2023 17:34:50 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/RAGPattern/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;7-种-rag-模式&#34;&gt;&#xA;  7 种 RAG 模式&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#7-%e7%a7%8d-rag-%e6%a8%a1%e5%bc%8f&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Naive RAG 是最基础的架构，包含简单的文档检索、处理和生成响应的流程&lt;/li&gt;&#xA;&lt;li&gt;Retrieve-and-rerank 在基础 RAG 上增加了重排序步骤，可以优化检索结果的相关性&lt;/li&gt;&#xA;&lt;li&gt;Multimodal RAG 能够处理图像等多种类型的数据，不仅限于文本&lt;/li&gt;&#xA;&lt;li&gt;Graph RAG 利用图数据库增强知识连接，可以更好地理解文档间的关系&lt;/li&gt;&#xA;&lt;li&gt;Hybrid RAG 结合了多种技术的优势，包含图结构和传统检索方法&lt;/li&gt;&#xA;&lt;li&gt;Agentic RAG Router 使用 AI Agent 来路由和处理查询，可以选择最适合的处理路径&lt;/li&gt;&#xA;&lt;li&gt;Agentic RAG Multi-Agent 使用多个专门的 AI Agent 协同工作，可以调用不同的工具（如向量搜索、网页搜索、Slack、Gmail 等）&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/v16n31qs.bmp&#34; alt=&#34;v16n31qs.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;核心组件&#34;&gt;&#xA;  核心组件&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;嵌入模型：将文本转换为向量表示&lt;/li&gt;&#xA;&lt;li&gt;生成模型：负责最终的内容生成&lt;/li&gt;&#xA;&lt;li&gt;重排序模型：优化检索结果的相关性&lt;/li&gt;&#xA;&lt;li&gt;向量数据库：存储和检索向量化的内容&lt;/li&gt;&#xA;&lt;li&gt;提示模板：规范化的查询处理模板&lt;/li&gt;&#xA;&lt;li&gt;AI Agent：智能决策和任务协调&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/v8Dd4mQmaemxpOyrrI7o6g&#34;&gt;RAG 架构图解：从基础到高级的7种模式&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/shao__meng/status/1866626166079230355?s=46&amp;amp;t=1AUvwyftFbcog4yHzgnysw&#34;&gt;RAG 架构图解：从基础到高级的7种模式&lt;/a&gt;   x&lt;/p&gt;</description>
    </item>
    <item>
      <title>GraphRAG &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/KG-RAG/graphRAG/</link>
      <pubDate>Fri, 22 Dec 2023 17:34:50 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/KG-RAG/graphRAG/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;graphrag&#34;&gt;&#xA;  GraphRAG&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#graphrag&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/GrpahRAG-210bfe211084800d978cdcebbfc1f337?source=copy_link&#34;&gt;GraphRAG&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)Agentic RAG &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/Agentic-RAG/AgenticRAG/</link>
      <pubDate>Sun, 25 Jun 2023 10:39:17 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/Agentic-RAG/AgenticRAG/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;agentic-rag&#34;&gt;&#xA;  Agentic RAG&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agentic-rag&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Agentic-RAG-1d5bfe211084804b95a6e661868813af?pvs=4&#34;&gt;(原理|实战)Agentic RAG&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理) Index &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGIndex/RAGIndex/</link>
      <pubDate>Sun, 21 May 2023 17:09:42 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGIndex/RAGIndex/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;index&#34;&gt;&#xA;  Index&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#index&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Index-1f0bfe21108480d19597f733f0f3f518?pvs=4&#34;&gt; (原理) Index&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战) Chunk &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGIndex/RAGChunk/</link>
      <pubDate>Sun, 21 May 2023 17:09:42 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGIndex/RAGChunk/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;chunk&#34;&gt;&#xA;  Chunk&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#chunk&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Index-Chunk-109bfe21108480558752d5f4e9a72dd6?pvs=4&#34;&gt; (原理|实战) Chunk&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Agent Planning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Planning/AgentPlanning/</link>
      <pubDate>Sat, 13 May 2023 06:57:12 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Planning/AgentPlanning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;types1&#34;&gt;&#xA;  Types[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#types1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/plans.webp&#34; alt=&#34;plans&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;任务分解&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;多计划选择&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;外部规划器辅助规划&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;反思和提炼&lt;/strong&gt;[20]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;记忆增强规划&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;任务分解&#34;&gt;&#xA;  任务分解&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bb%bb%e5%8a%a1%e5%88%86%e8%a7%a3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;ReACT 范式 [2]&#xA;把&lt;strong&gt;融合了Reasoning和Acting&lt;/strong&gt;的一种范式，推理过程是浅显易懂，仅仅&lt;strong&gt;包含thought-action-observation步骤&lt;/strong&gt;，很容易判断推理的过程的正确性，使用ReAct做决策甚至超过了强化学习.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;chain-of-thought推理-问题&#xA;事实幻想（fact hallucination）和错误传递（error propagation）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Plan-and-execute agents [2]&#xA;本质上是先计划再执行，即先把用户的问题分解成一个个的子任务，然后再执行各个子任务，最后合并输出得到结果&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;patterns&#34;&gt;&#xA;  Patterns&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#patterns&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Self-ask [2]&#xA;Self-ask是一种follow-up的使用范式，仅仅包含follow-up, immediate answer步骤，至于follow-up多少个step，完全由它自己决定，估计这就是Self-ask的名字的由来。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;《Understanding the planning of LLM agents: A survey》&lt;br&gt;&#xA;&lt;a href=&#34;https://mp.weixin.qq.com/s/1POXDVJDv3ob1HqpKjb3Mg&#34;&gt;大语言模型智能体规划能力综述: 分类、任务分解、选择、反思、记忆增强 &lt;/a&gt; 翻译&lt;br&gt;&#xA;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/693264551&#34;&gt;Agent四大范式 | 综述：全面理解Agent工作原理&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/642357544&#34;&gt;2023年新生代大模型Agents技术,ReAct,Self-Ask,Plan-and-execute,以及AutoGPT, HuggingGPT等应用&lt;/a&gt; ***  论文+代码&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;{% post_link &amp;lsquo;gptAgentReflection&amp;rsquo; %} self&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s/NhpJMmIcnF57qEuUkxD4kQ&#34;&gt;AI Agent规划能力全面拆解&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://baoyu.io/translations/ai-paper/2311.11797-igniting-language-intelligence-the-hitchhikers-guide-from-chain-of-thought-reasoning-to-language-agents&#34;&gt;引领语言智能：从思维链推理到语言智能体的探索指南 [译]&lt;/a&gt; paper&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=Mzg5NTc2OTcyOQ==&amp;amp;mid=2247488040&amp;amp;idx=1&amp;amp;sn=f404a5fc2b0380eac00564046abc77d5&#34;&gt;2023年大语言模型智能体规划技术(LLM Agent Planning)研究进展汇总&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG Framework</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/framework/RAGFramework/</link>
      <pubDate>Tue, 09 May 2023 16:34:11 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/framework/RAGFramework/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;框架-0&#34;&gt;&#xA;  框架 [0]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%a1%86%e6%9e%b6-0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/infiniflow/ragflow/tree/main&#34;&gt;&lt;strong&gt;ragflow&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/tree/master&#34;&gt;&lt;strong&gt;QAnything&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/chatchat-space/Langchain-Chatchat/releases/tag/v0.2.8&#34;&gt;&lt;strong&gt;langchainchat&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/labring/FastGPT&#34;&gt;&lt;strong&gt;FastGPT&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain/&#34;&gt;&lt;strong&gt;LangChain&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/run-llama/llama_index/&#34;&gt;&lt;strong&gt;LlamaIndex&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/langchain4j/langchain4j&#34;&gt;langchain4j&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Azure/GPT-RAG&#34;&gt;GPT-RAG&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Unstructured-IO/unstructured&#34;&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/StanGirard/quivr&#34;&gt;Quivr&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/langgenius/dify&#34;&gt;&lt;strong&gt;Dify&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/weaviate/Verba&#34;&gt;Verba&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/danswer-ai/danswer&#34;&gt;danswer&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol start=&#34;0&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648407281&amp;amp;idx=2&amp;amp;sn=f39b46cad1787123b485d76dff33bc93&#34;&gt;大模型RAG问答研发真实图鉴：一周出Demo，半年用不好，缝补之路漫漫 &lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s/ZoI4Dscm9f9m5-q4Dq4bag&#34;&gt;大模型RAG问答开源框架的两个风向:兼看大模型安全的学术评测&lt;/a&gt;&#xA;RAGFlow - 引入文档理解及溯源机制&#xA;QAnything - 优化embeddding+召回侧方向的&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://llamahub.ai/&#34;&gt;LlamaHub&lt;/a&gt;&lt;br&gt;&#xA;Mix and match our Data Loaders and Agent Tools to build custom RAG apps or use our LlamaPacks as a starting point for your retrieval use cases.&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s/XRfSqYwvuGB6sDJzRm0QVA&#34;&gt;FlashRAG：可能是最全的、最快搭建RAG的开源框架 &lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)Query Rewrite</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPreRetrieval/QueryRewrite/</link>
      <pubDate>Thu, 20 Apr 2023 22:51:05 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPreRetrieval/QueryRewrite/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;query-rewrite&#34;&gt;&#xA;  Query rewrite&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#query-rewrite&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;query-rewrite-12&#34;&gt;&#xA;  query rewrite [1][2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#query-rewrite-12&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.14283.pdf&#34;&gt;论文&lt;/a&gt;&lt;strong&gt;使用LLM重写用户查询&lt;/strong&gt;，而不是直接使用原始用户查询进行检索。&#xA;因为对于LLM 而言，&lt;strong&gt;原始查询不可能总是最佳检索结果&lt;/strong&gt;，可以让LLM重写查询。&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain/blob/master/cookbook/rewrite.ipynb&#34;&gt;Repo&lt;/a&gt; git&#xA;【问题的多样化】&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;transformation-多样性&#34;&gt;&#xA;  Transformation-多样性&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#transformation-%e5%a4%9a%e6%a0%b7%e6%80%a7&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;step-back&#34;&gt;&#xA;  Step Back&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#step-back&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;step-back问答回退策略-3&#34;&gt;&#xA;  Step Back问答回退策略 [3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#step-back%e9%97%ae%e7%ad%94%e5%9b%9e%e9%80%80%e7%ad%96%e7%95%a5-3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Step Back问答回退，首先提示LLM提出一个&lt;strong&gt;关于高级概念或原则的通用后退问题&lt;/strong&gt;，并检索有关它们的相关事实，使用此基础来帮助回答用户问题。&lt;/p&gt;&#xA;&lt;h3 id=&#34;step-back-prompting-12&#34;&gt;&#xA;  Step-back Prompting [1][2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#step-back-prompting-12&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.06117.pdf&#34;&gt;论文&lt;/a&gt;使用退一步提示，&lt;strong&gt;使用LLM生成&amp;quot;后退&amp;quot;(Step back prompting)问题&lt;/strong&gt;。&#xA;使用检索时，&amp;ldquo;后退&amp;quot;问题和原始问题都会被用来进行检索，然后这两个结果都会被用来作为语言模型回复的基础。&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain/blob/master/cookbook/stepback-qa.ipynb&#34;&gt;Repo&lt;/a&gt; git&#xA;【问题的抽象化】&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;transformation-抽象化&#34;&gt;&#xA;  Transformation-抽象化&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#transformation-%e6%8a%bd%e8%b1%a1%e5%8c%96&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;hyde&#34;&gt;&#xA;  HyDE&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#hyde&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;hyde混合策略3&#34;&gt;&#xA;  HyDE混合策略[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#hyde%e6%b7%b7%e5%90%88%e7%ad%96%e7%95%a53&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;LLM将&lt;strong&gt;问题&lt;/strong&gt;转换为回答问题的&lt;strong&gt;假设文档&lt;/strong&gt;。&lt;strong&gt;使用嵌入的假设文档检索真实文档&lt;/strong&gt;，前提是doc-doc相似性搜索可以产生更多相关匹配。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;HyDE&#xA;At a high level, HyDE is an embedding technique that takes queries, &lt;strong&gt;generates a hypothetical answer&lt;/strong&gt;, and then embeds that generated document and uses that as the final example.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;transformation-具体化&#34;&gt;&#xA;  Transformation-具体化&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#transformation-%e5%85%b7%e4%bd%93%e5%8c%96&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648406156&amp;amp;idx=1&amp;amp;sn=d91a4df105c4fc4c9523f7141bc1c24d&#34;&gt;知识图谱用于细粒度大模型幻觉评估：兼论Langchain-RAG问答中的问题改写范式 &lt;/a&gt;&#xA;RAG:  rewrite , Step back, fusion&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)Query Transformation</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPreRetrieval/QueryTransformation/</link>
      <pubDate>Thu, 20 Apr 2023 22:51:05 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPreRetrieval/QueryTransformation/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;query-transformation&#34;&gt;&#xA;  Query Transformation&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#query-transformation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;multi-query多查询策略3&#34;&gt;&#xA;  Multi Query多查询策略[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#multi-query%e5%a4%9a%e6%9f%a5%e8%af%a2%e7%ad%96%e7%95%a53&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;该方法&lt;strong&gt;从多个角度重写用户问题&lt;/strong&gt;，为每个重写的问题检索文档，返回所有查询的唯一文档。&lt;/p&gt;&#xA;&lt;h3 id=&#34;decomposition问题分解策略3&#34;&gt;&#xA;  Decomposition问题分解策略[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#decomposition%e9%97%ae%e9%a2%98%e5%88%86%e8%a7%a3%e7%ad%96%e7%95%a53&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Answer recursively迭代式回答&#xA;在问题分解的基础上，逐步迭代出答案，&lt;strong&gt;将上一步问题的答案，与下一步骤的答案进行拼接&lt;/strong&gt;，送入大模型进行问答&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Answer individually&#xA;也可以&lt;strong&gt;让每个subquery分别进行处理&lt;/strong&gt;，然后得到答案，然后再拼接成一个QA pairspprompt最终形成答案。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;xxx&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;xxx&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/pK2BRLrWpEKKIPFhUtGvcg&#34;&gt;一文详看Langchain框架中的RAG多阶段优化策略：从问题转换到查询路由再到生成优化&lt;/a&gt; ***   原理paper，代码示例&lt;br&gt;&#xA;[Multi Query多查询策略， Decomposition问题]， RAG-Fusion， Step Back， HyDE混合&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/langchain-ai/rag-from-scratch&#34;&gt;rag-from-scratch Repo&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(Survey)多模态 RAG &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/Multimodal-RAG/RAGMultimodal/</link>
      <pubDate>Tue, 14 Mar 2023 13:55:59 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/Multimodal-RAG/RAGMultimodal/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;多模态-rag&#34;&gt;&#xA;  多模态 RAG&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%9a%e6%a8%a1%e6%80%81-rag&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/A-Survey-on-Multimodal-Retrieval-Augmented-Generation-1ebbfe211084808ab147d95e07c9cfd4?pvs=4&#34;&gt;(Survey)多模态 RAG&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)SELF-INSTRUCT&#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/Data-Quality/Instruction-Diversity/SelfInstruct/</link>
      <pubDate>Tue, 21 Feb 2023 14:21:23 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/Data-Quality/Instruction-Diversity/SelfInstruct/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;self-instruct&#34;&gt;&#xA;  SELF-INSTRUCT&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#self-instruct&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/SELF-INSTRUCT-10dbfe21108480adb3c9c6b4d13b57d0?pvs=4&#34;&gt;(原理)SELF-INSTRUCT&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Multi-Agents &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Multi-agent/MultiAgents/</link>
      <pubDate>Sat, 21 Jan 2023 19:34:23 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Multi-agent/MultiAgents/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;multi-agents-原理&#34;&gt;&#xA;  Multi-Agents 原理&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#multi-agents-%e5%8e%9f%e7%90%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Multi-Agents-10dbfe211084801f882dd0fe42d93eef?pvs=4&#34;&gt;(原理)Multi-Agents&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Instruct Tuning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/Instruct-Tuning/InstructTuning/</link>
      <pubDate>Fri, 06 Jan 2023 19:09:20 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/Instruct-Tuning/InstructTuning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;in-context-learning--icl--上下文学习&#34;&gt;&#xA;  In Context Learning ( ICL ) 上下文学习&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#in-context-learning--icl--%e4%b8%8a%e4%b8%8b%e6%96%87%e5%ad%a6%e4%b9%a0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/ICL.webp&#34; alt=&#34;ICL&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;in context learning&lt;/strong&gt;，大意是在&lt;strong&gt;prompt learning的基础上，将少量有标签样本融入prompt&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;上图的ICL模型可以理解成&lt;strong&gt;有监督、无训练&lt;/strong&gt;的&lt;strong&gt;小样本学习&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;但&lt;strong&gt;并非所有ICL都不训练&lt;/strong&gt;。比如下图右上角的&lt;strong&gt;FLAN&lt;/strong&gt;就是用instruction tuning&lt;strong&gt;训练参数&lt;/strong&gt;的。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/ICL-tech.webp&#34; alt=&#34;ICL-tech&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;FLAN&lt;/strong&gt;，&lt;strong&gt;既属于 in context learning，也属于 instruction learning&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;instruction-learning-1&#34;&gt;&#xA;  Instruction Learning [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#instruction-learning-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;instruct-tuning-&#34;&gt;&#xA;  Instruct Tuning-&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#instruct-tuning-&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;pre&gt;&lt;code&gt;FLANv1, FLANv2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h3 id=&#34;instructgpt&#34;&gt;&#xA;  instructGPT&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#instructgpt&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h3 id=&#34;chatgpt&#34;&gt;&#xA;  chatGPT&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#chatgpt&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;instruction-tuning&#34;&gt;&#xA;  Instruction Tuning&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#instruction-tuning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/instructTuning.webp&#34; alt=&#34;instructTuning&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;对于已有的预训练模型，继续在多项任务（B、C、D等）上做训练，在其他任务（A）上做预测。&lt;strong&gt;虽然依然没见过任务A，但是根据对B、C、D等的训练，对A的效果有所提升；&lt;/strong&gt; [1]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Instruct Tuning 本质上也是Prompt Tuning&lt;/strong&gt; [2]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;研究了缩放对指令微调的影响 [3]&#xA;与微调指令的任务数量有关，&lt;strong&gt;任务数量越多效果越好&lt;/strong&gt;&#xA;与模型的大小有关，&lt;strong&gt;模型越大效果越好&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Prompt vs. Instruction Tuning  [4]&#xA;Prompt是去激发语言模型的&lt;strong&gt;补全能力&lt;/strong&gt;，比如给出上半句生成下半句、或者做完形填空，都还是像在做language model任务.&#xA;而Instruction Tuning则是激发语言模型的&lt;strong&gt;理解能力&lt;/strong&gt;，通过给出更明显的指令/指示，让模型去理解并做出正确的action&#xA;&lt;strong&gt;Prompt tuning&lt;/strong&gt;都是针对&lt;strong&gt;一个任务&lt;/strong&gt;的，比如做个情感分析任务的prompt tuning，精调完的模型只能用于情感分析任务，而经过&lt;strong&gt;Instruction Tuning多任务&lt;/strong&gt;精调后，可以用于其他任务的zero-shot&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Prompt Tuning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Soft-Prompt/PromptTuning/</link>
      <pubDate>Fri, 06 Jan 2023 19:06:15 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Soft-Prompt/PromptTuning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;npl范式-1&#34;&gt;&#xA;  NPL范式 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#npl%e8%8c%83%e5%bc%8f-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/npl4Paragiam.jpg&#34; alt=&#34;npl4Paragiam&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;prompt-tuning-2&#34;&gt;&#xA;  Prompt Tuning [2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#prompt-tuning-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;🔔 Prompt Tuning&#xA;&lt;ul&gt;&#xA;&lt;li&gt;🔗 文章：The Power of Scale for Parameter-Efficient Prompt Tuning (EMNLP 2021) &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.243/&#34;&gt;https://aclanthology.org/2021.emnlp-main.243/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;🔑关键词和摘要&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Keywords: Large-scale PLMs, Parameter-efficient Tuning, Prompt Tuning&lt;/li&gt;&#xA;&lt;li&gt;摘要&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Prompt变成可学习的向量，固定PLM，微调Prompt来适配下游任务&lt;/li&gt;&#xA;&lt;li&gt;PLM参数规模越大，Prompt Tuning的性能和全参数微调越接近&lt;/li&gt;&#xA;&lt;li&gt;这种基于&lt;strong&gt;Soft Prompt&lt;/strong&gt;的Prompt Tuning方法可以看作是&lt;strong&gt;Prefix Tuning的简化版本&lt;/strong&gt;（只加在输入上）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;⚙️研究设计和结论&#xA;&lt;ul&gt;&#xA;&lt;li&gt;方法&#xA;&lt;ul&gt;&#xA;&lt;li&gt;模型示意图：xxx&lt;/li&gt;&#xA;&lt;li&gt;模型基本思路：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;经典分类：P(Y | X; θ)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Hard Prompt: P(Y | [P;X] ; θ)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Soft Prompt: P(Y | [P;X] ; θ; Δ)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Pre-Training&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Fine-Tuning&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Prompt Tuning&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;实现细节：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;模型参数量&#xA;&lt;ul&gt;&#xA;&lt;li&gt;参数量：T5 ~ T5-XXL(10B)&lt;/li&gt;&#xA;&lt;li&gt;预训练：LM Adaptation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Prompt长度：xxx&#xA;&lt;ul&gt;&#xA;&lt;li&gt;1、5、20、100、150&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;初始化方法：xxx&#xA;&lt;ul&gt;&#xA;&lt;li&gt;随机初始化&lt;/li&gt;&#xA;&lt;li&gt;使用预设文本的词向量初始化，类似于设计hard prompt，然后将hard prompt转化为soft prompt&lt;/li&gt;&#xA;&lt;li&gt;使用类别词向量初始化，类似于提供选项&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;实验&#xA;&lt;ul&gt;&#xA;&lt;li&gt;数据集：SuperGLUE&lt;/li&gt;&#xA;&lt;li&gt;xxx&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Prompt的规模越大，性能相对而言会越好&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;xxx&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基于语义信息的初始化比随机初始化要好&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;xxx&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LM Adaptation 对性能提升显著&lt;/li&gt;&#xA;&lt;li&gt;Prompt Tuning还是需要大模型有较好的文本生成能力&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;xxx&#xA;&lt;ul&gt;&#xA;&lt;li&gt;模型参数规模越大，Prompt Tuning效果越好&lt;/li&gt;&#xA;&lt;li&gt;10B参数时与全参数微调性能接近&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;📚论文贡献&#xA;&lt;ul&gt;&#xA;&lt;li&gt;优点（计算友好）&#xA;&lt;ul&gt;&#xA;&lt;li&gt;大模型的&lt;strong&gt;微调新范式&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;一个中心模型服务多个下游任务&lt;/strong&gt;，&lt;strong&gt;节省参数存储量&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;无需优化模型参数&lt;/strong&gt;，节省优化器的计算量和存储量&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;只在输入层进行操作&lt;/strong&gt;，适合多任务场景下的计算合并&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;缺点（性能和收敛性存在问题）&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Prompt Tuning的&lt;strong&gt;收敛速度很慢&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Prompt Tuning的模型&lt;strong&gt;性能不稳定&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Few-shot场景上表现不佳&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;prompt-tuning3&#34;&gt;&#xA;  Prompt Tuning[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#prompt-tuning3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/promptTuning.JPG&#34; alt=&#34;promptTuning&#34; /&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)RAG OpenAI案例</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E6%A1%88%E4%BE%8B/RAGOpenAI/</link>
      <pubDate>Tue, 27 Dec 2022 11:11:05 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E6%A1%88%E4%BE%8B/RAGOpenAI/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;openai-rag-案例3&#34;&gt;&#xA;  OpenAI RAG 案例[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#openai-rag-%e6%a1%88%e4%be%8b3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/openai-rag.jpg&#34; alt=&#34;openai-rag&#34; /&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;retrieval with consine similarity&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;HyDE retrieval&lt;/strong&gt; [5]&#xA;Fine-tune Embeddings&#xA;&lt;strong&gt;Chunk/embedding experiments&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Reranking&lt;/strong&gt; [6][8]&#xA;Classification step&lt;/li&gt;&#xA;&lt;li&gt;Prompt engineering&#xA;&lt;strong&gt;Tool use&lt;/strong&gt;&#xA;&lt;strong&gt;Query expansion&lt;/strong&gt;[5]&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;query-transformations5&#34;&gt;&#xA;  Query Transformations[5]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#query-transformations5&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Query expansion&lt;/strong&gt;&#xA;Multi-query retriever&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;HyDE&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Step back prompting&#xA;[抽象prompting]&lt;/li&gt;&#xA;&lt;li&gt;Rewrite-Retrieve-Read&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;query-construction-4&#34;&gt;&#xA;  Query Construction [4]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#query-construction-4&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/structured_data_stacks.jpg&#34; alt=&#34;structured_data_stacks&#34; /&gt;&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Data source&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Text-to-metadata-filter&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Vectorstores&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/?ref=blog.langchain.dev#constructing-from-scratch-with-lcel&#34;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Text-to-SQL&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;SQL DB&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://python.langchain.com/docs/use_cases/qa_structured/sql?ref=blog.langchain.dev&#34;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href=&#34;https://blog.langchain.dev/llms-and-sql/&#34;&gt;&lt;strong&gt;blog&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href=&#34;https://blog.langchain.dev/incorporating-domain-specific-knowledge-in-sql-llm-solutions/&#34;&gt;&lt;strong&gt;blog&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Text-to-metadata-filter [7]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;A &lt;strong&gt;self-querying&lt;/strong&gt; retriever is one that, as the name suggests, has the  ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a &lt;strong&gt;structured query&lt;/strong&gt; and then applies that structured query to its underlying  VectorStore. This allows the retriever to not only use the user-input  query for semantic similarity comparison with the contents of stored  documents but to also &lt;strong&gt;extract filters from the user query on the  metadata of stored documents and to execute those filters&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)PEFT &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/FineTuning/</link>
      <pubDate>Fri, 18 Nov 2022 23:31:46 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/FineTuning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;peft原理&#34;&gt;&#xA;  PEFT原理&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#peft%e5%8e%9f%e7%90%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/PEFT-10dbfe2110848028b9afd05f05fdbde6?pvs=4&#34;&gt;(原理)PEFT&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Prompt Engineering</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Prompt-Engineering/PromptEngineering/</link>
      <pubDate>Thu, 10 Nov 2022 15:48:39 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Prompt-Engineering/PromptEngineering/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;basic-prompting-2&#34;&gt;&#xA;  Basic Prompting [2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#basic-prompting-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;zero-shot-prompting-3&#34;&gt;&#xA;  Zero-Shot Prompting [3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#zero-shot-prompting-3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h3 id=&#34;few-shot-prompting-3&#34;&gt;&#xA;  Few-Shot Prompting [3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#few-shot-prompting-3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;cot-2&#34;&gt;&#xA;  CoT [2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#cot-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;chain-of-thought-promptingcot-3&#34;&gt;&#xA;  Chain-of-Thought Prompting(CoT) [3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#chain-of-thought-promptingcot-3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Few-shot CoT&lt;/li&gt;&#xA;&lt;li&gt;Zero-shot COT&lt;br&gt;&#xA;&lt;strong&gt;&amp;ldquo;Let&amp;rsquo;s think step by step&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;self-consistencycot-sc-3&#34;&gt;&#xA;  Self-Consistency(CoT-SC) [3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#self-consistencycot-sc-3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;The idea is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to &lt;strong&gt;select&lt;/strong&gt; the most consistent answer.&lt;/p&gt;&#xA;&lt;h3 id=&#34;tree-of-thoughts-tot&#34;&gt;&#xA;  Tree of Thoughts (ToT)&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tree-of-thoughts-tot&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h3 id=&#34;cot-vs-cot-sc-vs-tot--3&#34;&gt;&#xA;  CoT vs. CoT-SC vs. ToT  [3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#cot-vs-cot-sc-vs-tot--3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/TOT.jpg&#34; alt=&#34;TOT&#34; /&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Agent 架构 &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Overview/Agent/</link>
      <pubDate>Wed, 02 Nov 2022 10:55:27 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Overview/Agent/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;agent-架构&#34;&gt;&#xA;  Agent 架构&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agent-%e6%9e%b6%e6%9e%84&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Agent-10dbfe211084806fa87cfd37aed482ea?pvs=4&#34;&gt;(原理)Agent 架构 &lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Langchain</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Langchain/Langchain/</link>
      <pubDate>Wed, 02 Nov 2022 10:45:06 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Langchain/Langchain/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;modules&#34;&gt;&#xA;  Modules&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#modules&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;main-modules&#34;&gt;&#xA;  main modules&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#main-modules&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;model-io&#34;&gt;&#xA;  Model I/O&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#model-io&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Language models  [10]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LLM&lt;/li&gt;&#xA;&lt;li&gt;Chat Model&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Embedding&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Prompts&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Prompt Template&lt;/li&gt;&#xA;&lt;li&gt;Few-shot example&lt;/li&gt;&#xA;&lt;li&gt;Example Selectors [类比选择]&#xA;关键字  相似度  长度&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Output parsers&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;function call&lt;/strong&gt;[2]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;retrieval&#34;&gt;&#xA;  Retrieval&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#retrieval&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Document Loaders&lt;/li&gt;&#xA;&lt;li&gt;Text Splitters&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Retrievers&lt;/strong&gt;[10]&lt;/li&gt;&#xA;&lt;li&gt;VectorStores&lt;/li&gt;&#xA;&lt;li&gt;index&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;agent&#34;&gt;&#xA;  Agent&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agent&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Plan-and-execute agents&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;additional-modules&#34;&gt;&#xA;  Additional modules&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#additional-modules&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;chains&#34;&gt;&#xA;  Chains&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#chains&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;2大类&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Chain interface[Legacy]&lt;/li&gt;&#xA;&lt;li&gt;LangChain Expression Language (LCEL)&#xA;LCEL is a declarative way to compose chains.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Foundational&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LLM&lt;/li&gt;&#xA;&lt;li&gt;Sequential- SequentialChain&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Router&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Transformation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;memory-10&#34;&gt;&#xA;  Memory [10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#memory-10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;帮语言模型补充上下文&lt;/li&gt;&#xA;&lt;li&gt;ConversationBufferMemory&lt;/li&gt;&#xA;&lt;li&gt;ConversationBufferWindowMemory&#xA;窗口&lt;/li&gt;&#xA;&lt;li&gt;ConversationSummaryMemory&lt;/li&gt;&#xA;&lt;li&gt;VectorStoreRetrieverMemory&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;function-call&#34;&gt;&#xA;  Function Call&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#function-call&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langchain.chains.openai_functions.base &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; (&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    create_openai_fn_chain,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    create_structured_output_chain,[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langchain.chains.openai_functions.citation_fuzzy_match &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; (&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    create_citation_fuzzy_match_chain,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langchain.chains.openai_functions.extraction &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; (&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    create_extraction_chain,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    create_extraction_chain_pydantic,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langchain.chains.openai_functions.qa_with_structure &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; (&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    create_qa_with_sources_chain,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    create_qa_with_structure_chain,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langchain.chains.openai_functions.tagging &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; (&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    create_tagging_chain,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    create_tagging_chain_pydantic,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;应用4&#34;&gt;&#xA;  应用[4]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ba%94%e7%94%a84&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Question &amp;amp; Answering Using Documents As Context[3]&lt;/li&gt;&#xA;&lt;li&gt;Extraction[Kor]&lt;/li&gt;&#xA;&lt;li&gt;Evaluation&lt;/li&gt;&#xA;&lt;li&gt;Querying Tabular Data[sqlite]&lt;/li&gt;&#xA;&lt;li&gt;Code Understanding&lt;/li&gt;&#xA;&lt;li&gt;Interacting with APIs&lt;/li&gt;&#xA;&lt;li&gt;Chatbots&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;chains-1-89&#34;&gt;&#xA;  Chains [1] [8][9]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#chains-1-89&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;chain &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; load_summarize_chain(llm, chain_type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stuff&amp;#34;&lt;/span&gt;, verbose&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;chain &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; load_summarize_chain(llm, chain_type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;map_reduce&amp;#34;&lt;/span&gt;, verbose&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;chain &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; load_summarize_chain(llm, chain_type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;refine&amp;#34;&lt;/span&gt;, verbose&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;chain &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; load_qa_chain(llm, chain_type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;map_rerank&amp;#34;&lt;/span&gt;, verbose&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;, return_intermediate_steps&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;链类型&lt;/th&gt;&#xA;          &lt;th&gt;整合方法&lt;/th&gt;&#xA;          &lt;th&gt;优缺点&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;stuff&lt;/td&gt;&#xA;          &lt;td&gt;将所有内容放入一个提示中，输入LLM&lt;/td&gt;&#xA;          &lt;td&gt;简单、廉价、效果好/ 对输入文本有一定token限制&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Map_reduce&lt;/td&gt;&#xA;          &lt;td&gt;每个问题和文本块单独给语言模型，并将答案汇总生成最终结果&lt;/td&gt;&#xA;          &lt;td&gt;输入任意数量文本，且并行处理/ 速度慢，费token&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Refine&lt;/td&gt;&#xA;          &lt;td&gt;迭代处理多个文本，基于前一个文档答案构建下一个答案&lt;/td&gt;&#xA;          &lt;td&gt;用于组合信息，依次构建答案/ 速度慢，费token&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Map_rerank&lt;/td&gt;&#xA;          &lt;td&gt;每个文档单独调用LLM,并要求返回一个得分，然后选择最高的得分&lt;/td&gt;&#xA;          &lt;td&gt;需要告诉模型评分的规则/ 费token&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/chains-type.jpg&#34; alt=&#34;chains-type&#34; /&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(综述)RAG &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAG/</link>
      <pubDate>Wed, 02 Nov 2022 09:57:59 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAG/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;rag综述&#34;&gt;&#xA;  RAG综述&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag%e7%bb%bc%e8%bf%b0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/RAG-108bfe21108480be9c7ee46ff02a1ad6?pvs=4&#34;&gt;(综述)RAG&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>GPT-工具和应用</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Application/gpt/</link>
      <pubDate>Mon, 09 May 2022 22:46:01 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Application/gpt/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;platform&#34;&gt;&#xA;  Platform&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#platform&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;国外&#xA;&lt;a href=&#34;https://poe.com/ChatGPT&#34;&gt;Poe&lt;/a&gt; ***&lt;/li&gt;&#xA;&lt;li&gt;国内&#xA;&lt;a href=&#34;https://saas.edu360.cn/system/chatgpt&#34;&gt;实战云&lt;/a&gt; gpt3.5  gpt4&#xA;&lt;a href=&#34;https://www.feijix.com/n/y0BnXI&#34;&gt;ChatGPT使用指南！&lt;/a&gt;   ***&#xA;&lt;a href=&#34;https://www.1888ai.com/base/chat&#34;&gt;灵犀百通&lt;/a&gt;  gpt3.5&#xA;&lt;a href=&#34;https://gpt.91chat-ai.cn/chat&#34;&gt;ChatGpt PLUS&lt;/a&gt;&#xA;&lt;a href=&#34;https://yiyan.baidu.com/&#34;&gt;文心一言&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;tools--mix&#34;&gt;&#xA;  Tools &amp;amp; Mix&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tools--mix&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;GPT学习宝典&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;聚合&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://gpt.candobear.com/toolbox&#34;&gt;GPT  工具箱&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;教程&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://gpt.candobear.com/courses&#34;&gt;学习资料&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://gp477l8icq.feishu.cn/wiki/JUXnwzSuviL5E9kh6jUc8FRinHe&#34;&gt;极客时间 AIGC 知识库&lt;/a&gt; ***&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;聚合&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://gp477l8icq.feishu.cn/wiki/M1uCwFNjkiAGC7k30TaclZqknPh&#34;&gt;AI工具大全&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://gp477l8icq.feishu.cn/wiki/RpabwPG9niFEu9kwJAQcAGxenDg&#34;&gt;AI主流工具精选&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://gp477l8icq.feishu.cn/wiki/VJ9ewqfOgiyrbQksbyLcrODtnkb&#34;&gt;AI经典项目&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://gp477l8icq.feishu.cn/wiki/QVV6w3XstiR7hlkK53Bc8f9DnMf&#34;&gt;AI导航站&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://longalong.feishu.cn/wiki/wikcneAKpN3u473N7J9EAC4Ga0b&#34;&gt;应用与变现案例&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.ailookme.com&#34;&gt;AI 工具箱&lt;/a&gt;  ***&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://gptdoc.sparkai.chat/&#34;&gt;ChatGPT Tutorial 101&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;应用&#34;&gt;&#xA;  应用&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ba%94%e7%94%a8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;思维导图&#34;&gt;&#xA;  思维导图&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%80%9d%e7%bb%b4%e5%af%bc%e5%9b%be&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://albus.org/&#34;&gt;albus&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;视频&#34;&gt;&#xA;  视频&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%a7%86%e9%a2%91&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://b.jimmylv.cn/&#34;&gt;BibiGPT&lt;/a&gt;&#xA;&lt;a href=&#34;https://crucible.docnavigator.in/&#34;&gt;Youtube tools&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;英语&#34;&gt;&#xA;  英语&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%8b%b1%e8%af%ad&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://callannie.ai/signin&#34;&gt;callannie&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;客户端&#34;&gt;&#xA;  客户端&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%a2%e6%88%b7%e7%ab%af&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ChatGPT 客户端&#xA;windows， mac&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;chrome-plugin&#34;&gt;&#xA;  Chrome plugin&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#chrome-plugin&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;WebChatGPT[instatlled]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;AIPRM for ChatGPT[instatlled]&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实现){Jina}Deep Research</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Deep-Research/%E5%AE%9E%E7%8E%B0/DeepResearchJina/</link>
      <pubDate>Tue, 16 Apr 2024 17:17:32 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Deep-Research/%E5%AE%9E%E7%8E%B0/DeepResearchJina/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;deep-researchjina&#34;&gt;&#xA;  Deep Research(Jina)&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#deep-researchjina&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Deep-Research-Jina-1d7bfe211084803d8405c0e74cdf395d?pvs=4&#34;&gt;(原理|实现)Deep Research(Jina)&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)Self-QA *</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/Data-Quality/Instruction-Diversity/DataSelfQA/</link>
      <pubDate>Wed, 27 Sep 2023 12:11:03 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/Data-Quality/Instruction-Diversity/DataSelfQA/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;self-qa10&#34;&gt;&#xA;  Self-QA[10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#self-qa10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.11952&#34;&gt;SELF-QA: Unsupervised Knowledge Guided Language Model Alignment&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;思想&#34;&gt;&#xA;  思想&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%80%9d%e6%83%b3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;知识引导的指令生成Knowledge-Guided Instruction Generation&lt;/p&gt;&#xA;&lt;h3 id=&#34;指令生成阶段&#34;&gt;&#xA;  指令生成阶段&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%8c%87%e4%bb%a4%e7%94%9f%e6%88%90%e9%98%b6%e6%ae%b5&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;采用语言模型本身来根据无监督的文本生成指令。这种方法使生成的指令具有领域针对性，并与所提供的无监督文本的内容相关。&#xA;&lt;ul&gt;&#xA;&lt;li&gt;非结构化的知识，如网页和书籍数据，直接使用。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;结构化数据&lt;/strong&gt;，如表格和知识图谱，在被利用之前需要&lt;strong&gt;转换为非结构化文本数据&lt;/strong&gt;。如通过使用模板填充槽或将每个数据条目与相应的属性名称连接起来来实现。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;指令答案生成阶段&#34;&gt;&#xA;  指令答案生成阶段&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%8c%87%e4%bb%a4%e7%ad%94%e6%a1%88%e7%94%9f%e6%88%90%e9%98%b6%e6%ae%b5&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;将&lt;strong&gt;生成的指令问题&lt;/strong&gt;让大模型进行预测，&lt;strong&gt;生成答案&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;self-qa-实战11&#34;&gt;&#xA;  Self-QA 实战[11]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#self-qa-%e5%ae%9e%e6%88%9811&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;SYSTEM_PROMPT &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    你是一个能根据提供的文本内容生成QA对的机器人。以下是你的任务要求：&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    1. 生成尽可能多的QA对。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    2. 每个QA对包含一个问题和一个简洁的答案。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    3. 答案必须用简体中文。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    4. 生成的QA对不能重复。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    5. 使用json格式将QA对包裹起来，问题用&amp;#34;question&amp;#34;表示，答案用&amp;#34;answer&amp;#34;表示。&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    示例格式：&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    [&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;            &amp;#34;question&amp;#34;: &amp;#34;...&amp;#34;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;            &amp;#34;answer&amp;#34;: &amp;#34;...&amp;#34;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        },&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;            &amp;#34;question&amp;#34;: &amp;#34;...&amp;#34;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;            &amp;#34;answer&amp;#34;: &amp;#34;...&amp;#34;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    ]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    以下是给定的文本内容：&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;self-qa&#34;&gt;&#xA;  Self-QA&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#self-qa&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;10.《第二章 大模型训练与微调研发背后的数据艺术》 LLM大语言模型算法特训 那位科技 ***&lt;br&gt;&#xA;&lt;strong&gt;SELF-INSTRUCT&lt;/strong&gt;， Baize， &lt;strong&gt;Evol-instruct&lt;/strong&gt;， &lt;strong&gt;Self-QA&lt;/strong&gt;， Ultra-chat&lt;/p&gt;</description>
    </item>
    <item>
      <title>(框架)RAG Langchain-Chatchat</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/framework/RAGchatchat/</link>
      <pubDate>Wed, 31 May 2023 11:31:35 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/framework/RAGchatchat/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;langchain-chatchat-架构&#34;&gt;&#xA;  Langchain-Chatchat 架构&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#langchain-chatchat-%e6%9e%b6%e6%9e%84&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/langchain&amp;#43;chatglm.jpg&#34; alt=&#34;langchain&amp;#43;chatglm&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;组件&#xA;&lt;ul&gt;&#xA;&lt;li&gt;本地知识库&lt;/li&gt;&#xA;&lt;li&gt;Embedding 模型&lt;/li&gt;&#xA;&lt;li&gt;向量数据库&lt;/li&gt;&#xA;&lt;li&gt;Prompt Template&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;langchain-chatchat&#34;&gt;&#xA;  Langchain-Chatchat&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#langchain-chatchat&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;部署&#xA;&lt;ul&gt;&#xA;&lt;li&gt;windows 10 [5]&#xA;部署本地， 没显存，卡&lt;/li&gt;&#xA;&lt;li&gt;Linux [2]&#xA;部署   32C125G ，没显存， 推理很慢&lt;/li&gt;&#xA;&lt;li&gt;Docker&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/chatchat-space/Langchain-Chatchat&#34;&gt;Langchain-Chatchat &lt;/a&gt; master&#xA;Langchain 与 ChatGLM 等语言模型的本地知识库问答&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/chatchat-space/Langchain-Chatchat/tree/v0.2.4&#34;&gt;Langchain-Chatchat&lt;/a&gt;  v0.2.4&lt;br&gt;&#xA;&lt;a href=&#34;https://gitee.com/deepeye/langchain-ChatGLM&#34;&gt;langchain-ChatGLM&lt;/a&gt;  gitee&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/www6v/Langchain-Chatchat-Colab&#34;&gt;Colab for Langchain-Chatchat&lt;/a&gt;   linux 可以部署  v0.2.6&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/649055955&#34;&gt;langChain-ChatGLM 尝试，踩坑记录&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/651189680&#34;&gt;Langchain-Chatchat + 阿里通义千问Qwen 保姆级教程 | 次世代知识管理解决方案&lt;/a&gt;    Langchain-Chatchat + 通义千问&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_43094965/article/details/133044128&#34;&gt;win10 安装 Langchain-Chatchat 避坑指南（2023年9月18日v0.2.4版本，包含全部下载内容！）&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(原理|实战)RAG Fusion</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPostRetrieval/RAGFusion/</link>
      <pubDate>Sun, 14 May 2023 18:23:41 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPostRetrieval/RAGFusion/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h3 id=&#34;rag-fusion多查询结果融合策略&#34;&gt;&#xA;  RAG-Fusion多查询结果融合策略&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag-fusion%e5%a4%9a%e6%9f%a5%e8%af%a2%e7%bb%93%e6%9e%9c%e8%9e%8d%e5%90%88%e7%ad%96%e7%95%a5&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;将多个召回查询的结果进行&lt;strong&gt;合并&lt;/strong&gt;[3]&lt;/p&gt;&#xA;&lt;p&gt;其思想在于通过生成多个用户查询和重新排序结果来解决RAG固有的约束；利用倒数排序融合（RRF）和自定义向量评分加权，生成全面准确的结果。[2]&lt;/p&gt;&#xA;&lt;h3 id=&#34;代码&#34;&gt;&#xA;  代码&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bb%a3%e7%a0%81&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_fusion.ipynb&#34;&gt;RAG Fusion&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1&#34;&gt;Forget RAG, the Future is RAG-Fusion&lt;/a&gt;  失效&#xA;&lt;a href=&#34;https://mp.weixin.qq.com/s/N7HgjsqgCVf2i-xy05qZtA&#34;&gt;使用RAG-Fusion和RRF让RAG在意图搜索方面更进一步&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/NFjn8pUsQaSx85nhBphORA&#34;&gt;再谈大模型RAG问答中的三个现实问题：兼看RAG-Fusion多query融合策略、回答引文生成策略及相关数据集概述&lt;/a&gt; &lt;br&gt;&#xA;二、基于大模型生成能力自动生成引文&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/pK2BRLrWpEKKIPFhUtGvcg&#34;&gt;一文详看Langchain框架中的RAG多阶段优化策略：从问题转换到查询路由再到生成优化&lt;/a&gt; ***   原理paper，代码示例&lt;br&gt;&#xA;Multi Query多查询策略， Decomposition问题， [RAG-Fusion]， Step Back， HyDE混合&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/langchain-ai/rag-from-scratch&#34;&gt;rag-from-scratch Repo&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(原理|实战)Query Routing</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPreRetrieval/RAGRouting/</link>
      <pubDate>Sun, 14 May 2023 16:28:20 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPreRetrieval/RAGRouting/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;类型1&#34;&gt;&#xA;  类型[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%b1%bb%e5%9e%8b1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;LLM Routers&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LLM Completion Routers&lt;/li&gt;&#xA;&lt;li&gt;LLM Function Calling Routers&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Semantic Routers&lt;/strong&gt; [2]&lt;/li&gt;&#xA;&lt;li&gt;Zero Shot Classification Routers&lt;/li&gt;&#xA;&lt;li&gt;Language Classification Routers&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/overview.png&#34; alt=&#34;overview&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;logical-and-semantic-routing3&#34;&gt;&#xA;  Logical and Semantic routing[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#logical-and-semantic-routing3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;logical-routing&#34;&gt;&#xA;  Logical routing&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#logical-routing&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/logical-router.png&#34; alt=&#34;Logical routing&#34; /&gt;&lt;/p&gt;&#xA;&lt;details &gt;&lt;summary&gt;code&lt;/summary&gt;&#xA;  &lt;div class=&#34;markdown-inner&#34;&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; typing &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Literal&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langchain_core.prompts &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; ChatPromptTemplate&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langchain_core.pydantic_v1 &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; BaseModel, Field&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langchain_openai &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; ChatOpenAI&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Data model&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;RouteQuery&lt;/span&gt;(BaseModel):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Route a user query to the most relevant datasource.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    datasource: Literal[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;python_docs&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;js_docs&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;golang_docs&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Field(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        description&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Given a user question choose which datasource would be most relevant for answering their question&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    )&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# LLM with function call &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;llm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ChatOpenAI(model&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gpt-3.5-turbo-0125&amp;#34;&lt;/span&gt;, temperature&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;structured_llm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;with_structured_output(RouteQuery)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Prompt &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;system &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;You are an expert at routing a user question to the appropriate data source.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Based on the programming language the question is referring to, route it to the relevant data source.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;prompt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ChatPromptTemplate&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_messages(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;system&amp;#34;&lt;/span&gt;, system),&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;human&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{question}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;),&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Define router &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;router &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; prompt &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; structured_llm&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;choose_route&lt;/span&gt;(result):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;python_docs&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;datasource&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lower():&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;### Logic here &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;chain for python_docs&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;js_docs&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;datasource&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lower():&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;### Logic here &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;chain for js_docs&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;### Logic here &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;golang_docs&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langchain_core.runnables &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; RunnableLambda&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;full_chain &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; router &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; RunnableLambda(choose_route)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;full_chain&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;invoke({&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;question&amp;#34;&lt;/span&gt;: question})&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;&#xA;&lt;/details&gt;&#xA;&lt;h3 id=&#34;semantic-routing&#34;&gt;&#xA;  Semantic routing&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#semantic-routing&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/semantic-router.png&#34; alt=&#34;Semantic routing&#34; /&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战)LangGraph</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Multi-agent/MultiAgentsPractice/</link>
      <pubDate>Sun, 07 May 2023 16:14:30 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Multi-agent/MultiAgentsPractice/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;langgraph-1&#34;&gt;&#xA;  LangGraph [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#langgraph-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;agent-supervisor&#34;&gt;&#xA;  Agent Supervisor&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agent-supervisor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb&#34;&gt;Agent Supervisor Repo&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;h3 id=&#34;multi-agent-collaboration&#34;&gt;&#xA;  Multi Agent Collaboration&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#multi-agent-collaboration&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/multi-agent-collaboration.ipynb&#34;&gt;Basic Multi-agent Collaboration&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;h3 id=&#34;hierarchical-agent-teams&#34;&gt;&#xA;  Hierarchical Agent Teams&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#hierarchical-agent-teams&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb&#34;&gt;Hierarchical Agent Teams&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;langgraph&#34;&gt;&#xA;  LangGraph&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#langgraph&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://blog.langchain.dev/langgraph-multi-agent-workflows/&#34;&gt;LangGraph: Multi-Agent Workflows&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1F541117kW/&#34;&gt;LangGraph：Multi-Agent 实战&lt;/a&gt; V&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Multi-Agent  Fail &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Multi-agent/MultiAgentsFail/</link>
      <pubDate>Sun, 07 May 2023 16:14:30 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Multi-agent/MultiAgentsFail/</guid>
      <description>&lt;h1 id=&#34;multi-agent--fail&#34;&gt;&#xA;  Multi-Agent  Fail&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#multi-agent--fail&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Why-Do-Multi-Agent-LLM-Systems-Fail-1c4bfe211084809383b2cb0bd5f82296?pvs=4&#34;&gt;Multi-Agent  Fail&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Modular RAG &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAGModularRAG/</link>
      <pubDate>Fri, 21 Apr 2023 19:22:24 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAGModularRAG/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;modular-rag&#34;&gt;&#xA;  Modular RAG&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#modular-rag&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Modular-RAG-108bfe21108480468c35c5b45d991778?pvs=4&#34;&gt;(原理)Modular RAG&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)RAG Baichuan案例</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E6%A1%88%E4%BE%8B/RAGBaichuan/</link>
      <pubDate>Tue, 18 Apr 2023 14:26:22 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E6%A1%88%E4%BE%8B/RAGBaichuan/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;baichuan-rag1&#34;&gt;&#xA;  Baichuan RAG[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#baichuan-rag1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;借鉴了Meta的CoVe技术&lt;/li&gt;&#xA;&lt;li&gt;自研的TSF（Think-Step Further)技术&#xA;猜测其本质应该是对Step-back prompting方法的改良&lt;/li&gt;&#xA;&lt;li&gt;自研了Baichuan-Text-Embedding向量模型&lt;/li&gt;&#xA;&lt;li&gt;混合检索&#xA;向量检索与稀疏检索并行的&lt;/li&gt;&#xA;&lt;li&gt;self-Critique&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;总结2&#34;&gt;&#xA;  总结[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%80%bb%e7%bb%932&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;**多轮问答等场景的召回和传统搜索引擎的召回分布还不太一样。**百川借助子问题检索效果更高的特点，对原始复杂问题进行拆解、拓展来解决复杂问题检索质量偏差的问题。&lt;/li&gt;&#xA;&lt;li&gt;**对于没见过的语料直接用向量检索的结果可能不太理想。**百川在大量语料上利用无监督方法训练embedding模型来优化效果。而行业大模型更倾向于私有的数据，要提升私有数据的训练效果还得继续在私有化数据上训练效果会更佳。&lt;/li&gt;&#xA;&lt;li&gt;**Query拓展 + 多路召回 + Rerank + self-Critique可能是现阶段比较好的一种RAG方式，但是其也会带来更多成本。**总体思路有点像ReAct[3]系列的进阶版本，其在搜索侧和答案修正侧都做了更多的一些工作来优化实际效果。其缺点是需要多次调用大模型，会带来额外的成本，真实线上是否采用这种策略还有待验证。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648407638&amp;amp;idx=1&amp;amp;sn=5c167b4a11bc483f5790ef1e0340d670&#34;&gt;大模型RAG问答行业最佳案例及微调、推理双阶段实现模式：基于模块化(Modular)RAG自定义RAG Flow&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/675770700&#34;&gt;百川智能RAG方案总结：搜索出生的百川智能大模型RAG爬坑之路&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/658469464&#34;&gt;LLM/百川Baichuan2-53B搜索增强-开放API&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650901201&amp;amp;idx=1&amp;amp;sn=3a9bd61403fb4b024ec5d8c128990495&#34;&gt;大模型+搜索构建完整技术栈，百川智能用搜索增强给企业定制化下了一剂「猛药」&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://blog.csdn.net/qq_27590277/article/details/135421245&#34;&gt;百川智能RAG方案总结：搜索出生的百川智能大模型RAG爬坑之路&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>P-Tuning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Soft-Prompt/PEFTPtuning/</link>
      <pubDate>Fri, 24 Mar 2023 22:17:49 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Soft-Prompt/PEFTPtuning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;p-tuning2&#34;&gt;&#xA;  P-Tuning[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#p-tuning2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;P-Tuning 的创新之处在于将提示（Prompt）转化为&lt;strong&gt;可学习的嵌入层（Embedding Layer）&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;架构&#34;&gt;&#xA;  架构&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9e%b6%e6%9e%84&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/ptuning.png&#34; alt=&#34;ptuning&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;一个关于“The capital of Britain is [MASK]” 示例：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;蓝色是上下文 “Britain”&lt;/li&gt;&#xA;&lt;li&gt;红色是目标单词 “[MASK]”，&lt;/li&gt;&#xA;&lt;li&gt;橙色区域是提示词。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;传统方式 与 P-Tuning 对比：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在（a）中，提示生成器只接收离散奖励；&lt;/li&gt;&#xA;&lt;li&gt;在（b）中，连续的&lt;strong&gt;提示嵌入（Prompt Embedding）&lt;/strong&gt; 和**提示编码器（Prompt Encoder）**以可微的方式进行 优化。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;p-tuning-v22&#34;&gt;&#xA;  P-Tuning v2[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#p-tuning-v22&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;背景&#34;&gt;&#xA;  背景&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%83%8c%e6%99%af&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;之前的方法在以下两方面有所&lt;strong&gt;限制&lt;/strong&gt;：&#xA;• 模型规模差异：在大型预训练模型中，Prompt Tuning 和&#xA;P-Tuning 能取得与全面微调相似的效果，但在参数较少&#xA;的模型上则表现不佳。&#xA;• 任务类型差异：无论是 Prompt Tuning 还是 P-Tuning，&#xA;在序列标注任务上的表现都较差。&lt;/p&gt;&#xA;&lt;h3 id=&#34;目的&#34;&gt;&#xA;  目的&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e7%9a%84&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;P-Tuning v2 旨在使提示调整（Prompt Tuning）在不同规模的预训练模型上，针对各种下游任务都能达到类似全面微调（Fine-tuning）的效果。&lt;/p&gt;&#xA;&lt;h3 id=&#34;架构-1&#34;&gt;&#xA;  架构 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9e%b6%e6%9e%84-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/ptuning-v2.png&#34; alt=&#34;ptuning-v2&#34; /&gt;&#xA;在每一层都加入了Prompts tokens 作为输入,  而不是仅仅加在输入层&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战)多模态 RAG</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/Multimodal-RAG/RAGMultimodalPractice/</link>
      <pubDate>Tue, 14 Mar 2023 13:55:59 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/Multimodal-RAG/RAGMultimodalPractice/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;多模态rag-多向量检索器-1011&#34;&gt;&#xA;  多模态RAG-多向量检索器 [10][11]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%9a%e6%a8%a1%e6%80%81rag-%e5%a4%9a%e5%90%91%e9%87%8f%e6%a3%80%e7%b4%a2%e5%99%a8-1011&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;semi-structured-tables--text-rag-20&#34;&gt;&#xA;  semi-structured (tables + text) RAG [20]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#semi-structured-tables--text-rag-20&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/1.png&#34; alt=&#34;1.png&#34; /&gt;&#xA;分析pdf中表格&lt;/p&gt;&#xA;&lt;h3 id=&#34;multi-modal-text--tables--images-rag--13&#34;&gt;&#xA;  multi-modal (text + tables + images) RAG  [13]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#multi-modal-text--tables--images-rag--13&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/2.png&#34; alt=&#34;2.png&#34; /&gt;&#xA;分析PDF中图片&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Option 1&lt;/strong&gt;  [基于CLIP] [23]   &lt;em&gt;[30][32][33]&lt;/em&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Use multimodal embeddings &lt;strong&gt;(such as &lt;a href=&#34;https://openai.com/research/clip&#34;&gt;CLIP&lt;/a&gt;)&lt;/strong&gt; to embed images and text&lt;/li&gt;&#xA;&lt;li&gt;Retrieve both using similarity search&lt;/li&gt;&#xA;&lt;li&gt;Pass &lt;strong&gt;raw images and text chunks&lt;/strong&gt; to a multimodal LLM for answer synthesis&lt;br&gt;&#xA;{选项1：对文本和表格生成summary，然后应用多模态embedding模型把文本/表格summary、原始图片转化成embedding存入多向量检索器。对话时，根据query召回原始文本/表格/图像。然后将其喂给多模态LLM生成应答结果。}[10]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Option 2&lt;/strong&gt;   [21]&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Use a multimodal LLM (such as &lt;a href=&#34;https://openai.com/research/gpt-4v-system-card&#34;&gt;GPT4-V&lt;/a&gt;, &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;LLaVA&lt;/a&gt;, or &lt;a href=&#34;https://www.adept.ai/blog/fuyu-8b&#34;&gt;FUYU-8b&lt;/a&gt;) to produce &lt;strong&gt;text summaries from images&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Embed and retrieve text&lt;/li&gt;&#xA;&lt;li&gt;Pass text chunks to an LLM for answer synthesis&lt;br&gt;&#xA;【将图片转成摘要，和其他文本信息整合在文本粒度进行检索】[12]&lt;br&gt;&#xA;{选项2：首先应用多模态大模型（GPT4-V、LLaVA、FUYU-8b）生成图片summary。然后对文本/表格/图片summary进行向量化存入多向量检索器中。当生成应答的多模态大模型不具备时，可根据query召回原始文本/表格+图片summary。}[10]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Option 3 [24]    &lt;em&gt;[31][34]&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Survey)Instruct Tuning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/Instruct-Tuning/InstructTuningSurvey/</link>
      <pubDate>Sun, 12 Mar 2023 16:00:05 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/Instruct-Tuning/InstructTuningSurvey/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://arxiv.org/abs/2308.10792&#34;&gt;大语言模型指令微调综述&lt;/a&gt;&#xA;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/654054370&#34;&gt;一篇关于LLM指令微调的综述&lt;/a&gt;&lt;br&gt;&#xA;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/657138921&#34;&gt;[论文]大语言模型指令调优综述&lt;/a&gt;&lt;br&gt;&#xA;1xx. &lt;a href=&#34;https://blog.csdn.net/qq_41185868/article/details/132613338&#34;&gt;Paper：《Instruction Tuning for Large Language Models: A Survey—大型语言模型的指令调优的综述》翻译与解读&lt;/a&gt;&lt;br&gt;&#xA;1xx. &lt;a href=&#34;https://github.com/xiaoya-li/Instruction-Tuning-Survey&#34;&gt;Instruction Tuning for Large Language Models: A Survey&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;p&gt;【前面大部分是Instruct-Tuning， 中间一部分是Multi-modality Instruction Tuning】&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)Self-Reflective RAG</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/Agentic-RAG/RAGSelfReflective/</link>
      <pubDate>Thu, 02 Mar 2023 17:12:22 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/Agentic-RAG/RAGSelfReflective/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;cognitive-architecture-2&#34;&gt;&#xA;  Cognitive Architecture [2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#cognitive-architecture-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Cognitive architectures for RAG [1]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;crag&#34;&gt;&#xA;  CRAG&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#crag&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.15884.pdf&#34;&gt;Corrective Retrieval Augmented Generation&lt;/a&gt; Figure 2&lt;/p&gt;&#xA;&lt;h3 id=&#34;实现10&#34;&gt;&#xA;  实现[10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e7%8e%b010&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Corrective-RAG (CRAG) is a strategy for RAG that incorporates &lt;strong&gt;self-reflection / self-grading&lt;/strong&gt; on retrieved documents.&lt;/p&gt;&#xA;&lt;p&gt;In the paper here, a few steps are taken:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If at least one document exceeds the threshold for relevance, then it proceeds to generation&lt;/li&gt;&#xA;&lt;li&gt;Before generation, it performs knowledge refinement&lt;/li&gt;&#xA;&lt;li&gt;This partitions the document into &amp;ldquo;knowledge strips&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;It grades each strip, and filters our irrelevant ones&lt;/li&gt;&#xA;&lt;li&gt;If all documents fall below the relevance threshold or if the grader is unsure, then the framework seeks an additional datasource&lt;/li&gt;&#xA;&lt;li&gt;It will use web search to supplement retrieval&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;We will implement some of these ideas from scratch using LangGraph:&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)COT</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Planning/COT/</link>
      <pubDate>Wed, 08 Feb 2023 11:23:01 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Planning/COT/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;cot4&#34;&gt;&#xA;  CoT[4]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#cot4&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;CoT(Chain of Thought)&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CoT-SC(Self Consistency)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;ToT(Tree of Thoughts)&#xA;分为了Thought Decomposition，Thought Generator，State Evaluator，Search algorithms&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;GoT(Graph of Thoughts)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;AoT(Algorithm of Thoughts)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol start=&#34;4&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/654034193&#34;&gt;2023年能够解决复杂问题的思维链技术：Cot，ToT，GoT，AoT&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://github.com/zchuz/CoT-Reasoning-Survey&#34;&gt;CoT-Reasoning-Survey &lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648404176&amp;amp;idx=1&amp;amp;sn=2eafdf5426bfe1347869b9af268d4238&#34;&gt;大模型COT思维链推理的几个关键问题：从评测基准、结构变体到增强方案的系统综述 &lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s/X2lcVLFFlFgQCzacret4Vg&#34;&gt;大模型思维链推理的综述：进展、前沿和未来&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战)RAG</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/RAGPractice/</link>
      <pubDate>Sat, 31 Dec 2022 07:42:52 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/RAGPractice/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;data-processing17&#34;&gt;&#xA;  Data processing[17]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#data-processing17&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;长文本   变成   QA pair&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;规则匹配&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;利用LLM抽取&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;人工处理&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;医疗问答rag20&#34;&gt;&#xA;  医疗问答RAG[20]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8c%bb%e7%96%97%e9%97%ae%e7%ad%94rag20&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;架构&#34;&gt;&#xA;  架构&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9e%b6%e6%9e%84&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/arch.JPG&#34; alt=&#34;arch&#34; /&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;chuck&#34;&gt;&#xA;  chuck&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#chuck&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;段落&lt;/strong&gt;&#xA;句子&#xA;token&lt;/p&gt;&#xA;&lt;h3 id=&#34;数据格式&#34;&gt;&#xA;  数据格式&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%95%b0%e6%8d%ae%e6%a0%bc%e5%bc%8f&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;{&amp;ldquo;id&amp;rdquo;: xxx, &amp;ldquo;病情描述&amp;rdquo;: &amp;ldquo;xxx&amp;rdquo;,  &amp;ldquo;治疗方案&amp;rdquo;: &amp;ldquo;xxx&amp;rdquo; }&lt;/p&gt;&#xA;&lt;h3 id=&#34;改写query&#34;&gt;&#xA;  改写query&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%94%b9%e5%86%99query&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;HyDE&lt;/li&gt;&#xA;&lt;li&gt;RAG Fusion -&amp;gt; Generate Similar query&#xA;用户的查询不精准，要扩充query, 用大模型改写&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;召回模型&#34;&gt;&#xA;  召回模型&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%ac%e5%9b%9e%e6%a8%a1%e5%9e%8b&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;bert模型&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;sbert&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;2个bert模型&lt;/strong&gt;，共享参数，s1,s2向量化后做&lt;strong&gt;相似度&lt;/strong&gt;计算&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;速度快&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;相似度&#xA;欧式距离&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;在百万语料上训练&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;语料格式&lt;/strong&gt;&lt;br&gt;&#xA;[s1][s2] 0 - 无关&#xA;[s1][s2] 1-类似&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;根据query, 召回id和value整条记录&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;排序模型&#34;&gt;&#xA;  排序模型&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%8e%92%e5%ba%8f%e6%a8%a1%e5%9e%8b&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;bert模型&#xA;&lt;ul&gt;&#xA;&lt;li&gt;1个bert模型&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;速度慢&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;格式&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;query[sep]s2  -&amp;gt; 经过softmax，产生2分类，0-1&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;也要训练&#xA;&lt;ul&gt;&#xA;&lt;li&gt;同&lt;strong&gt;召回模型训练方式&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;索引方式&#34;&gt;&#xA;  索引方式&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%b4%a2%e5%bc%95%e6%96%b9%e5%bc%8f&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;树索引&lt;/li&gt;&#xA;&lt;li&gt;知识图谱的索引&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;大模型&#34;&gt;&#xA;  大模型&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%a7%e6%a8%a1%e5%9e%8b&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;综合归纳的作用&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;xxx&#34;&gt;&#xA;  xxx&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#xxx&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;17&#34;&gt;&#xA;&lt;li&gt;&amp;laquo;大模型结合 RAG 构建客服场景自动问答系统&amp;raquo;  NVIDIA大模型日系列活动&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;医疗问答&#34;&gt;&#xA;  医疗问答&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8c%bb%e7%96%97%e9%97%ae%e7%ad%94&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;20&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1fW421P7u6?p=5&#34;&gt;基于百万语料的医疗RAG项目&lt;/a&gt; v&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Retrievers</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Langchain/Retrievers/</link>
      <pubDate>Sat, 31 Dec 2022 06:25:34 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Langchain/Retrievers/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;langchain-retrievers10&#34;&gt;&#xA;  Langchain Retrievers[10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#langchain-retrievers10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;multiqueryretriever&#34;&gt;&#xA;  MultiQueryRetriever&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#multiqueryretriever&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;The MultiQueryRetriever automates the process of prompt tuning by using an LLM to &lt;strong&gt;generate multiple queries from different perspectives for a given user input query&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;contextual-compression&#34;&gt;&#xA;  Contextual compression&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#contextual-compression&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h3 id=&#34;ensemble-retriever&#34;&gt;&#xA;  Ensemble Retriever&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#ensemble-retriever&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;The EnsembleRetriever takes a list of retrievers as input and ensemble the results of their get_relevant_documents() methods and &lt;strong&gt;rerank the results based on the Reciprocal Rank Fusion algorithm&lt;/strong&gt;.&#xA;The most common pattern is to &lt;strong&gt;combine a sparse retriever (like BM25) with a dense retriever (like embedding similarity)&lt;/strong&gt;, because their strengths are complementary. It is also known as “hybrid search”.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Fine-Tuning 时机</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/%E5%AE%9E%E8%B7%B5/FineTuningWhen/</link>
      <pubDate>Wed, 28 Dec 2022 03:06:21 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/%E5%AE%9E%E8%B7%B5/FineTuningWhen/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;何时进行微调1&#34;&gt;&#xA;  何时进行微调[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bd%95%e6%97%b6%e8%bf%9b%e8%a1%8c%e5%be%ae%e8%b0%831&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;语言模型（LLM）可以通过至少两种方式学习新知识：权重更新（例如预训练或微调）或提示（例如检索增强生成，RAG）。模型的权重就像长期记忆，而提示就像短期记忆。这个OpenAI Cookbook给出了一个有用的比喻：当你对模型进行微调时，就像是在离考试还有一周的时候准备复习。当你通过提示（例如检索）向提示中插入知识时，就像是在有开放笔记的考试中。&lt;/p&gt;&#xA;&lt;p&gt;基于这一点，&lt;strong&gt;不建议使用微调来教授LLM新的知识或事实回忆&lt;/strong&gt;；OpenAI的John Schulman在一次讲话中指出，微调可能会&lt;strong&gt;增加虚构&lt;/strong&gt;。微调&lt;strong&gt;更适合教授专门的任务&lt;/strong&gt;，但应与提示或RAG相对比。正如这里所讨论的，对于具有丰富示例和/或缺乏上下文学习能力的LLM来说，微调对于定义明确的任务可能是有帮助的。这篇Anyscale博客很好地总结了这些观点：&lt;strong&gt;微调是为形式而非事实&lt;/strong&gt;[3]。&lt;/p&gt;&#xA;&lt;h1 id=&#34;what-4&#34;&gt;&#xA;  what [4]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#what-4&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;这是一个很好的问题。我大致将微调类比为人的专业知识：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;用文字描述一个任务 ~= 零样本提示&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;给出解决任务的示例 ~= 少样本提示&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;允许人们练习任务 ~= 微调&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;考虑到这个比喻，令人惊奇的是我们有了可以仅通过提示就能在许多任务上达到高水平准确性的模型，但我也预计达到顶级性能可能需要微调，特别是在具有明确定义的具体任务的应用中，在这些任务中我们可以收集大量数据并在其上进行“练习”。&lt;/p&gt;&#xA;&lt;p&gt;这可能是一个需要牢记的&lt;strong&gt;粗略图景&lt;/strong&gt;。&lt;strong&gt;小型模型&lt;/strong&gt;无法进行上下文学习，并且从提示工程中受益甚少，但根据任务的难度，&lt;strong&gt;仍然有可能将它们微调为表现良好的专家&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;需要注意的是，所有这些都还是非常新颖的。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/fine-tuning.jpg&#34; alt=&#34;fine-tuning&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;common-use-cases2&#34;&gt;&#xA;  Common use cases[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#common-use-cases2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;微调可以改善结果的一些常见&lt;strong&gt;用例&lt;/strong&gt;包括：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;设定风格、语气、格式或其他定性因素&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;提高生成所需输出的可靠性&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;纠正无法按照复杂提示要求执行的问题&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;以特定方式处理许多边缘情况&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;执行难以用提示清晰表达的新技能或任务&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;从较高层面来看，这些情况下微调更容易实现“&lt;strong&gt;展示而非告诉&lt;/strong&gt;”的效果。在接下来的部分中，我们将探讨如何为微调设置数据以及各种示例，这些示例中微调改善了基线模型的性能。&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/&#34;&gt;Using LangSmith to Support Fine-tuning&lt;/a&gt;&#xA;&lt;a href=&#34;https://colab.research.google.com/drive/1tpywvzwOS74YndNXhI8NUaEfPeqOc7ub?usp=sharing&amp;amp;ref=blog.langchain.dev&#34;&gt;colab&lt;/a&gt;   LANGCHAIN_API_KEY&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/fine-tuning&#34;&gt;Fine-tuning&lt;/a&gt;  openai ***&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts&#34;&gt;Fine tuning is for form, not facts&lt;/a&gt; ***&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/karpathy/status/1655994367033884672&#34;&gt;Andrej Karpathy twitter&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(实战)PEFT 概述</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/FineTuningPEFT/</link>
      <pubDate>Tue, 20 Dec 2022 11:25:22 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/FineTuningPEFT/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;huggingface--peft中的任务1&#34;&gt;&#xA;  Huggingface  PEFT中的任务[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#huggingface--peft%e4%b8%ad%e7%9a%84%e4%bb%bb%e5%8a%a11&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;class TaskType(str, enum.Enum):&#xD;&#xA;    SEQ_CLS = &amp;#34;SEQ_CLS&amp;#34;  # 3. 序列分类任务&#xD;&#xA;    SEQ_2_SEQ_LM = &amp;#34;SEQ_2_SEQ_LM&amp;#34;  # 2. 条件生成任务&#xD;&#xA;    CAUSAL_LM = &amp;#34;CAUSAL_LM&amp;#34;  #  1. 因果语言建模任务&#xD;&#xA;    TOKEN_CLS = &amp;#34;TOKEN_CLS&amp;#34;  #  4. Token 分类任务&#xD;&#xA;    QUESTION_ANS = &amp;#34;QUESTION_ANS&amp;#34;&#xD;&#xA;    FEATURE_EXTRACTION = &amp;#34;FEATURE_EXTRACTION&amp;#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;1-因果语言建模任务causal-language-modeling&#34;&gt;&#xA;  1. 因果语言建模任务（Causal Language Modeling）&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#1-%e5%9b%a0%e6%9e%9c%e8%af%ad%e8%a8%80%e5%bb%ba%e6%a8%a1%e4%bb%bb%e5%8a%a1causal-language-modeling&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;因果语言建模任务（CLM），在这种建模方法中，模型试图预测给定上下文中的下一个单词，该上下文通常包括在当前单词之前的所有单词。&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-条件生成任务conditional-generation&#34;&gt;&#xA;  2. 条件生成任务（Conditional Generation）&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#2-%e6%9d%a1%e4%bb%b6%e7%94%9f%e6%88%90%e4%bb%bb%e5%8a%a1conditional-generation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;条件生成任务（Conditional Generation），根据给定的输入（可能是文本、图片等）生成符合条件的输出。&#xA;条件生成的应用包括但不限于机器翻译、文本摘要、图像描述等。这些任务通常需要模型在输入和输出之间建立复杂的映射关系。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;因果语言建模任务  vs.  条件生成任务&#xA;因果语言建模主要关注于生成连贯、自然的文本，而条件生成关注于生成满足特定条件或任务要求的文本。这两种建模方法在某些场景下可能会互相使用和结合，以实现更复杂的自然语言处理任务。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;h3 id=&#34;3-序列分类任务sequence-classification&#34;&gt;&#xA;  3. 序列分类任务（Sequence Classification）&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#3-%e5%ba%8f%e5%88%97%e5%88%86%e7%b1%bb%e4%bb%bb%e5%8a%a1sequence-classification&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;序列分类（Sequence Classification），对整个句子进行分类。如: 获取评论的情绪，检测电子邮件是否为垃圾邮件，确定句子在语法上是否正确或两个句子在逻辑上是否相关等&lt;/p&gt;&#xA;&lt;h3 id=&#34;4-token-分类任务token-classification&#34;&gt;&#xA;  4. Token 分类任务（Token Classification）&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#4-token-%e5%88%86%e7%b1%bb%e4%bb%bb%e5%8a%a1token-classification&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Token 分类任务（Token Classification），对句子中的每个词进行分类。如: 识别句子的语法成分（名词、动词、形容词）或命名实体（人、地点、组织）。&lt;/p&gt;</description>
    </item>
    <item>
      <title>向量数据库</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Application/VectorStore/</link>
      <pubDate>Sun, 27 Nov 2022 18:19:59 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Application/VectorStore/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;向量数据库&#34;&gt;&#xA;  向量数据库&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%90%91%e9%87%8f%e6%95%b0%e6%8d%ae%e5%ba%93&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;国产&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Milvus&lt;/li&gt;&#xA;&lt;li&gt;Tencent&lt;/li&gt;&#xA;&lt;li&gt;zilliz cloud&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;国外&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Pinecone&lt;/li&gt;&#xA;&lt;li&gt;FAISS&#xA;[ANN]&lt;/li&gt;&#xA;&lt;li&gt;Chroma&lt;/li&gt;&#xA;&lt;li&gt;Weaviate&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;向量数据库-索引方式-7&#34;&gt;&#xA;  向量数据库-索引方式 [7]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%90%91%e9%87%8f%e6%95%b0%e6%8d%ae%e5%ba%93-%e7%b4%a2%e5%bc%95%e6%96%b9%e5%bc%8f-7&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/index.jpg&#34; alt=&#34;index&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;向量的相似度算法3&#34;&gt;&#xA;  向量的相似度算法[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%90%91%e9%87%8f%e7%9a%84%e7%9b%b8%e4%bc%bc%e5%ba%a6%e7%ae%97%e6%b3%953&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Cosine Similarity *&#xA;余弦&lt;/li&gt;&#xA;&lt;li&gt;Dot Product *&lt;/li&gt;&#xA;&lt;li&gt;Squared Euclidean (L2-Squared) *&#xA;欧式距离&lt;/li&gt;&#xA;&lt;li&gt;Manhattan (L1 Norm or Taxicab Distance) *&lt;/li&gt;&#xA;&lt;li&gt;Hamming *&lt;/li&gt;&#xA;&lt;li&gt;ANN&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;比较4&#34;&gt;&#xA;  比较[4]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%af%94%e8%be%834&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Similarity Metric&lt;/th&gt;&#xA;          &lt;th&gt;Vector properties considered&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Euclidean distance&lt;/td&gt;&#xA;          &lt;td&gt;Magnitudes and direction&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Cosine similarity&lt;/td&gt;&#xA;          &lt;td&gt;Only direction&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Dot product similarity&lt;/td&gt;&#xA;          &lt;td&gt;Magnitudes and direction&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/476025527&#34;&gt;云原生向量数据库Milvus扫盲，看完这篇就够了&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/477231485&#34;&gt;云原生向量数据库Milvus（二）-数据与索引的处理流程、索引类型及Schema&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://weaviate.io/blog/distance-metrics-in-vector-search?ref=blog.langchain.dev&#34;&gt;Distance Metrics in Vector Search&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.pinecone.io/learn/vector-similarity/&#34;&gt;Vector Similarity Explained&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;xxx&lt;/li&gt;&#xA;&lt;li&gt;xxx&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.modb.pro/db/1694527960317513728&#34;&gt;向量数据库（第 1 部分）：每个数据库有何不同？&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://cloud.tencent.com/developer/article/2352088&#34;&gt;微信向量检索分析一体化数仓探索：OLAP For Embedding&lt;/a&gt; ***&#xA;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/646832642&#34;&gt;Meta向量数据库Faiss介绍&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战) [OpenAI]Function Call</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/FunctionCall/</link>
      <pubDate>Wed, 16 Nov 2022 15:32:04 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/FunctionCall/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;function-call&#34;&gt;&#xA;  Function Call&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#function-call&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;调用顺序--0-12&#34;&gt;&#xA;  调用顺序  [0] [1][2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%b0%83%e7%94%a8%e9%a1%ba%e5%ba%8f--0-12&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Function Calling 整个功能的调用顺序大致如下&#xA;&lt;ul&gt;&#xA;&lt;li&gt;声明函数：定义当前函数的名称，描述，以及对应的参数信息，并请求对应的接口；&lt;/li&gt;&#xA;&lt;li&gt;解析函数参数：接受对应的接口返回，并解析对应的函数参数信息；&lt;/li&gt;&#xA;&lt;li&gt;执行函数：根据对应的参数信息调用本地函数；&lt;/li&gt;&#xA;&lt;li&gt;上报结果：将本地函数执行的结果上报给 Chat 接口；&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/functioncall1.png&#34; alt=&#34;functioncall1&#34; /&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;代码-2&#34;&gt;&#xA;  代码 [2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bb%a3%e7%a0%81-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h3 id=&#34;goal&#34;&gt;&#xA;  goal&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#goal&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;The goal of the OpenAI Function APIs is to more reliably return valid and useful function calls than a generic text completion or chat API.&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol start=&#34;0&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;http://lihuaxi.xjx100.cn/news/1382737.html&#34;&gt;大模型开发(十一)：Chat Completions模型的Function calling功能详解&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.duidaima.com/Group/Topic/OtherTools/13709&#34;&gt;如何使用Chat Completions接口的函数调用功能&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/Lvbaby_/article/details/131892482&#34;&gt;OpenAI开发系列（十一）：Function calling功能的实际应用流程与案例解析&lt;/a&gt;   代码  流程图&#xA;&lt;a href=&#34;https://github.com/www6v/AIGC/tree/master/basic/Function-calling&#34;&gt;代码&lt;/a&gt;  git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/Lvbaby_/article/details/131933871&#34;&gt;OpenAI开发系列（十三）：利用Function calling功能开发基于大模型的实时天气查询助手&lt;/a&gt; 未&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理) Agent Guide &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Overview/AgentGuide/</link>
      <pubDate>Wed, 02 Nov 2022 10:55:27 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Overview/AgentGuide/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;agent-guide&#34;&gt;&#xA;  Agent Guide&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agent-guide&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/A-Visual-Guide-to-LLM-Agents-1ecbfe21108480eeb5f9f0f6a31e821e?pvs=4&#34;&gt;(原理) Agent Guide&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Google)Deep Research</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Deep-Research/%E5%AE%9E%E7%8E%B0/DeepresearchGoogle/</link>
      <pubDate>Tue, 26 Mar 2024 12:13:50 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Deep-Research/%E5%AE%9E%E7%8E%B0/DeepresearchGoogle/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;googledeep-research&#34;&gt;&#xA;  (Google)Deep Research&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#googledeep-research&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Google-Deep-Research-220bfe21108480ce9348f628408a85e9?source=copy_link&#34;&gt;(Google)Deep Research&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战)PEFT P-Tuning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Soft-Prompt/PEFTPtuningPractice/</link>
      <pubDate>Sun, 28 Jan 2024 19:04:02 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Soft-Prompt/PEFTPtuningPractice/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h3 id=&#34;最佳实践1&#34;&gt;&#xA;  最佳实践[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b51&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;要看losss, 也要看&lt;strong&gt;业务的loss&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;生成模型常用的评价方法&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;BLEU 能评估&lt;/strong&gt;流畅度**&lt;/li&gt;&#xA;&lt;li&gt;结果都是流畅的前提下，ROUGE 反应参照句中多少内容被生成的句子包含（召回）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;垂直模型&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;stf之后失去通用能力&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;要有&lt;strong&gt;通用能力&lt;/strong&gt;, 需要&lt;strong&gt;pre-train和STF中都融入通用的语料&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;每个模型的学习率lr不一样&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;chatglm的学习率&#xA;LR=2e-2&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;学习率&#34;&gt;&#xA;  学习率&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ad%a6%e4%b9%a0%e7%8e%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;改的&lt;strong&gt;特别大&lt;/strong&gt;&#xA;模型训练的时候会&lt;strong&gt;震荡&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;改的&lt;strong&gt;特别小&lt;/strong&gt;&#xA;模型训练的时候会&lt;strong&gt;收敛非常慢&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;《13-基于 ChatGLM2的 Fine-tuning 实战》 AI 大模型全栈工程师培养计划  2期&#xA;&lt;a href=&#34;https://github.com/www6v/fine-tuning-lab/blob/agiclass-v1/chatglm/train_pt2.sh&#34;&gt;train_pt2.sh&lt;/a&gt; git   基于法律文本的chatglm的p-tuning&#xA;&lt;a href=&#34;https://github.com/www6v/fine-tuning-lab/blob/agiclass-v1/chatglm2/train_pt2.sh&#34;&gt;train_pt2.sh&lt;/a&gt; git   基于法律文本的chatglm-2的P-tuning v2&#xA;&lt;a href=&#34;https://github.com/www6v/fullStackLLM/blob/master/08-fine-tuning/peft/index.ipynb&#34;&gt;课件&lt;/a&gt;&#xA;bili有相关的总结的视频&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Fine Tuning-Bert</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/%E5%AE%9E%E8%B7%B5/FineTuningBert/</link>
      <pubDate>Fri, 26 Jan 2024 12:06:45 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/%E5%AE%9E%E8%B7%B5/FineTuningBert/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;基于bert的二分类&#34;&gt;&#xA;  基于bert的二分类&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%9f%ba%e4%ba%8ebert%e7%9a%84%e4%ba%8c%e5%88%86%e7%b1%bb&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;代码 - 全参FT,非PEFT&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; datasets&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; datasets &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; load_dataset&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; datasets &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; load_metric&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; transformers &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; AutoTokenizer, AutoModel&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; transformers &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; AutoModelForSequenceClassification&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; transformers &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; TrainingArguments&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; transformers &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Trainer&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; transformers&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; transformers &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; DataCollatorWithPadding&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.metrics &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; f1_score&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; os&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch.nn &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; nn&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;SEED&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# ALBERT是一种压缩过的BERT&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;MODEL_NAME &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;albert-base-v2&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;DATASET_NAME &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;glue&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 一组NLP评测任务&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;DATASET_TASK &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;mrpc&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# MRPC 是其中一个子任务 -- Microsoft Research Paraphrase Corpus&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 在Bert的基础上加了一个线性分类器&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;MyClassifier&lt;/span&gt;(torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Module):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;__init__&lt;/span&gt;(self, backbone):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        super()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;__init__&lt;/span&gt;()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bert_encoder &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; backbone&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linear &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;768&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;compute_loss&lt;/span&gt;(self, logits, labels):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        loss_fct &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;CrossEntropyLoss()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; loss_fct(logits, labels)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;forward&lt;/span&gt;(self, input_ids, attention_mask,labels&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bert_encoder(input_ids&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;input_ids, attention_mask&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;attention_mask)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; output&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;last_hidden_state[:, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, :]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linear(output)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; labels &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compute_loss(output, labels)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; loss, output&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; output&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 加载数据集对应的评估方法&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;glue_metric &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; datasets&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load_metric(DATASET_NAME, DATASET_TASK)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;compute_metrics&lt;/span&gt;(eval_pred):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    logits, labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; eval_pred&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    predictions &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argmax(logits, axis&lt;span style=&#34;color:#f92672&#34;&gt;=-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; glue_metric&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compute(predictions&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;predictions, references&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;labels)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 加载数据集&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;raw_datasets &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; load_dataset(DATASET_NAME,DATASET_TASK)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 训练集&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;raw_train_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; raw_datasets[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;train&amp;#34;&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 验证集&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;raw_valid_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; raw_datasets[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;validation&amp;#34;&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;columns &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; raw_train_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;column_names&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 设置随机种子&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;transformers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_seed(SEED)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 定义tokenizer&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tokenizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AutoTokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_pretrained(MODEL_NAME)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 定义数据处理函数，把原始数据转成input_ids, attention_mask, labels&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;process_fn&lt;/span&gt;(examples):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    inputs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tokenizer(examples[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sentence1&amp;#34;&lt;/span&gt;], examples[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sentence2&amp;#34;&lt;/span&gt;], truncation&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;, max_length&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    examples[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;input_ids&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; inputs[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;input_ids&amp;#34;&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    examples[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;attention_mask&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; inputs[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;attention_mask&amp;#34;&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    examples[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;labels&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; examples[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;label&amp;#34;&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; examples&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tokenized_train_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; raw_train_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    process_fn,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    batched&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    remove_columns&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;columns&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tokenized_valid_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; raw_valid_dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    process_fn,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    batched&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    remove_columns&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;columns&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 定义数据校准器（自动生成batch）&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;collater &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataCollatorWithPadding(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tokenizer, return_tensors&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pt&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 定义模型 -- 其实Transformer可以直接用AutoModelForSequenceClassification&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 我手工写了分类器层，为了方便大家理解什么叫在Transformer上面做分类任务&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;backbone &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AutoModel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_pretrained(MODEL_NAME)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MyClassifier(backbone)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 定义训练参数&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;training_args &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; TrainingArguments(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    output_dir&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;./output&amp;#34;&lt;/span&gt;,        &lt;span style=&#34;color:#75715e&#34;&gt;# checkpoint保存路径&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    evaluation_strategy&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;steps&amp;#34;&lt;/span&gt;,    &lt;span style=&#34;color:#75715e&#34;&gt;# 每N步做一次eval&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    overwrite_output_dir&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    num_train_epochs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,             &lt;span style=&#34;color:#75715e&#34;&gt;# 训练epoch数&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    per_device_train_batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,  &lt;span style=&#34;color:#75715e&#34;&gt;# 每张卡的batch大小&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    gradient_accumulation_steps&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,   &lt;span style=&#34;color:#75715e&#34;&gt;# 累加几个step做一次参数更新&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    per_device_eval_batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,  &lt;span style=&#34;color:#75715e&#34;&gt;# evaluation batch size&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    logging_steps&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;,             &lt;span style=&#34;color:#75715e&#34;&gt;# 每20步eval一次&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    save_steps&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;,                &lt;span style=&#34;color:#75715e&#34;&gt;# 每20步保存一个checkpoint&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    learning_rate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2e-5&lt;/span&gt;,             &lt;span style=&#34;color:#75715e&#34;&gt;# 学习率&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    warmup_ratio&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;,               &lt;span style=&#34;color:#75715e&#34;&gt;# 预热（可选）&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 定义训练器&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;trainer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Trainer(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    model&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;model, &lt;span style=&#34;color:#75715e&#34;&gt;# 待训练模型&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    args&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;training_args, &lt;span style=&#34;color:#75715e&#34;&gt;# 训练参数&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    data_collator&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;collater, &lt;span style=&#34;color:#75715e&#34;&gt;# 数据校准器&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    train_dataset&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tokenized_train_dataset, &lt;span style=&#34;color:#75715e&#34;&gt;# 训练集&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    eval_dataset&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tokenized_valid_dataset, &lt;span style=&#34;color:#75715e&#34;&gt;# 验证集&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    compute_metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;compute_metrics, &lt;span style=&#34;color:#75715e&#34;&gt;# 评价指标&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 禁用wandb（与huggingface.co同步的机制）&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;environ[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;WANDB_DISABLED&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 开始训练&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;trainer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;train()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/www6v/fullStackLLM/blob/master/08-fine-tuning/huggingface/index.ipynb&#34;&gt;Bert fine-tuning 二分类&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(框架) Qanything</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/framework/RAGQanything/</link>
      <pubDate>Mon, 19 Jun 2023 10:14:53 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/framework/RAGQanything/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;qanything&#34;&gt;&#xA;  QAnything&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#qanything&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;arch1&#34;&gt;&#xA;  Arch[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#arch1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/netease-youdao/QAnything/raw/master/docs/images/qanything_arch.png&#34; alt=&#34;Arch&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;索引（indexing）&#xA;通过Embedding为每一个文本块生成一个向量表示，用于计算&lt;strong&gt;文本向量&lt;/strong&gt;和&lt;strong&gt;问题向量&lt;/strong&gt;之间的&lt;strong&gt;相似度&lt;/strong&gt;。创建索引将原始文本块和Embedding向量以键值对的形式存储，以便将来进行快速和频繁的搜索。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;检索（Retrieval）&#xA;使用Embedding模型将用户输入问题转换为向量，计算问题的Embedding向量和语料库中文本块Embedding向量之间的相似度，选择&lt;strong&gt;相似度最高的前K个文档块&lt;/strong&gt;作为当前问题的增强上下文信息。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;生成（Generation）&#xA;将检索得到的前K个文本块和用户问题一起送进大模型，让大模型基于给定的文本块来回答用户的问题。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;1st-retrievalembedding1&#34;&gt;&#xA;  1st Retrieval（embedding）[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#1st-retrievalembedding1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Bcembedding模型 [3]&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;中英双语和跨语种能力&lt;/li&gt;&#xA;&lt;li&gt;多领域覆盖&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Embedding 可以给出一个得分，但是这个得分描述的更多的是&lt;strong&gt;相似性&lt;/strong&gt;。Embedding本质上是一个&lt;strong&gt;双编码器&lt;/strong&gt;，两个文本在模型内部没有任何信息交互。只在最后计算两个向量的余弦相似度时才进行唯一一次交互。所以Embedding检索只能把&lt;strong&gt;最相似的&lt;/strong&gt;文本片段给你，&lt;strong&gt;没有&lt;/strong&gt;能力来判断候选文本和query之间的&lt;strong&gt;相关性&lt;/strong&gt;。但是&lt;strong&gt;相似又不等于相关&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;【embedding -&amp;gt; 相似性】&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/embedding.png&#34; alt=&#34;embedding&#34; /&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;2nd-retrievalrerank1&#34;&gt;&#xA;  2nd Retrieval（rerank）[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#2nd-retrievalrerank1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Rerank [3]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Rerank本质是一个&lt;strong&gt;Cross-Encoder&lt;/strong&gt;的模型。Cross-Encoder能让两个文本片段一开始就在BERT模型各层中通过self-attention进行交互。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/reranker.png&#34; alt=&#34;reranker&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;【rerank -&amp;gt; 相关性】&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;qanything-1&#34;&gt;&#xA;  QAnything&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#qanything-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything&#34;&gt;QAnything Repo&lt;/a&gt; git&lt;/li&gt;&#xA;&lt;li&gt;xxx&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1HF4m1w7rY/&#34;&gt;有道QAnything背后的故事：关于RAG的一点经验分享&lt;/a&gt; V&lt;br&gt;&#xA;&lt;a href=&#34;https://mp.weixin.qq.com/s/FUex1Q984-IhQ-FoLZTf5Q&#34;&gt;有道QAnything背后的故事&amp;mdash;关于RAG的一点经验分享&lt;/a&gt;   文字版&lt;br&gt;&#xA;[公众号有其他文章]&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&amp;amp;mid=2247489671&amp;amp;idx=1&amp;amp;sn=564a232c3c7919c70a7a1cf5efa77628&#34;&gt;前沿重器[45] RAG开源项目Qanything源码阅读1-概述+服务&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理&amp;实战)AutoGen &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Multi-agent/AgentAutogen/</link>
      <pubDate>Mon, 05 Jun 2023 21:37:46 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Multi-agent/AgentAutogen/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;autogen&#34;&gt;&#xA;  AutoGen&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#autogen&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/AutoGen-1debfe211084807b9163d8c6a0162307?pvs=4&#34;&gt;(原理&amp;amp;实战)AutoGen&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Work)RAG 故障点 &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/RAGFailure/</link>
      <pubDate>Tue, 09 May 2023 18:28:50 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/RAGFailure/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;rag-故障点&#34;&gt;&#xA;  RAG 故障点&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag-%e6%95%85%e9%9a%9c%e7%82%b9&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/RAG-1e2bfe21108480e4a0c7ec8ece4f18da?pvs=4&#34;&gt;RAG 故障点&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG 优化 *</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/RAGOptimize/</link>
      <pubDate>Tue, 09 May 2023 18:28:50 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/RAGOptimize/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;朴素rag-embedding&#34;&gt;&#xA;  朴素RAG Embedding&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9c%b4%e7%b4%a0rag-embedding&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;embedding-召回方案及局限性1&#34;&gt;&#xA;  Embedding 召回方案及局限性[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#embedding-%e5%8f%ac%e5%9b%9e%e6%96%b9%e6%a1%88%e5%8f%8a%e5%b1%80%e9%99%90%e6%80%a71&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;召回&lt;strong&gt;精度低&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;粒度过粗&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;不支持条件查询/统计&lt;/li&gt;&#xA;&lt;li&gt;不能替代信息提取&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;解决方案&#34;&gt;&#xA;  解决方案&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;问题理解——准确识别&lt;strong&gt;用户意图&lt;/strong&gt;(传统NLP)  [2]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;基于&lt;strong&gt;关键词Embedding&lt;/strong&gt;的入库和搜索 [2]&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;关键词提取&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;实现信息抽取（Information Extraction，IE）&#xA;&lt;ul&gt;&#xA;&lt;li&gt;实体关系三元组抽取(RE, Relation Extraction )&lt;/li&gt;&#xA;&lt;li&gt;命名实体识别(NER, Name-Entity Recognition)&lt;/li&gt;&#xA;&lt;li&gt;事件抽取(EE, Event Extraction)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;基于 LLM 提取 [不推荐]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;结果不准确、开销也大&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;传统 NLP 方法提取&lt;/strong&gt;[推荐]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;名词短语提取与整合&lt;/li&gt;&#xA;&lt;li&gt;依存分析&lt;/li&gt;&#xA;&lt;li&gt;成分句法分析&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;总结&#xA;从&lt;strong&gt;完整语句的 Embedding&lt;/strong&gt;，切换为&lt;strong&gt;关键词 Embedding&lt;/strong&gt;：&lt;/li&gt;&#xA;&lt;li&gt;优势&#xA;&lt;ul&gt;&#xA;&lt;li&gt;相比传统 Embedding，大幅提升&lt;strong&gt;召回精准度&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;使用传统 NLP 在专项问题处理上，相比 LLM 提供更好的精度和性能。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;知识库存储选型&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Vector Store&#xA;&lt;ul&gt;&#xA;&lt;li&gt;分片:  区分&lt;strong&gt;层级结构&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Relational Database&lt;/li&gt;&#xA;&lt;li&gt;Graph Database&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;图数据检索&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;行业问答3&#34;&gt;&#xA;  行业问答[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%a1%8c%e4%b8%9a%e9%97%ae%e7%ad%943&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;挑战&#34;&gt;&#xA;  挑战&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%8c%91%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;版面复杂多样&lt;/li&gt;&#xA;&lt;li&gt;文本分块&#xA;&lt;strong&gt;存在知识点被分割、不完整的情况&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;多因素影响内容召回效果&#xA;&lt;ul&gt;&#xA;&lt;li&gt;例如：文档内容相似度高(专业文档细分领域、版本迭代等)；&lt;/li&gt;&#xA;&lt;li&gt;通用的&lt;strong&gt;向量相似度算法&lt;/strong&gt;效果不好(问题与问题匹配 VS问题与答案匹配)；&lt;/li&gt;&#xA;&lt;li&gt;召回率受文档库增大而降低&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;优化&#34;&gt;&#xA;  优化&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bc%98%e5%8c%96&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;向量化上的优化&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)SFT Scaling</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/DataSFTScaling/</link>
      <pubDate>Wed, 26 Apr 2023 16:55:39 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/DataSFTScaling/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;论文地址&#xA;《When Scaling Meets LLM Fine-tuning: The Effect of Data, Model and Fine-tuning Method》&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;摘要1&#34;&gt;&#xA;  摘要[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%91%98%e8%a6%811&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;这篇论文研究了大型语言模型（LLMs）的微调（finetuning）问题，尤其是在不同规模因素下的微调性能。作者探讨了包括LLM模型大小、预训练数据大小、新微调参数大小和微调数据大小在内的多个因素，并考虑了两种微调方法：全模型微调（FMT）和参数高效微调（PET，包括prompt tuning和LoRA）。研究发现LLM微调遵循基于&lt;strong&gt;功率的乘法联合规模法则&lt;/strong&gt;，&lt;strong&gt;LLM模型规模的增加对微调性能的提升大于预训练数据规模的增加，而PET参数规模的增加通常效果不佳&lt;/strong&gt;。此外，&lt;strong&gt;微调方法的选择高度依赖于具体任务和微调数据&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;【 功率的乘法联合规模法则: 微调数据数量 &amp;lt;&amp;ndash;&amp;gt; xxx】&lt;br&gt;&#xA;【模型大小(标题里的Model ) &amp;gt; 预训练数据(标题里的Data),   PET参数(标题里的Fine-tuning Method) 无效】&lt;br&gt;&#xA;【微调方法的选择高度依赖于具体任务和微调数据】&lt;/p&gt;&#xA;&lt;h1 id=&#34;实验方法1&#34;&gt;&#xA;  实验方法[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e9%aa%8c%e6%96%b9%e6%b3%951&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;实验基于两组预训练的双语LLMs（英语&amp;amp;德语，英语&amp;amp;中文），模型大小从1B到16B。作者在WMT机器翻译（英语-德语、英语-中文）和多语言摘要（英语、德语、法语和西班牙语）任务上进行了大规模研究，最多使用20M微调示例。实验设置包括：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;下游任务&lt;/strong&gt;：选择机器翻译和多语言摘要作为微调的下游任务。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;LLMs和预训练&lt;/strong&gt;：采用解码器仅Transformer模型，使用修改后的UL2目标进行训练。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;微调设置&lt;/strong&gt;：研究了三种微调方法（FMT、Prompt和LoRA），并探索了四种不同的规模因素。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;评估&lt;/strong&gt;：使用基于token级别的困惑度（PPL）选择最佳检查点进行评估，并使用BLEURT和RougeL评估生成质量。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;结论1&#34;&gt;&#xA;  结论[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%bb%93%e8%ae%ba1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;提出了一个乘法联合规模法则来描述微调数据大小和其他规模因素之间的规模关系。&lt;/li&gt;&#xA;&lt;li&gt;LLM模型规模的增加对微调性能的提升大于预训练数据规模的增加。&lt;/li&gt;&#xA;&lt;li&gt;PET参数规模的增加对于LoRA和Prompt的效果有限，且有时甚至会导致反向规模效应。&lt;/li&gt;&#xA;&lt;li&gt;微调方法的选择对于下游任务来说并不简单，需要根据任务特性和微调数据的可用性来决定。&lt;/li&gt;&#xA;&lt;li&gt;微调可能会提高模型对相关任务的零样本泛化能力，尤其是当基础LLM较大时，Prompt和LoRA通常比FMT表现得更好。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;作者指出，尽管研究提供了有价值的见解，但也存在一些局限性，如联合规模法则主要基于封闭生成任务的实证结果，缺乏理论基础。未来的工作将扩展到多模态LLMs，探索微调数据质量的影响，并考虑开放和创造性的生成任务以及微调的多任务设置。&lt;/p&gt;&#xA;&lt;h1 id=&#34;重要结论2&#34;&gt;&#xA;  重要结论[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e9%87%8d%e8%a6%81%e7%bb%93%e8%ae%ba2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;作者们探讨了大型语言模型（LLMs）在微调（finetuning）过程中不同规模因素对性能的影响。以下是论文的一些重要结论及其对“SCALING”概念的解释：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;乘法联合缩放法则&lt;/strong&gt;：作者提出了一个基于&lt;strong&gt;乘法的联合缩放法则（multiplicative joint scaling law）&lt;/strong&gt;，用于描述微调数据大小与其他缩放因素（如LLM模型大小、预训练数据大小、PET参数大小）之间的关系。这个法则表明，&lt;strong&gt;微调性能与这些因素的乘法组合有关&lt;/strong&gt;，而不是简单的加法关系。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;模型大小对微调的影响&lt;/strong&gt;：研究发现，&lt;strong&gt;增加LLM模型的大小对微调性能的提升比增加预训练数据的大小更为显著&lt;/strong&gt;。这表明在有限资源下，&lt;strong&gt;优先考虑扩大模型规模而不是数据规模&lt;/strong&gt;，可能会带来更好的微调效果。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;参数高效微调（PET）的局限性&lt;/strong&gt;：尽管PET方法（如prompt tuning和LoRA）旨在通过优化少量参数来提高性能，但研究发现&lt;strong&gt;增加PET参数的大小对于微调性能的提升效果有限，有时甚至会出现反向缩放现象&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;任务和数据依赖性&lt;/strong&gt;：微调的缩放特性高度依赖于具体任务和数据。这意味着&lt;strong&gt;没有一种通用的最优微调方法&lt;/strong&gt;，选择哪种微调方法需要根据下游任务的特性和可用的微调数据量来决定。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;微调对零样本泛化能力的影响&lt;/strong&gt;：尽管微调通常是为了提高特定任务的性能，但研究发现，基于LLM的微调仍然可以促进对相关任务的零样本泛化能力。特别是PET方法在保留模型的泛化能力方面表现更好。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;微调数据量的临界点&lt;/strong&gt;：论文中还讨论了不同微调方法之间的临界点，即在特定的微调数据量下，一种方法可能比另一种方法表现得更好。这个临界点会随着任务和模型大小的不同而变化。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;这些结论对理解LLM微调过程中的“SCALING”具有重要意义。它们揭示了不同规模因素如何相互作用以及它们对微调性能的共同影响，为在实际应用中选择和优化微调策略提供了理论依据。通过这些发现，研究者和实践者可以更好地理解在特定条件下如何有效地缩放和配置他们的模型以获得最佳性能。&lt;/p&gt;</description>
    </item>
    <item>
      <title>文档智能</title>
      <link>https://www6v.github.io/www6vAIGC/docs/DocumentAI/DocumentAI/</link>
      <pubDate>Wed, 19 Apr 2023 06:18:08 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/DocumentAI/DocumentAI/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;文档理解-两种范式1-10&#34;&gt;&#xA;  文档理解 两种范式[1, 10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%96%87%e6%a1%a3%e7%90%86%e8%a7%a3-%e4%b8%a4%e7%a7%8d%e8%8c%83%e5%bc%8f1-10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;pipelineocr&#34;&gt;&#xA;  pipeline(OCR)&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#pipelineocr&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ERNIElayout [1]&lt;/li&gt;&#xA;&lt;li&gt;LayoutLM系列 [1]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;端到端ocr-free&#34;&gt;&#xA;  端到端(OCR-free)&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%ab%af%e5%88%b0%e7%ab%afocr-free&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;基于小模型的OCR-free微调方案&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Donut&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;基于大模型的OCR-FREE微调方案&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LLaVAR [12]&lt;/li&gt;&#xA;&lt;li&gt;TextMonkey [11]&lt;/li&gt;&#xA;&lt;li&gt;mPLUG-DocOwl1.5  [20]&lt;br&gt;&#xA;DocOwl1.5由mPLUG-Owl2初始化，使用&lt;strong&gt;ViT/L-14作为视觉编码器&lt;/strong&gt;，并使用带有模态自适应模块的7B大模型作为&lt;strong&gt;解码器&lt;/strong&gt;。&#xA;每个子图像由ViT/L-14编码为1,024个特征，然后由&lt;strong&gt;H-Reducer缩减为256个特征&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;TextMonkey [20]&lt;br&gt;&#xA;为了减少图像特征的冗余，继承了&lt;strong&gt;Qwen-VL&lt;/strong&gt;中的图像&lt;strong&gt;重采样器&lt;/strong&gt;，在每个窗口中都会使用。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;文档版式分析数据集10&#34;&gt;&#xA;  文档版式分析数据集[10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%96%87%e6%a1%a3%e7%89%88%e5%bc%8f%e5%88%86%e6%9e%90%e6%95%b0%e6%8d%ae%e9%9b%8610&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;老刘-分享&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/FsjoUUFssMv2UkbxM-IJ3A&#34;&gt;值得一看的文档理解前沿方案及版式分析开源数据：三种模式、九大数据集 &lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/Yuliang-Liu/Monkey&#34;&gt;Monkey&lt;/a&gt;&lt;br&gt;&#xA;&lt;a href=&#34;http://vlrlab-monkey.xyz:7684/&#34;&gt;Monkey Demo&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/670175648&#34;&gt;LLaVAR：增强的视觉指令微调&lt;/a&gt;&lt;br&gt;&#xA;&lt;a href=&#34;https://llavar.github.io/&#34;&gt;LLaVAR: Enhanced Visual Instruction Tuning for Text-rich Image Understanding&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://huggingface.co/blog/zh/document-ai&#34;&gt;加速 Document AI (文档智能) 发展&lt;/a&gt;&lt;br&gt;&#xA;&lt;a href=&#34;https://baijiahao.baidu.com/s?id=1755096032832674219&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;加速 Document AI (文档智能) 发展&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s/d2Nns1qashMbcXPMG-4McQ&#34;&gt;阿里面向企业数字化的文档智能技术与应用&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;其他&#34;&gt;&#xA;  其他&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%85%b6%e4%bb%96&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;20&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/1MSOZfbKcPW1BTT4f9XvQg&#34;&gt;也看跨模态大模型遇见文档理解：mPLUG-DocOwl1.5及TextMonkey方案中的数据工程 &lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(实战)Agent Tuning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentTuning/</link>
      <pubDate>Fri, 07 Apr 2023 16:56:18 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentTuning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;基于微调的agent---function-call12&#34;&gt;&#xA;  基于微调的Agent - Function Call[1][2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%9f%ba%e4%ba%8e%e5%be%ae%e8%b0%83%e7%9a%84agent---function-call12&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基座模型&lt;br&gt;&#xA;internLM&lt;/li&gt;&#xA;&lt;li&gt;微调框架&lt;br&gt;&#xA;xtuner&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/dirs.JPG&#34; alt=&#34;dirs&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/xtuner-agent.png&#34; alt=&#34;xtuner-agent&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;agent-tuning3&#34;&gt;&#xA;  Agent Tuning[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agent-tuning3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基座模型&lt;br&gt;&#xA;Yi-6B&lt;/li&gt;&#xA;&lt;li&gt;Datasets&lt;/li&gt;&#xA;&lt;li&gt;微调框架&lt;br&gt;&#xA;LLama-Factory&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;环境准备&#34;&gt;&#xA;  环境准备&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%8e%af%e5%a2%83%e5%87%86%e5%a4%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# source code&#xA;git clone -b v0.7.1  &amp;lt;https://github.com/hiyouga/LLaMA-Factory.git&amp;gt;&#xA;git switch -c v0.7.1&#xA;cd LLaMA-Factory&#xA;&#xA;# package 安装&#xA;conda create -n llama_factory python=3.10&#xA;conda activate llama_factory&#xA;pip install llmtuner==0.5.1&#xA;&#xA;# 环境变量&#xA;export CUDA_VISIBLE_DEVICES=0 # 使用第一块 GPU&#xA;export USE_MODELSCOPE_HUB=1 # 使用魔搭社区下载渠道&#xA;&#xA;# 阿里云必须加这句，不然页面会报异常&#xA;$ export GRADIO_ROOT_PATH=/${JUPYTER_NAME}/proxy/7860/&#xA;&#xA;# 启动&#xA;python -m llmtuner.webui.interface&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;训练流程&#34;&gt;&#xA;  训练流程&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ad%e7%bb%83%e6%b5%81%e7%a8%8b&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# flash-attn 安装&#xA;pip install flash-attn --no-build-isolation&#xA;&#xA;pip install modelscope -U&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;训练脚本&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# 训练轮数 1.0&#xA;&#xA;CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\&#xA;    --stage sft \\&#xA;    --do_train True \\&#xA;    --model_name_or_path 01ai/Yi-6B \\&#xA;    --finetuning_type lora \\&#xA;    --template default \\&#xA;    --flash_attn True \\&#xA;    --dataset_dir data \\&#xA;    --dataset glaive_toolcall,alpaca_gpt4_en,alpaca_gpt4_zh,oaast_sft_zh \\&#xA;    --cutoff_len 1024 \\&#xA;    --learning_rate 5e-05 \\&#xA;    --num_train_epochs 1.0 \\&#xA;    --max_samples 8000 \\&#xA;    --per_device_train_batch_size 4 \\&#xA;    --gradient_accumulation_steps 4 \\&#xA;    --lr_scheduler_type cosine \\&#xA;    --max_grad_norm 1.0 \\&#xA;    --logging_steps 5 \\&#xA;    --save_steps 100 \\&#xA;    --warmup_steps 0 \\&#xA;    --lora_rank 8 \\&#xA;    --lora_dropout 0.1 \\&#xA;    --lora_target all \\&#xA;    --output_dir saves/Yi-6B/lora/yi-agent-6b \\&#xA;    --fp16 True \\&#xA;    --plot_loss True&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;训练配置&#xA;&lt;img src=&#34;./images/agentTuningUI.png&#34; alt=&#34;agentTuningUI.png&#34; /&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Agent 分类[有趣|有用]</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Overview/AgentCategory/</link>
      <pubDate>Thu, 06 Apr 2023 23:18:53 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Overview/AgentCategory/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;有趣的ai更像人的ai&#34;&gt;&#xA;  有趣的AI：更像人的AI&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9c%89%e8%b6%a3%e7%9a%84ai%e6%9b%b4%e5%83%8f%e4%ba%ba%e7%9a%84ai&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;好看的皮囊--多模态&#34;&gt;&#xA;  好看的皮囊  多模态&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a5%bd%e7%9c%8b%e7%9a%84%e7%9a%ae%e5%9b%8a--%e5%a4%9a%e6%a8%a1%e6%80%81&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;多模态&lt;strong&gt;理解能力&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;多模态数据端到端预训练的模型&#xA;Gemini&lt;/li&gt;&#xA;&lt;li&gt;工程化&#xA;projection layer&lt;/li&gt;&#xA;&lt;li&gt;直接用文本去粘接 encoder、decoder 和文本大模型&lt;/li&gt;&#xA;&lt;li&gt;eg【自己动手做出Gemini演示视频的效果】&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;多模态&lt;strong&gt;生成能力&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;视频生成&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Live2D，3D 模型&lt;/li&gt;&#xA;&lt;li&gt;DeepFake&#xA;录制一个真人视频， 把视频中的人脸换成指定的人脸照片&lt;/li&gt;&#xA;&lt;li&gt;Image Animation&#xA;给定一张照片，随后根据这张照片生成一系列的对应视频&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Video Diffusion&lt;/strong&gt;&#xA;对物理世界的建模&#xA;成本最高&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;有趣的灵魂&#34;&gt;&#xA;  有趣的灵魂&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9c%89%e8%b6%a3%e7%9a%84%e7%81%b5%e9%ad%82&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;个性&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基于prompt&#xA;完整地刻画出一个人物的历史、个性、记忆和性格&#xA;长文本&lt;/li&gt;&#xA;&lt;li&gt;基于微调的 agent&#xA;&lt;ul&gt;&#xA;&lt;li&gt;更关键的还是数据&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;对话性语料&lt;/strong&gt; &amp;amp; &lt;strong&gt;事实性语料&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;第一步，我们先用对话性语料去微调他的个性和说话风格&lt;/li&gt;&#xA;&lt;li&gt;第二步，再去把事实性语料进行数据清洗后，基于各种角度提问，生成这个人物第一人称口吻的回答，这叫做&lt;strong&gt;数据增强&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;慢思考&lt;/strong&gt;与记忆&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;组件&#xA;&lt;strong&gt;记忆、情感&lt;/strong&gt;、任务规划、工具&lt;/li&gt;&#xA;&lt;li&gt;长期记忆&#xA;&lt;ul&gt;&#xA;&lt;li&gt;事实性的记忆&#xA;&lt;ul&gt;&#xA;&lt;li&gt;总结&#xA;文本总结  MemGPT&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;RAG 和信息压缩&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;长上下文  &lt;strong&gt;长上下文&lt;/strong&gt;&#xA;结合持久化 KV Cache&#xA;成本还是太高&#xA;【eg.  文本总结 + RAG】&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;程序性的记忆&#xA;&lt;ul&gt;&#xA;&lt;li&gt;few-shot&lt;/li&gt;&#xA;&lt;li&gt;微调&#xA;短期来看仍然是效果最好的路线&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;有用的ai更像工具的ai&#34;&gt;&#xA;  有用的AI：更像工具的AI&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9c%89%e7%94%a8%e7%9a%84ai%e6%9b%b4%e5%83%8f%e5%b7%a5%e5%85%b7%e7%9a%84ai&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;大模型基础能力&#34;&gt;&#xA;  大模型基础能力&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%9f%ba%e7%a1%80%e8%83%bd%e5%8a%9b&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;复杂任务的规划和分解&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;遵循复杂指令&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;自主使用工具&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;减少幻觉&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;1p-3p-产品法则&#34;&gt;&#xA;  1P-3P 产品法则&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#1p-3p-%e4%ba%a7%e5%93%81%e6%b3%95%e5%88%99&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;分类&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Work|实战)Plan&amp;Execute,ReWOO</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Planning/AgentPlanAndExecute/</link>
      <pubDate>Thu, 02 Mar 2023 09:31:47 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Planning/AgentPlanAndExecute/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;plan-and-execute-0&#34;&gt;&#xA;  Plan-and-execute [0]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#plan-and-execute-0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;原理&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Figure 2 - 基于prompt [1]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;代码&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;plan [2]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Planning Step&lt;/li&gt;&#xA;&lt;li&gt;Re-Plan Step&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;问题&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;冗余的提示和重复的执行 -&amp;gt; ReWOO&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;rewoo-0&#34;&gt;&#xA;  ReWOO [0]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rewoo-0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;原理&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Abstract [10]&#xA;增强语言模型（ALM）将大型语言模型（LLM）的推理能力与允许知识检索和执行操作的工具相结合。现有的ALM系统以交错的方式触发LLM的思考过程，同时从这些工具中获取观察结果。&lt;strong&gt;具体而言，LLM推理过程中会调用外部工具，然后在获取工具响应后停止，基于之前的响应令牌来决定下一步的操作。这种范式虽然直观且易于实现，但常常由于冗余的提示和重复的执行而导致计算复杂度极高&lt;/strong&gt;。本研究首次解决了这些挑战，提出了一种模块化的范式ReWOO（无观察推理），&lt;strong&gt;将推理过程与外部观察结果分离，从而显著减少了令牌的消耗&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;Figure 1 [10]&#xA;Planner里有格式化的#E&lt;/li&gt;&#xA;&lt;li&gt;Figure 2  [10]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;代码 [11]&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Executor-tool_execution() -&amp;gt; 状态机&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;问题&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;是否可以并行？-&amp;gt; LLMCompiler&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;llmcompiler&#34;&gt;&#xA;  LLMCompiler&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llmcompiler&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;原理&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Abstract [20]&#xA;LLM的多函数调用能力催生了基于LLM的软件开发，使其能够解决更复杂的问题。然而，当前的多函数调用方法通常需要&lt;strong&gt;针对每个函数进行顺序推理和执行，这可能导致较高的延迟、成本以及不准确的行为&lt;/strong&gt;。为了解决这个问题，我们引入了LLMCompiler，它可以&lt;strong&gt;并行执行函数，以高效地编排多函数调用&lt;/strong&gt;。LLMCompiler&lt;strong&gt;借鉴了经典编译器的原理&lt;/strong&gt;，在LLM中使用&lt;strong&gt;三个组件&lt;/strong&gt;来简化并行函数调用：（i）LLM规划器，制定执行策略和依赖关系；（ii）任务获取单元，分派和更新函数调用任务；（iii）执行器，以并行方式执行这些任务。通过LLMCompiler，用户可以指定工具以及可选的上下文示例，LLMCompiler会自动计算函数调用的优化编排。重要的是，LLMCompiler可以与LLaMA-2等开源模型以及OpenAI的GPT模型一起使用。&lt;/li&gt;&#xA;&lt;li&gt;Figure 2  [20]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;代码 [21]&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Planner&lt;/li&gt;&#xA;&lt;li&gt;Task Fetching Unit&lt;/li&gt;&#xA;&lt;li&gt;Joiner&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;plan-and-execute&#34;&gt;&#xA;  Plan-and-execute&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#plan-and-execute&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;0&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1vJ4m1s7Zn/&#34;&gt;LangGraph：Plan-Execute Agents 实战&lt;/a&gt; V&#xA;&lt;a href=&#34;https://blog.langchain.dev/planning-agents/&#34;&gt;Plan-and-Execute Agents&lt;/a&gt; ***&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.04091.pdf&#34;&gt;Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought&#xA;Reasoning by Large Language Models&lt;/a&gt;  Figure 2&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langgraph/blob/main/examples/plan-and-execute/plan-and-execute.ipynb&#34;&gt;plan-and-execute&lt;/a&gt;    git&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;rewoo&#34;&gt;&#xA;  ReWOO&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rewoo&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;10&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.18323.pdf&#34;&gt;ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langgraph/blob/main/examples/rewoo/rewoo.ipynb&#34;&gt;Reasoning without Observation&lt;/a&gt; git&lt;br&gt;&#xA;&lt;a href=&#34;https://www.bilibili.com/video/BV1Au4m1N7ix/&#34;&gt;ReWoo Agent框架代码实现&lt;/a&gt; V&lt;br&gt;&#xA;1xx.  &lt;a href=&#34;https://zhuanlan.zhihu.com/p/671491031&#34;&gt;ReWOO: 高效增强语言模型中解偶观测和推理&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;llmcompiler-1&#34;&gt;&#xA;  LLMCompiler&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llmcompiler-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;20&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2312.04511v1.pdf&#34;&gt;An LLM Compiler for Parallel Function Calling&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langgraph/blob/main/examples/llm-compiler/LLMCompiler.ipynb&#34;&gt;LLMCompiler&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Langchain  Agent</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Langchain/LangchainAgent/</link>
      <pubDate>Wed, 11 Jan 2023 17:08:21 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Langchain/LangchainAgent/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;langchain-agent&#34;&gt;&#xA;  Langchain Agent&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#langchain-agent&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Conversational&lt;/li&gt;&#xA;&lt;li&gt;OpenAI assistants&lt;/li&gt;&#xA;&lt;li&gt;OpenAI functions&lt;/li&gt;&#xA;&lt;li&gt;OpenAI Multi Functions Agent&lt;/li&gt;&#xA;&lt;li&gt;OpenAI tools&#xA;OpenAI parallel function calling (a.k.a. tool calling)&lt;/li&gt;&#xA;&lt;li&gt;ReAct&#xA;ZeroShotReactAgent&lt;/li&gt;&#xA;&lt;li&gt;Self-ask with search&lt;/li&gt;&#xA;&lt;li&gt;Structured tool chat&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;langchain-apps&#34;&gt;&#xA;  Langchain Apps&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#langchain-apps&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;rag-chroma-private-2&#34;&gt;&#xA;  rag-chroma-private [2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag-chroma-private-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;本地 部署&lt;/strong&gt;&#xA;This template performs RAG with no reliance on external APIs.&#xA;It utilizes &lt;strong&gt;Ollama the LLM, GPT4All for embeddings, and Chroma for the vectorstore&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;research-assistant-34&#34;&gt;&#xA;  research-assistant [3][4]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#research-assistant-34&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;This template implements a version of&#xA;&amp;ldquo;GPT Researcher&amp;rdquo; that you can use as a starting point for a &lt;strong&gt;research agent&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Survey) Agent 优化 &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/AgentOpt/</link>
      <pubDate>Sun, 01 Jan 2023 10:33:11 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/AgentOpt/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;agent-优化&#34;&gt;&#xA;  Agent 优化&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agent-%e4%bc%98%e5%8c%96&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/A-Survey-on-the-Optimization-of-Large-Language-Model-based-Agents-1e2bfe21108480219731da1c3a4a0c17?pvs=4&#34;&gt;(Survey) Agent 优化&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战)Agent</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/AgentPractice/</link>
      <pubDate>Sun, 01 Jan 2023 10:33:11 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/AgentPractice/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;assistant-api-3&#34;&gt;&#xA;  Assistant API [3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#assistant-api-3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;assistant-api功能介绍&#34;&gt;&#xA;  Assistant API功能介绍&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#assistant-api%e5%8a%9f%e8%83%bd%e4%bb%8b%e7%bb%8d&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;从功能实现层面来说，Assistant API是截至目前最完整、性能最强大的AI应用开发API，具体功能如下：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;首先，Assistant API前所未有的能够&lt;strong&gt;调用OpenAI各模型的各项能力&lt;/strong&gt;，包括可以调用Chat系列模型（即GPT系列模型）完成文本对话、调用DALL·E 3进行绘图、调用GPT-4-vision进行图像识别、以及调用Text-to-Speech模型进行语音转文字等，并且支持在一轮对话中调用不同模型；&lt;/li&gt;&#xA;&lt;li&gt;其次，Assistant API还&lt;strong&gt;内置了代码解释器功能（Code interpreter）和海量文本信息提取功能（Knowledge retrieval）&lt;strong&gt;同时也一如既往支持借助&lt;/strong&gt;Function calling&lt;/strong&gt;进行模型功能层面拓展，此外，非常重要的是，Assistant API还支持在一轮对话中调用多个工具；&lt;/li&gt;&#xA;&lt;li&gt;其三，此外对于开发者非常友好的一点是，Assistant API最小运行单元为持久化的线程对象（persistent Threads），因此在实际运行Assistant API时，不仅能可以精确控制每一步的执行过程，同时persistent Threads也会保留每轮对话的核心信息，并且当超出模型接收信息最大上下文限制时能够自动删除早期信息，从而实现对模型短期记忆的合理管理；&lt;/li&gt;&#xA;&lt;li&gt;其四，Assistant API还能够直&lt;strong&gt;接连接OpenAI在线文档库&lt;/strong&gt;，即如果用户将外部文档保存在OpenAI云空间内，则可以在调用Assistant API时实时访问文档库中的任意文件，甚至可以在不同线程中调用不同的文档。而在借助Assistant API的Knowledge retrieval功能，则可以让大模型实时获取这些文件信息，并且合理管理短期记忆；&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;实战&#34;&gt;&#xA;  实战&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;lagent--agentlego4&#34;&gt;&#xA;  Lagent &amp;amp; AgentLego[4]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#lagent--agentlego4&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Assistant API详解与Agent开发实战-九天Hector&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/InternLM/Tutorial/tree/camp2/agent&#34;&gt;Lagent &amp;amp; AgentLego 智能体应用搭建&lt;/a&gt;&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/InternLM/Tutorial/blob/camp2/agent/lagent.md&#34;&gt;Lagent：轻量级智能体框架&lt;/a&gt;&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/InternLM/Tutorial/blob/camp2/agent/agentlego.md&#34;&gt;AgentLego：组装智能体“乐高”&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://qwenlm.github.io/zh/blog/qwen-agent-2405/&#34;&gt;使用Qwen-Agent将上下文记忆扩展到百万量级&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Advanced RAG &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAGPerformance/</link>
      <pubDate>Wed, 07 Dec 2022 09:44:24 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAGPerformance/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;advanced-rag&#34;&gt;&#xA;  Advanced RAG&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#advanced-rag&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Advanced-RAG-108bfe2110848030b150e8c09baa7232?pvs=4&#34;&gt;(原理)Advanced RAG&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)[Prompting]Coding</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Prompt-Engineering/PromptCode/</link>
      <pubDate>Fri, 28 May 2021 12:12:45 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Prompt-Engineering/PromptCode/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;工具&#34;&gt;&#xA;  工具&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%b7%a5%e5%85%b7&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Copilot *** - 收费&lt;/li&gt;&#xA;&lt;li&gt;AWS CodeWhispter&lt;/li&gt;&#xA;&lt;li&gt;Cursor ***&lt;/li&gt;&#xA;&lt;li&gt;tabnine 免费&lt;/li&gt;&#xA;&lt;li&gt;Code Llama  - 开源&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;代码相关-简单任务-1&#34;&gt;&#xA;  代码相关-简单任务 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bb%a3%e7%a0%81%e7%9b%b8%e5%85%b3-%e7%ae%80%e5%8d%95%e4%bb%bb%e5%8a%a1-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;注释&lt;/strong&gt;&lt;br&gt;&#xA;你作为一名程序员，请解释一下下面这段代码&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;防御性编程&lt;/strong&gt;&#xA;请为这段代码增加防御性编程的功能&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;写单元测试&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;时间复杂度  time complexity&lt;/strong&gt;&#xA;这段代码的时间复杂度是多少&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;流程图&#xA;画出redis master和slave之间同步的流程图&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Writing shell script&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Writing git commands&#xA;一个分支中的代码合并到另一个分支中&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Improve code&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  How do i improve this code&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;?&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  fruits &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;apple&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;banana&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;cherry&amp;#34;&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  newlist &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; fruits:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; x:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    newlist&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(x)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(newlist)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Translating Code&lt;/strong&gt; 代码转换&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Convert this Python code to Javascript&lt;/li&gt;&#xA;&lt;li&gt;请把下面这段python代码转换成Java代码&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;代码相关--繁琐工作-1&#34;&gt;&#xA;  代码相关- 繁琐工作 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bb%a3%e7%a0%81%e7%9b%b8%e5%85%b3--%e7%b9%81%e7%90%90%e5%b7%a5%e4%bd%9c-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Building API&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Langgraph)Deep Research</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Deep-Research/%E5%AE%9E%E7%8E%B0/DeepresearchLanggraph/</link>
      <pubDate>Tue, 26 Mar 2024 12:13:50 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Deep-Research/%E5%AE%9E%E7%8E%B0/DeepresearchLanggraph/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;langgraphdeep-research&#34;&gt;&#xA;  (Langgraph)Deep Research&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#langgraphdeep-research&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Langgraph-Deep-Research-1c2bfe211084803ab640f7304172834a?source=copy_link&#34;&gt;(Langgraph)Deep Research&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(框架)RAGflow &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/framework/RAGRAGflow/</link>
      <pubDate>Mon, 19 Jun 2023 10:25:33 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/framework/RAGRAGflow/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;ragflow&#34;&gt;&#xA;  RAGflow&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#ragflow&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/RagFlow-1b5bfe21108480618819d7abf6c7704f?pvs=4&#34;&gt;RAGflow&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG 评估</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAGEval/</link>
      <pubDate>Wed, 07 Jun 2023 17:02:42 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAGEval/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://www.bilibili.com/video/BV1Jz421Q7Lw/&#34;&gt;如何利用RAGAs评估RAG系统的好坏&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/blackinkkkxi/RAG_langchain/blob/main/learn/evaluation/RAGAS-langchian.ipynb&#34;&gt;使用LangChain和RAGAS对RAG系统进行自动有效评估&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://www.bilibili.com/video/BV1aZ421W7DB/&#34;&gt;一次搞懂RAG评估，三个角度LangChain，LlamaIndex，RAGAS&lt;/a&gt;&lt;br&gt;&#xA;&lt;a href=&#34;https://techdiylife.github.io/blog/blog.html?category1=c02&amp;amp;blogid=0053&#34;&gt;RAG评估资料大全 &lt;/a&gt;&lt;br&gt;&#xA;RAG评估指标：两种视角理解评估指标&lt;br&gt;&#xA;&lt;a href=&#34;https://docs.smith.langchain.com/old/cookbook/testing-examples/rag_eval&#34;&gt;RAG Evaluation&lt;/a&gt;&lt;br&gt;&#xA;&lt;a href=&#34;https://docs.smith.langchain.com/old/cookbook/testing-examples/ragas&#34;&gt;RAG evaluation with RAGAS&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648404511&amp;amp;idx=2&amp;amp;sn=fefb78c1d920cb5b437f2e3da9935637&#34;&gt;再看大模型RAG检索增强如何评估：RAGAS开源自动化评估框架&lt;/a&gt;&lt;br&gt;&#xA;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648404476&amp;amp;idx=2&amp;amp;sn=d07b27dc9162ab0aaec3108004e4cfbe&#34;&gt;大模型RAG检索增强问答如何评估：噪声、拒答、反事实、信息整合四大能力评测任务探索 &lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)RAG Rerank</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPostRetrieval/RAGRerank/</link>
      <pubDate>Sun, 14 May 2023 18:23:41 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPostRetrieval/RAGRerank/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;reranker-22&#34;&gt;&#xA;  Reranker [22]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#reranker-22&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;A reranking model — also known as a &lt;strong&gt;cross-encoder&lt;/strong&gt; — is a type of model that,&lt;strong&gt;given a query and document pair, will output a similarity score.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f0d2f75571bb58eecf2520a23d300a5fc5b1e2c-2440x1100.png&amp;amp;w=3840&amp;amp;q=65&#34; alt=&#34;Rerankers&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;产品&#34;&gt;&#xA;  产品&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%ba%a7%e5%93%81&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;bge-ranker-20&#34;&gt;&#xA;  BGE Ranker [20]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#bge-ranker-20&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;交叉编码器&lt;/strong&gt;将对查询和答案实时计算相关性分数，这比**向量模型(即双编码器)**更准确，但比向量模型更耗时。 因此，它可以用来对嵌入模型返回的前k个文档重新排序。 我们在多语言数据上训练了交叉编码器，数据格式与向量模型相同，因此您可以根据我们的示例 轻松地对其进行微调。&lt;/p&gt;&#xA;&lt;h3 id=&#34;bce24&#34;&gt;&#xA;  BCE[24]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#bce24&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;中文效果比BGE好[老刘说nlp]&lt;/p&gt;&#xA;&lt;h3 id=&#34;优秀的组合-21&#34;&gt;&#xA;  优秀的组合 [21]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bc%98%e7%a7%80%e7%9a%84%e7%bb%84%e5%90%88-21&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;OpenAI + CohereRerank&#xA;Voyage + big-reranker-large&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol start=&#34;20&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md&#34;&gt;BGE Reranker&lt;/a&gt;&#xA;&lt;a href=&#34;https://www.bilibili.com/video/BV1sQ4y137Ft/&#34;&gt;transformers二次开发——bge-reranker模型微调流程&lt;/a&gt; V&#xA;&lt;a href=&#34;https://mp.weixin.qq.com/s/XnkQFCdbvjox1Y06IbIlYw&#34;&gt;RAG 再添新利器！智源开源最强检索排序模型 BGE Re-Ranker v2.0 &lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Agent Challenge</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/AgentChallenge/</link>
      <pubDate>Sat, 13 May 2023 07:17:37 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/AgentChallenge/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;问题和局限性-4&#34;&gt;&#xA;  问题和局限性 [4]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e9%97%ae%e9%a2%98%e5%92%8c%e5%b1%80%e9%99%90%e6%80%a7-4&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;记忆召回问题&lt;br&gt;&#xA;只是做简单的 embedding 相似性召回，很容易发现召回的结果不是很好&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;错误累积问题&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;探索效率问题&lt;br&gt;&#xA;中途引入人工的判断干预和反馈输入&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;任务终止与结果验证&lt;br&gt;&#xA;模型 agent 的工作如何终止也是一个挑战&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;挑战-8&#34;&gt;&#xA;  挑战 [8]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%8c%91%e6%88%98-8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;如何让-agent-选择合适的工具&#34;&gt;&#xA;  如何让 agent 选择合适的工具&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a6%82%e4%bd%95%e8%ae%a9-agent-%e9%80%89%e6%8b%a9%e5%90%88%e9%80%82%e7%9a%84%e5%b7%a5%e5%85%b7&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Toolformer - fine tune&lt;/li&gt;&#xA;&lt;li&gt;Gorilla - retrieval，fine tune&lt;br&gt;&#xA;【solution: SFT or RL】&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;不必要的工具使用&#34;&gt;&#xA;  不必要的工具使用&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%b8%8d%e5%bf%85%e8%a6%81%e7%9a%84%e5%b7%a5%e5%85%b7%e4%bd%bf%e7%94%a8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;“Human Input”也写成一种工具，让模型来主动发起对人类的提问&lt;br&gt;&#xA;&lt;a href=&#34;https://python.langchain.com/docs/integrations/tools/human_tools&#34;&gt;Human as a tool&lt;/a&gt;&lt;br&gt;&#xA;【solution: human-in-the-loop】&lt;/p&gt;&#xA;&lt;h3 id=&#34;agent-返回的格式不稳定&#34;&gt;&#xA;  Agent 返回的格式不稳定&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agent-%e8%bf%94%e5%9b%9e%e7%9a%84%e6%a0%bc%e5%bc%8f%e4%b8%8d%e7%a8%b3%e5%ae%9a&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;这里常见的做法是让 LLM &lt;strong&gt;按照 json 这类常见的 schema 来返回&lt;/strong&gt;，一般稳定性会高一些（相比“Action:”这种）。&lt;br&gt;&#xA;此外自动修复重试也很实用，可以利用 LangChain 里的 &lt;strong&gt;output parsers&lt;/strong&gt; 来帮助完成。&lt;br&gt;&#xA;【solution: json output】&lt;/p&gt;&#xA;&lt;h3 id=&#34;记住之前的操作避免重复&#34;&gt;&#xA;  记住之前的操作，避免重复&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%b0%e4%bd%8f%e4%b9%8b%e5%89%8d%e7%9a%84%e6%93%8d%e4%bd%9c%e9%81%bf%e5%85%8d%e9%87%8d%e5%a4%8d&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;AutoGPT - retrieval 结合近期操作记录&lt;br&gt;&#xA;【solution: memory】&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Data Selection</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/DataSelection/</link>
      <pubDate>Fri, 05 May 2023 19:14:41 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/DataSelection/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;ifd1&#34;&gt;&#xA;  IFD[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#ifd1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;三个步骤&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Learning from Brief Experience&#xA;利用少量进行进行&lt;strong&gt;模型初学&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Evaluating Based on Experience&#xA;利用初学模型计算原始数据中所有&lt;strong&gt;IFD指标&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;算法&#xA;&lt;ul&gt;&#xA;&lt;li&gt;条件回答分数（ Conditioned Answer Score，CAS）&lt;/li&gt;&#xA;&lt;li&gt;直接答案分数（Direct Answer Score，DAS）&lt;/li&gt;&#xA;&lt;li&gt;指令跟随难度（Instruction-Following Difficulty，IFD）分数&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Retraining from Self-Guided Experience&#xA;利用&lt;strong&gt;樱桃数据&lt;/strong&gt;进行模型&lt;strong&gt;重训练&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;mods2&#34;&gt;&#xA;  MoDS[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#mods2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;质量筛选&#xA;采用OpenAssistant的&lt;strong&gt;reward-model&lt;/strong&gt;-debertav3-large-v2模型（一个基于&lt;strong&gt;DeBERTa架构&lt;/strong&gt;设计的奖励模型）对数据进行&lt;strong&gt;质量打分&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;多样性筛选&#xA;为了避免所选质量数据高度相似，通过&lt;strong&gt;K-Center-Greedy算法&lt;/strong&gt;进行数据筛选，在最大化多样性的情况下，使指令数据集最小。&#xA;在该步骤中，采用&lt;strong&gt;BERT模型&lt;/strong&gt;为指令数据生成句向量来计算不同数据之间的距离。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;必要性筛选&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;deita-3&#34;&gt;&#xA;  DEITA [3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#deita-3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;复杂性评分&#34;&gt;&#xA;  复杂性评分&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%8d%e6%9d%82%e6%80%a7%e8%af%84%e5%88%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;复杂性评估的方法&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Random Selection：随机选择样本。&lt;/li&gt;&#xA;&lt;li&gt;Instruction Length：按照指令的长度计算复杂性。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Perplexity&lt;/strong&gt;：通过预训练模型计算回复的困惑度作为复杂性指标，困惑值越大意味着数据样本越难。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Direct Scoring&lt;/strong&gt;：利用ChaGPT给指令的复杂性打分。&lt;/li&gt;&#xA;&lt;li&gt;Instruction Node：利用ChatGPT将指令转换成语义树，通过树的节点数作为复杂性指标。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Instag Complexity&lt;/strong&gt;：利用ChatGPT对部分数据进行打标签，再训练一个Llama模型，再利用训练后的Llama模型对全量数据预测，标签越多说明数据约复杂。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;IFD&lt;/strong&gt;：指令跟随难度作为复杂性指标。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;DEITA评估复杂性的方法，主要先对一个小规模种子数据集（2k）进行数据复杂性&lt;strong&gt;扩展&lt;/strong&gt;，再利&lt;strong&gt;用ChatGPT对扩展数据进行打分&lt;/strong&gt;，并&lt;strong&gt;训练一个Llama1-7B的模型&lt;/strong&gt;，最后利用训练后的模型对数据的打分作为复杂性评估指标。&lt;/p&gt;&#xA;&lt;h3 id=&#34;质量评分&#34;&gt;&#xA;  质量评分&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%b4%a8%e9%87%8f%e8%af%84%e5%88%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;质量评估的方法有&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Random Selection：随机选择样本。&lt;/li&gt;&#xA;&lt;li&gt;Response Length：采用输出长度作为质量评估指标。&lt;/li&gt;&#xA;&lt;li&gt;Direct Scoring：利用ChatGPT直接评估对特定指令输出结果的准确性。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;DEITA评估质量的方法，&lt;strong&gt;与评估复杂性方法一致&lt;/strong&gt;。先对一个小规模种子数据集（2k，与复杂性数据一致）进行数据质量扩展，再利用ChatGPT对扩展数据进行打分并训练一个Llama1-7B的模型，最后利用训练后的模型对数据的打分作为质量评估指标。&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战)PromptTuning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Soft-Prompt/PromptTuningPractice/</link>
      <pubDate>Wed, 25 Jan 2023 19:11:28 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Soft-Prompt/PromptTuningPractice/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/646748939&#34;&gt;大模型参数高效微调技术实战（二）-Prompt Tuning&lt;/a&gt;&#xA;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/635686756&#34;&gt;大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/www6v/llm-action/blob/main/train/peft/clm/peft_prompt_tuning_clm.ipynb&#34;&gt;peft_prompt_tuning_clm.ipynb&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)[Prompting]How to use</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Prompt-Engineering/Prompt/</link>
      <pubDate>Wed, 26 May 2021 22:50:04 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Prompt-Engineering/Prompt/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;乔哈里沟通视窗-4-象限&#34;&gt;&#xA;  乔哈里沟通视窗-4 象限&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%b9%94%e5%93%88%e9%87%8c%e6%b2%9f%e9%80%9a%e8%a7%86%e7%aa%97-4-%e8%b1%a1%e9%99%90&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;你不知道gpt知道&#34;&gt;&#xA;  你不知道，GPT知道&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bd%a0%e4%b8%8d%e7%9f%a5%e9%81%93gpt%e7%9f%a5%e9%81%93&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;1、元问题：我想了解xxxx，我应该向你问哪些问题？&lt;br&gt;&#xA;2、请给我列出xxx领域/行业相关的，最常用的50个概念，并做简单解释。如果有英文缩写，请给出完整的英文解释。&lt;br&gt;&#xA;3、请详细介绍一下elon musk的主要生平事迹。请详细介绍一下tesla这家企业的发展历程。&lt;/p&gt;&#xA;&lt;h3 id=&#34;你知道gpt也知道&#34;&gt;&#xA;  你知道，GPT也知道&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bd%a0%e7%9f%a5%e9%81%93gpt%e4%b9%9f%e7%9f%a5%e9%81%93&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;检验认知：&#xA;1、对于xxx主题/技能，你认为哪些是我必须理解和掌握的核心要点？&lt;br&gt;&#xA;2、我理解的xxx是这样的，你觉得我的理解对吗？&lt;br&gt;&#xA;3、我对xxx有一些想法，你能帮我批判性地分析一下这些想法的优点和缺点吗？&lt;br&gt;&#xA;4、我正在考虑xxx的决定，你能帮我分析一下可能的结果和影响吗？&lt;/p&gt;&#xA;&lt;p&gt;扩充认知：&lt;br&gt;&#xA;1、我知道xxx的概念，我想知道更多关于xxx的信息。&lt;br&gt;&#xA;2、我在xxx问题上遇到困难，你能提供一些可能的解决方案或建议吗？&lt;br&gt;&#xA;3、我想要深入学习xxx，你能推荐一些进阶的学习资源或学习路径吗？&lt;br&gt;&#xA;4、我想要在xxx领域有所创新，你能提供一些启发或想法吗？&lt;br&gt;&#xA;5、我想在xxx领域提升自己，你能根据最新的研究和趋势给我一些建议吗？&lt;br&gt;&#xA;6、我正在考虑学习xxx，你能给我一些关于这个领域未来发展的观点吗？&lt;br&gt;&#xA;7、（背景信息xxx），我要做关于xxx的研究，我认为原因是，还有其他可能的原因吗？给出一些可能的研究假设。&lt;br&gt;&#xA;8、我是一个xx新手，马上要采访这个行业的资深大佬，我应该向他请教哪些有价值的问题？&lt;/p&gt;&#xA;&lt;h3 id=&#34;你知道gpt不知道&#34;&gt;&#xA;  你知道，GPT不知道&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bd%a0%e7%9f%a5%e9%81%93gpt%e4%b8%8d%e7%9f%a5%e9%81%93&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;介绍背景现象之后可以向gpt发问，你怎么看待这种现象？可能的原因有哪些？这会对xxx产生什么样的影响？你觉得xxx应该怎么做？&lt;/p&gt;&#xA;&lt;h3 id=&#34;你和gpt都不知道&#34;&gt;&#xA;  你和GPT都不知道&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bd%a0%e5%92%8cgpt%e9%83%bd%e4%b8%8d%e7%9f%a5%e9%81%93&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;如果xxx，这对社会会产生什么影响？&lt;/p&gt;&#xA;&lt;h1 id=&#34;达克效应&#34;&gt;&#xA;  达克效应&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%be%be%e5%85%8b%e6%95%88%e5%ba%94&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;检验自己认知能力水平提问句式&#34;&gt;&#xA;  检验自己认知/能力水平提问句式&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%a3%80%e9%aa%8c%e8%87%aa%e5%b7%b1%e8%ae%a4%e7%9f%a5%e8%83%bd%e5%8a%9b%e6%b0%b4%e5%b9%b3%e6%8f%90%e9%97%ae%e5%8f%a5%e5%bc%8f&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;1、为了测试我对xxx的理解程度，你会问我什么问题来检验我的水平，最少10个。&lt;br&gt;&#xA;2、我是xx领域的专家，你会问我哪些问题来检验我的专业水平？&lt;br&gt;&#xA;3、追问一句，这些我都懂，还有更专业更细更深的问题吗？&lt;br&gt;&#xA;4、你问我答的游戏&lt;/p&gt;&#xA;&lt;p&gt;扩展自己能力边界的提问句式我已经很精通xxx了，我想知道我是否还有需要学习的地方？然后不停的问，还有呢还有呢？&lt;/p&gt;&#xA;&lt;h1 id=&#34;知道做到&#34;&gt;&#xA;  知道做到&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9f%a5%e9%81%93%e5%81%9a%e5%88%b0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;让GPT完成具体任务&lt;br&gt;&#xA;1、我想做xxx，你能给我提供什么帮助？&lt;br&gt;&#xA;2、我想要你做xxx，我应该给你输入什么信息？&lt;br&gt;&#xA;3、直接下指令&lt;/p&gt;&#xA;&lt;h1 id=&#34;角色关系&#34;&gt;&#xA;  角色关系&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%a7%92%e8%89%b2%e5%85%b3%e7%b3%bb&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;模拟虚拟人物&lt;/li&gt;&#xA;&lt;li&gt;模拟名人&lt;/li&gt;&#xA;&lt;li&gt;模拟一段关系&lt;/li&gt;&#xA;&lt;li&gt;模拟多个具体的人&lt;/li&gt;&#xA;&lt;li&gt;模拟多类人&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;通用&#34;&gt;&#xA;  通用&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e9%80%9a%e7%94%a8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;沟通模式&#34;&gt;&#xA;  沟通模式&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%b2%9f%e9%80%9a%e6%a8%a1%e5%bc%8f&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;prompt =  定义角色+背景信息+任务目标+输出要求&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG KG</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/KG-RAG/RAGKG/</link>
      <pubDate>Mon, 19 Jun 2023 14:24:23 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/KG-RAG/RAGKG/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h3 id=&#34;vectorkg-rag1516&#34;&gt;&#xA;  Vector+KG RAG[15][16]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#vectorkg-rag1516&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h3 id=&#34;rag-多跳问题&#34;&gt;&#xA;  RAG 多跳问题&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag-%e5%a4%9a%e8%b7%b3%e9%97%ae%e9%a2%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;rag-多跳问题-1&#34;&gt;&#xA;  RAG 多跳问题&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag-%e5%a4%9a%e8%b7%b3%e9%97%ae%e9%a2%98-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://neo4j.com/developer-blog/knowledge-graphs-llms-multi-hop-question-answering/&#34;&gt;Knowledge Graphs &amp;amp; LLMs: Multi-Hop Question Answering&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://cloud.tencent.com.cn/developer/article/2409038&#34;&gt;知识图谱和 LLM：多跳问答-腾讯云开发者社区-腾讯云&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/qq_41185868/article/details/138514051&#34;&gt;LLMs之KG-RAG：KG-RAG/GraphRAG(基于知识图谱的RAG系统)的简介(可以解决多跳问题/同时支持结构化和非结构化数据查询)、经验技巧、案例应用之详细攻略-CSDN博客&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://blog.csdn.net/qq_36931982/article/details/139118215&#34;&gt;MultiHop-RAG：多跳查询的基准检索增强生成_rag多跳查询-CSDN博客&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;llmkg--知识图谱&#34;&gt;&#xA;  LLM+KG  知识图谱&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llmkg--%e7%9f%a5%e8%af%86%e5%9b%be%e8%b0%b1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;15&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://neo4j.com/developer-blog/unstructured-knowledge-graph-neo4j-langchain/&#34;&gt;Enhanced QA Integrating Unstructured Knowledge Graph Using Neo4j and LangChain&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.langchain.dev/using-a-knowledge-graph-to-implement-a-devops-rag-application/&#34;&gt;Using a Knowledge Graph to implement a DevOps RAG application&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s/buV1j4DtDiVavtGCJIsedQ&#34;&gt;大模型辅助图谱构建的4个策略对比：兼看大模型与知识图谱结合的3个综述 &lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG vs SFT</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAGSFT/</link>
      <pubDate>Wed, 07 Jun 2023 17:02:42 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAGSFT/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;rag-vs-ft1&#34;&gt;&#xA;  RAG vs FT[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag-vs-ft1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;rag-vs-微调场景&#34;&gt;&#xA;  RAG vs 微调[场景]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag-vs-%e5%be%ae%e8%b0%83%e5%9c%ba%e6%99%af&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;动态数据：RAG&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;模型能力定制：微调&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;幻觉：RAG &amp;gt; 微调&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;可解释性：RAG&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;成本：RAG&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;依赖通用能力：RAG&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;​       微调会有灾难性的遗忘&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;延迟：微调&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;​       rag的流程长&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;智能设备：微调&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;【rag和微调可以一起使用】&lt;/p&gt;&#xA;&lt;h3 id=&#34;应用-case&#34;&gt;&#xA;  应用 Case&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ba%94%e7%94%a8-case&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;A: 投资理财规划师   &lt;strong&gt;[用RAG]&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;处理动态数据：RAG&lt;/li&gt;&#xA;&lt;li&gt;很强的对话能力：RAG&lt;/li&gt;&#xA;&lt;li&gt;金融能力：微调 ×&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;B: 金融信息抽取Bot  &lt;strong&gt;[用微调]&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;很强的抽取能力：微调  [特定的能力]&lt;/li&gt;&#xA;&lt;li&gt;金融能力&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;C: 销售机器人   &lt;strong&gt;[用RAG+微调]&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;多轮对话/动态：RAG&lt;/li&gt;&#xA;&lt;li&gt;销售技巧/语气：微调&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;rag-vs-ft-2&#34;&gt;&#xA;  RAG vs FT [2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag-vs-ft-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/rag-vs-ft.jpg&#34; alt=&#34;rag-vs-ft.jpg&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;todo:  有中文翻译的图片&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1xM4m117FP/&#34;&gt;大模型项目选择RAG还是微调：三个案例&lt;/a&gt;  v&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1vJ4m1M7qG/&#34;&gt;大模型项目选择RAG还是微调：八个判断依据&lt;/a&gt; v&lt;/p&gt;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;#1《Retrieval-Augmented Generation for Large Language Models: A Survey》&lt;/p&gt;</description>
    </item>
    <item>
      <title>Agent 12-Factor &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/Agent12Factor/</link>
      <pubDate>Sat, 06 Apr 2024 23:18:53 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/Agent12Factor/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;agent-12-factor&#34;&gt;&#xA;  Agent 12-Factor&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agent-12-factor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/LLM-App-12factor-1e4bfe21108480faae9bf48f6b4af95e?pvs=4&#34;&gt;Agent 12-Factor&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Work)[SFT]Gorilla</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentToolGorilla/</link>
      <pubDate>Sat, 08 Apr 2023 07:58:38 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentToolGorilla/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;&lt;a href=&#34;https://ar5iv.labs.arxiv.org/html/2305.15334&#34;&gt;Gorilla: Large Language Model Connected with Massive APIs&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/ShishirPatil/gorilla&#34;&gt;gorilla&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;方法论1&#34;&gt;&#xA;  方法论[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%96%b9%e6%b3%95%e8%ae%ba1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;数据集收集&#34;&gt;&#xA;  数据集收集&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%95%b0%e6%8d%ae%e9%9b%86%e6%94%b6%e9%9b%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;API文档&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;HuggingFace平台托管和提供了约203,681个模型。然而，其中许多模型的文档质量较差，缺乏依赖项，模型卡中没有信息等问题。&lt;/li&gt;&#xA;&lt;li&gt;为了筛选出质量较好的模型，从每个领域选择了前20个模型。考虑了多模态数据领域的7个领域，CV领域的8个领域，NLP领域的12个领域，音频领域的5个领域，表格数据领域的2个领域和强化学习领域的2个领域。&lt;/li&gt;&#xA;&lt;li&gt;经过筛选，从HuggingFace获得了总共925个模型。从TensorFlow Hub获得了801个模型，并从Torch Hub获得了95个模型。&lt;/li&gt;&#xA;&lt;li&gt;这些模型的信息被转换为&lt;strong&gt;JSON对象&lt;/strong&gt;，其中包含了领域（domain）、框架（framework）、功能（functionality）、API名称（api_name）、API调用（api_call）、API参数（api_arguments）、环境要求（environment_requirements）、示例代码（example_code）、性能（performance）和描述（description）等字段。&lt;/li&gt;&#xA;&lt;li&gt;选择这些字段是为了将其泛化到机器学习领域之外的其他领域，包括RESTful API调用。&#xA;&lt;strong&gt;总而言之&lt;/strong&gt;，通过筛选和整理，从HuggingFace、TensorFlow Hub和Torch Hub等平台获取了&lt;strong&gt;总共1,645个模型&lt;/strong&gt;的信息，并将其以&lt;strong&gt;JSON对象&lt;/strong&gt;的形式进行了记录和描述。这些信息包括模型的领域、框架、功能、API调用示例、性能等，以便在机器学习和其他领域中使用和参考。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;指令生成 （Instruction Generation ）&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在&lt;strong&gt;self-instruct&lt;/strong&gt;范例[42]的指导下，使用GPT-4生成了合成指令数据。&lt;/li&gt;&#xA;&lt;li&gt;提供了三个上下文示例和一个参考API文档，要求模型生成调用API的真实世界用例。&lt;/li&gt;&#xA;&lt;li&gt;明确指示模型在创建指令时不要使用任何API名称或提示。&lt;/li&gt;&#xA;&lt;li&gt;为每个三个模型中心构建了六个示例（&lt;strong&gt;指令-API对&lt;/strong&gt;），共计18个点，这些数据是手动生成或修改的。&lt;/li&gt;&#xA;&lt;li&gt;对于&lt;strong&gt;1,645个&lt;/strong&gt;API数据点中的每一个，从相应的六个指令示例中随机选择3个，生成&lt;strong&gt;总共10个指令-API对&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;强调只需要使用GPT-4生成指令，可以与开源替代方案（如LLaMA、Alpaca等）进行交换。&#xA;&lt;strong&gt;总而言之&lt;/strong&gt;，通过使用GPT-4生成指令，并结合上下文示例和参考API文档，在每个模型中心构建了六个示例，共计18个点。这些示例被用于&lt;strong&gt;生成1,645个API数据点中的每一个的指令-API对，生成总共10个对应关系&lt;/strong&gt;。与开源替代方案相比，GPT-4的指令生成功能被应用在这个过程中。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;gorilla&#34;&gt;&#xA;  Gorilla&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gorilla&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;带有约束的API调用（API Call with Constraints）&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;API调用通常具有固有的&lt;strong&gt;约束&lt;/strong&gt;，这些约束要求LLM不仅理解API调用的功能，还要&lt;strong&gt;根据不同的约束参数对调用进行分类&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;机器学习API调用中常见的约束集是参数大小和准确性的下限。这些约束要求LLM能够根据提示理解和回答问题，例如根据提示选择参数少于10M的图像分类模型，并且至少保持70%的ImageNet准确率。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;对LLM来说，理解和推理出请求中嵌入的各种约束是一个巨大的挑战&lt;/strong&gt;。LLM需要细致地理解用户的功能描述，并能够正确地处理伴随这些调用的复杂约束。&lt;/li&gt;&#xA;&lt;li&gt;这个挑战凸显了在实际API调用中对LLM的复杂要求。仅仅理解API调用的基本功能是不够的，&lt;strong&gt;模型还必须能够应对伴随这些调用的约束，如参数大小和准确性要求&lt;/strong&gt;。&#xA;总而言之，在机器学习API调用中，LLM面临着理解和处理约束的挑战。除了理解API调用的基本功能外，LLM还需要能够识别和满足伴随调用的约束要求，如参数大小和准确性的下限。这需要模型具备更细致的理解和推理能力，以满足实际API调用的复杂需求。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/640697382&#34;&gt;Gorilla：与大规模API相连的大型语言模型&lt;/a&gt; ***&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://apposcmf8kb5033.pc.xiaoe-tech.com/live_pc/l_64a7d5afe4b09d7237a04b5b&#34;&gt;Gorilla：链接海量API的大型语言模型&lt;/a&gt; V&#xA;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/632583909&#34;&gt;大猩猩（Gorilla）🦍，连接大量 API 的大型语言模型，能成为未来AI应用的核心么？&lt;/a&gt; ***&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://gorilla.cs.berkeley.edu/&#34;&gt;Gorilla: Large Language Model Connected with Massive APIs&lt;/a&gt;&#xA;1xx. &lt;a href=&#34;https://gorilla.cs.berkeley.edu/blog.html&#34;&gt;Gorilla blog&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Work)[SFT]Toolformer</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentToolformer/</link>
      <pubDate>Fri, 03 Feb 2023 22:26:11 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentToolformer/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/abs/2302.04761&#34;&gt;Toolformer: Language Models Can Teach Themselves to Use Tools&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/lucidrains/toolformer-pytorch&#34;&gt;Implementation of Toolformer&lt;/a&gt;  git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;toolformer1&#34;&gt;&#xA;  Toolformer[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#toolformer1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;🔑关键词和摘要&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Keywords: Large-scale PLMs,  Tool Learning&lt;/li&gt;&#xA;&lt;li&gt;xxx&#xA;&lt;ul&gt;&#xA;&lt;li&gt;驱动语言模型去使用简单的模型来调用外部的工具&lt;/li&gt;&#xA;&lt;li&gt;Toolformer通过语言模型的方法去决定去调用哪些API，传入哪些参数&lt;/li&gt;&#xA;&lt;li&gt;Tooformer是在自监督层面执行的，只需要对每个API的语言描述&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;⚙️研究设计和结论&#xA;&lt;ul&gt;&#xA;&lt;li&gt;方法&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Toolformer调用示例：xxx&lt;/li&gt;&#xA;&lt;li&gt;关键要素：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;模型对工具的使用应该是自监督的，这样可以省去很大的标注开销&lt;/li&gt;&#xA;&lt;li&gt;模型应该自行地去决定在何时间，用何方法来调用工具&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;方法概要：&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;受到in-context learning的启发，给定少量的人写的关于API的描述，让模型去自行生成潜在API调用的语言建模数据&lt;/li&gt;&#xA;&lt;li&gt;构建一个自监督的Loss函数，让模型来决定哪些API的调用有助于它的语言建模的预测&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;方法细节：&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;xxx&#xA;&lt;ul&gt;&#xA;&lt;li&gt;给定一个纯文本数据集，构建出一个带有API调用的数据集，然后在此数据集上做微调&lt;/li&gt;&#xA;&lt;li&gt;第一步：使用in-context learning来生成大量的潜在可能的API调用&lt;/li&gt;&#xA;&lt;li&gt;第二步：执行这些API，返回得到结果&lt;/li&gt;&#xA;&lt;li&gt;第三步：检查返回的结果是否有助于语言模型的预测，过滤掉其他的API&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;API调用采样&#xA;&lt;ul&gt;&#xA;&lt;li&gt;给每一个API来撰写提示来鼓励模型使用这些API，例如QA的提示是 xxx&lt;/li&gt;&#xA;&lt;li&gt;对于文本的每一个位置，如果这个位置是&lt;API&gt;（即API调用的开始）的概率大于一个阈值，则将此位置保留到一个集合I中&lt;/li&gt;&#xA;&lt;li&gt;对于集合I中的每一个位置，通过模型生成最多m个API调用，并且以&lt;/API&gt;结尾（如果生成的调用没有以&lt;/API&gt;结尾，直接舍去）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;API执行&#xA;&lt;ul&gt;&#xA;&lt;li&gt;去执行所有的API调用，返回文本序列&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;API过滤&#xA;&lt;ul&gt;&#xA;&lt;li&gt;构建自监督的语言模型的loss函数&lt;/li&gt;&#xA;&lt;li&gt;第一个的含义：进行API的调用，并且使用API结果的Loss&lt;/li&gt;&#xA;&lt;li&gt;第二个的含义：空字符串的Loss和调用API但不返回结果Loss的最小值&lt;/li&gt;&#xA;&lt;li&gt;这时我们希望模型使用API并且返回结果对语言建模有帮助，且帮助很明显-&amp;gt;前者的loss显著比后者小&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;微调和推理&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在经过如上操作后，就可以得到带有API调用的数据集，然后将模型在上面进行微调&lt;/li&gt;&#xA;&lt;li&gt;当模型在解码阶段输出&amp;quot;-&amp;gt;&amp;ldquo;符号时，意味着需要调用API了，调用得到返回结果然后拼接上去&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;实验&#xA;&lt;ul&gt;&#xA;&lt;li&gt;模型：GPT-J （67亿参数）&lt;/li&gt;&#xA;&lt;li&gt;原始数据：CCNet&lt;/li&gt;&#xA;&lt;li&gt;知识探测任务LAMA&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Toolformer可以大幅超过之前的方法，甚至是GPT-3等大模型&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;数学数据集&lt;/li&gt;&#xA;&lt;li&gt;问答&lt;/li&gt;&#xA;&lt;li&gt;这里即使是Toolformer也无法超越GPT-3，可见预训练规模可以囊括更多知识&lt;/li&gt;&#xA;&lt;li&gt;模型规模的影响&lt;/li&gt;&#xA;&lt;li&gt;模型的参数量到一定规模后才拥有使用工具的能力&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;📚论文贡献&#xA;&lt;ul&gt;&#xA;&lt;li&gt;优点&#xA;&lt;ul&gt;&#xA;&lt;li&gt;将语言模型使用外部工具的进行很自然的结合&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;不需要标注大量数据，使用自监督的方法进行学习&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;缺点&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;工具无法交互，也无法链式使用（每个API调用都是独立的）&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;定义的工具尚且有限，扩展工具则需要用模型标注新的数据&lt;/li&gt;&#xA;&lt;li&gt;随着基础模型zero-shot能力的增强，这种需要构建数据并且fine-tune的做法可能会比较麻烦&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;OpenBMB BMTools: &lt;a href=&#34;https://github.com/OpenBMB/BMTools&#34;&gt;https://github.com/OpenBMB/BMTools&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV18s4y1u7nJ/&#34;&gt;清华博士带你搞懂大模型自学工具使用（Toolformer)【论文速读】&lt;/a&gt; V 有思维导图&lt;br&gt;&#xA;1xx. &lt;a href=&#34;https://finisky.github.io/toolformer-summary/&#34;&gt;使LLM善假于物: Toolformer &lt;/a&gt;&lt;br&gt;&#xA;1xx. &lt;a href=&#34;https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#external-apis&#34;&gt;Prompt Engineering &lt;/a&gt;&lt;br&gt;&#xA;1xx. &lt;a href=&#34;https://nakaizura.blog.csdn.net/article/details/130817902&#34;&gt;Toolformer and Tool Learning（LLMs如何使用工具）&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(Work)Agent-Tools</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentTool/</link>
      <pubDate>Fri, 27 Jan 2023 16:32:24 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentTool/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2304.08354.pdf&#34;&gt;Tool Learning with Foundation Models&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/thunlp/ToolLearningPapers&#34;&gt;ToolLearningPapers&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;分类1&#34;&gt;&#xA;  分类[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%86%e7%b1%bb1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;tool-augmented-vs-tool-oriented-kimi-总结&#34;&gt;&#xA;  Tool-augmented vs. Tool-oriented [kimi 总结]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tool-augmented-vs-tool-oriented-kimi-%e6%80%bb%e7%bb%93&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Tool-augmented Learning（工具增强学习）:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;这种学习方式指的是在基础模型（如大型预训练语言模型）的基础上，&lt;strong&gt;通过引入外部工具来增强模型的能力&lt;/strong&gt;。这些工具可以是任何可以被模型通过某种接口调用的系统或服务，例如搜索引擎、数据库、API等。&lt;/li&gt;&#xA;&lt;li&gt;工具增强学习的核心在于模型利用这些工具来获取额外的信息或执行特定的任务，从而弥补模型自身知识和能力的不足。&lt;/li&gt;&#xA;&lt;li&gt;例如，&lt;strong&gt;一个语言模型可能通过调用天气API来获取最新的天气信息，或者通过搜索引擎来找到相关问题的答案&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Tool-oriented Learning（面向工具的学习）:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;面向工具的学习则更多地关注于模型如何学习和理解如何使用这些工具。这不仅仅是调用工具API那么简单，而是&lt;strong&gt;涉及到模型对工具的深入理解和策略性使用&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;在这种学习模式下，模型可能需要&lt;strong&gt;学习如何组合使用多个工具&lt;/strong&gt;，或者在复杂任务中动态调整对工具的使用策略，以实现更高效的问题解决。&lt;/li&gt;&#xA;&lt;li&gt;例如，模型可能需要学习如何在&lt;strong&gt;规划一次旅行&lt;/strong&gt;时，先后调用地图API、航班搜索API和酒店预订API，同时还要根据用户反馈和环境变化动态调整计划。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;总的来说，Tool-augmented Learning 强调的是通过外部工具来扩展模型的能力，而 Tool-oriented Learning 则更侧重于模型对工具使用的学习和优化。两者都是工具学习（Tool Learning）的重要组成部分，但在实际应用中可能会有不同的实现方式和关注点。&lt;/p&gt;&#xA;&lt;h3 id=&#34;tool-augmented-learning&#34;&gt;&#xA;  Tool-augmented Learning&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tool-augmented-learning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Toolformer&lt;br&gt;&#xA;{% post_link &amp;lsquo;gptAgentToolformer&amp;rsquo; %}&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;tool-oriented-learning&#34;&gt;&#xA;  Tool-oriented Learning&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tool-oriented-learning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ToolMaker[10]&lt;/li&gt;&#xA;&lt;li&gt;CREATOR[11]&lt;/li&gt;&#xA;&lt;li&gt;ToolLLM [12]&lt;/li&gt;&#xA;&lt;li&gt;Visual ChatGPT[13]&lt;/li&gt;&#xA;&lt;li&gt;HuggingGPT[13]&lt;/li&gt;&#xA;&lt;li&gt;Gorilla&#xA;{% post_link &amp;lsquo;gptAgentToolGorilla&amp;rsquo; %}&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/624459759&#34;&gt;大模型工具学习权威综述，BMTools 背后的论文！&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://blog.csdn.net/xixiaoyaoww/article/details/130278978&#34;&gt;清华发布工具学习框架，让ChatGPT操控地图、股票查询，贾维斯已来？&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战) QLoRA *</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Lora/PEFTQLora/</link>
      <pubDate>Fri, 12 Jan 2024 10:36:44 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Lora/PEFTQLora/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;aside&gt;&#xA;💡&#xA;&lt;ul&gt;&#xA;&lt;li&gt;QAT&#xA;&lt;ul&gt;&#xA;&lt;li&gt;4 bit NormalFloat(NF4) 量化&lt;/li&gt;&#xA;&lt;li&gt;双量化&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/aside&gt;&#xA;&lt;h1 id=&#34;技术原理-1&#34;&gt;&#xA;  技术原理 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%8a%80%e6%9c%af%e5%8e%9f%e7%90%86-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;使用一种新颖的高精度技术将预训练模型量化为 4 bit，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。&lt;br&gt;&#xA;QLoRA提出了两种技术实现高保真 4 bit微调——4 bit NormalFloat(NF4) 量化和双量化。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;4bit NormalFloat（NF4）：对于正态分布权重而言，一种信息理论上最优的新数据类型，该数据类型对正态分布数据产生比 4 bit整数和 4bit 浮点数更好的实证结果。&lt;/li&gt;&#xA;&lt;li&gt;双量化：对第一次量化后的那些常量再进行一次量化，减少存储空间。&lt;/li&gt;&#xA;&lt;li&gt;分页优化器: 使用此功能为优化器状态（Optimizer）分配分页内存，然后在 GPU 内存不足时将其自动卸载到 CPU 内存，并在优化器更新步骤需要时将其加载回 GPU 内存。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/qlora.png&#34; alt=&#34;qlora.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;实验证明，无论是使用16bit、8bit还是4bit的适配器方法，都能够复制16bit全参数微调的基准性能。这说明，尽管量化过程中会存在性能损失，但通过适配器微调，完全可以恢复这些性能。&lt;/p&gt;&#xA;&lt;h1 id=&#34;总结&#34;&gt;&#xA;  总结&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%80%bb%e7%bb%93&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;QLoRA [189] quantizes the weights of LLMs into 4-bit and subsequently employs LoRA [224] in BF16 for each 4-bit weight matrix to fine-tune the quantized model. QLoRA allows for the efficient fine-tuning of a 65B parameter LLM on one GPU with only 30GB of memory.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Agent  Memory &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Memory/AgentMemory/</link>
      <pubDate>Mon, 05 Jun 2023 11:40:10 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Memory/AgentMemory/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;memory&#34;&gt;&#xA;  Memory&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#memory&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Memory-1d5bfe211084808580b3ffc0a435ee18?pvs=4&#34;&gt;Memory&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)LIMA, LESS</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/Data-Quality/Instruction-Quality/DataSFTQuality/</link>
      <pubDate>Thu, 27 Apr 2023 17:22:35 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/Data-Quality/Instruction-Quality/DataSFTQuality/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;lima-1kimi&#34;&gt;&#xA;  LIMA [1][kimi]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#lima-1kimi&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;LIMA（Less Is More for Alignment）的实验通过一系列设计精良的步骤来探究数据质量、多样性以及数量对模型性能的影响，从而得出了&lt;strong&gt;提高数据质量和增加提示多样性比单纯增加数据量更能提升模型性能的结论&lt;/strong&gt;。以下是&lt;strong&gt;实验方法&lt;/strong&gt;的关键步骤：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;精心策划的微调数据&lt;/strong&gt;：LIMA模型在&lt;strong&gt;1000个&lt;/strong&gt;精心策划的提示和回复上进行了&lt;strong&gt;微调&lt;/strong&gt;，这些数据被设计为模拟真实用户与AI助手的交互。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;消融实验&lt;/strong&gt;：通过消融实验，研究者们观察了在增加数据量的同时不增加提示多样性时，模型性能的提升是否有限；而在优化数据质量时，性能是否有显著提升。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;数据构造&lt;/strong&gt;：研究者从Stack Exchange、wikiHow和Pushshift Reddit数据集收集数据，并进行了&lt;strong&gt;质量和多样性&lt;/strong&gt;的控制。这些数据集被用来构造训练样本，以确保输入的多样性和输出的一致性。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;质量与多样性的对比&lt;/strong&gt;：研究者比较了经过质量过滤的Stack Exchange数据和同质化的wikiHow数据对模型性能的影响。结果显示，更&lt;strong&gt;多样化的Stack Exchange数据在性能上优于同质化的wikiHow数据&lt;/strong&gt;。 【多样化】&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;数量的对比&lt;/strong&gt;：研究者对从Stack Exchange抽取的指数级增加的训练集进行了测试，发现&lt;strong&gt;训练集的翻倍并没有改善响应质量&lt;/strong&gt;，从而说明单纯增加数据量并不一定能提升性能。【数量】&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;质量控制的实验&lt;/strong&gt;：研究者还比较了未经过任何质量或风格过滤的Stack Exchange数据集与经过过滤的数据集上训练的模型性能，发现&lt;strong&gt;过滤后&lt;/strong&gt;的数据集上训练的模型性能&lt;strong&gt;更优&lt;/strong&gt;。【质量】&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;人类评估&lt;/strong&gt;：为了评估LIMA模型的性能，研究者进行了人类偏好研究，将LIMA的输出与其他几个基线模型的输出进行比较，并让人群工作者选择他们更喜欢的输出。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;通过这些实验步骤，LIMA的研究得出了&lt;strong&gt;数据质量和提示多样性对于提升模型性能的重要性远超过单纯增加数据量的结论&lt;/strong&gt;。这些发现支持了“浅层对齐假说”，即模型在预训练阶段已经学习到了几乎所有知识和能力，而微调过程主要是学习与人类交互的风格和格式。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;总结 [1]&lt;/p&gt;&#xA;&lt;p&gt;消融实验显示，&lt;strong&gt;当扩大数据量而不同时扩大提示多样性时，收益会大大减少，而在优化数据质量时，收益会大大增加&lt;/strong&gt;&#xA;【&lt;strong&gt;数量&lt;/strong&gt; &amp;lt;&amp;ndash;&amp;gt; &lt;strong&gt;多样性&lt;/strong&gt;  &lt;strong&gt;质量&lt;/strong&gt;】&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;less-核心思想-10&#34;&gt;&#xA;  LESS 核心思想 [10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#less-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3-10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;通过仅给出&lt;strong&gt;少数体现特定能力的示例&lt;/strong&gt;，从大量指令数据集中&lt;strong&gt;有效地选择5%有影响力的数据&lt;/strong&gt;用于目标指令微调，结果优于全量数据集进行微调，并且所选子集在不同模型参数规模和不同模型系列中仍然普遍有效。&lt;/p&gt;&#xA;&lt;h1 id=&#34;less10kimi&#34;&gt;&#xA;  LESS[10][kimi]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#less10kimi&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;LESS（Selecting Influential Data for Targeted Instruction Tuning）的实验方法和相应的结论如下：&lt;/p&gt;&#xA;&lt;h3 id=&#34;实验方法&#34;&gt;&#xA;  实验方法：&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e9%aa%8c%e6%96%b9%e6%b3%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;热身训练（Warmup Training）&lt;/strong&gt;：使用LoRA（Low-Rank Adaptation）技术对预训练模型进行热身训练，以适应特定的数据分布。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;梯度数据存储（Gradient Data Store）&lt;/strong&gt;：构建了一个具有投影低维梯度特征的梯度数据存储，该存储可以重复用于不同的目标任务。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;数据选择算法&lt;/strong&gt;：利用数据存储和算法选择与体现特定能力的少数示例最相似的训练数据点。?&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;模型训练&lt;/strong&gt;：使用选择的数据子集来训练目标模型。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;评估&lt;/strong&gt;：在不同的下游任务上评估LESS选择的数据子集的性能，包括MMLU、TYDIQA和BBH数据集。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;结论&#34;&gt;&#xA;  结论：&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%bb%93%e8%ae%ba&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;LESS的有效性&lt;/strong&gt;：LESS&lt;strong&gt;在不同的模型中都是有效的&lt;/strong&gt;，能够在多个评估数据集上提高性能。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;数据子集的性能&lt;/strong&gt;：&lt;strong&gt;使用LESS选择的5%的数据通常优于使用完整数据集进行训练的结果&lt;/strong&gt;。这表明完整数据集可能包含与特定目标任务无关或有害的数据点。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;数据的可转移性&lt;/strong&gt;：使用较小模型选择的数据可以提高较大模型和不同模型系列的性能，证明了LESS选择的数据具有高度的可转移性。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;与其他方法的比较&lt;/strong&gt;：LESS是唯一一致有效的方法，相较于其他基线方法（如随机选择、BM25、DSIR、RDS）表现出更好的性能。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;计算成本&lt;/strong&gt;：LESS的计算成本较高，但由于其有效性，这一成本是合理的。&lt;/p&gt;</description>
    </item>
    <item>
      <title>(List)SFT数据集</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/DatasetSFTList/</link>
      <pubDate>Mon, 24 Apr 2023 17:38:55 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/DatasetSFTList/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;sft数据集12&#34;&gt;&#xA;  SFT数据集[1][2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#sft%e6%95%b0%e6%8d%ae%e9%9b%8612&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648402424&amp;amp;idx=1&amp;amp;sn=e2d26821b6e9a5a2871e0ddbca565c30&#34;&gt;大模型再总结及ChatSQL实践案例分享：大模型训练数据及工具的5张脑图总结及ChatSQL开源项目实现解析&lt;/a&gt;&#xA;1、通用指令微调数据&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/chaoswork/sft_datasets&#34;&gt;开源SFT数据集整理&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Reflection Agent *</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Reflection/AgentReflection/</link>
      <pubDate>Fri, 07 Apr 2023 02:26:20 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Reflection/AgentReflection/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;react&#34;&gt;&#xA;  ReAct&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#react&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&lt;br&gt;&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2210.03629&#34;&gt;ReAct: Synergizing Reasoning and Acting in Language Models&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/ysymyth/ReAct&#34;&gt;ReAct&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Project page&lt;br&gt;&#xA;&lt;a href=&#34;https://react-lm.github.io/&#34;&gt;ReAct: Synergizing Reasoning and Acting in Language Models&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;实现1&#34;&gt;&#xA;  实现[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e7%8e%b01&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/hotpotqa.png&#34; alt=&#34;hotpotqa&#34; /&gt;&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; json&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;folder &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;./prompts/&amp;#39;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;prompt_file &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;prompts_naive.json&amp;#39;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(folder &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; prompt_file, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; f:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    prompt_dict &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; json&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(f)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;webthink_examples &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; prompt_dict[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;webthink_simple6&amp;#39;&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;instruction &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types: &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;(2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;(3) Finish[answer], which returns the answer and finishes the task.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Here are some examples.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;webthink_prompt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; instruction &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; webthink_examples&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;reflexion&#34;&gt;&#xA;  Reflexion&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#reflexion&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;论文-1&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2303.11366&#34;&gt;Reflexion: Language Agents with Verbal Reinforcement Learning&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Wizard</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/Data-Quality/Instruction-Complexity/DataWizard/</link>
      <pubDate>Sat, 18 Mar 2023 18:57:23 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/Data-Quality/Instruction-Complexity/DataWizard/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;wizard-方法&#34;&gt;&#xA;  Wizard 方法&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#wizard-%e6%96%b9%e6%b3%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;自动指令数据进化-1&#34;&gt;&#xA;  自动指令数据进化 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%87%aa%e5%8a%a8%e6%8c%87%e4%bb%a4%e6%95%b0%e6%8d%ae%e8%bf%9b%e5%8c%96-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;1）指令进化&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;In-Depth Evolving 提示 [深度]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;五种类型的提示来增强指令&#xA;增加约束 + 深化 + 具体化 + 增加推理步骤 + 复杂化输入&lt;/li&gt;&#xA;&lt;li&gt;核心部分&#xA;&lt;strong&gt;In-Depth Evolving的提示的核心部分是 &amp;ldquo;你的目标是将一个给定的提示改写成更复杂的版本，使那些著名的人工智能系统（如ChatGPT和GPT4）更难处理。但改写后的提示必须是合理的，能被人理解，并能被人回应&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;In-Breadth Evolving提示 [广度]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;目的&#xA;旨在提高&lt;strong&gt;主题覆盖率&lt;/strong&gt;、&lt;strong&gt;技能覆盖率&lt;/strong&gt;和整体数据集的&lt;strong&gt;多样性&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;2）响应生成&lt;/p&gt;&#xA;&lt;p&gt;3）消除进化&#xA;即过滤未能进化的指令&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;质量--多样性-复杂度&#34;&gt;&#xA;  质量-&amp;gt; 多样性, 复杂度&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%b4%a8%e9%87%8f--%e5%a4%9a%e6%a0%b7%e6%80%a7-%e5%a4%8d%e6%9d%82%e5%ba%a6&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648401462&amp;amp;idx=1&amp;amp;sn=764f0302918174cea29ae22ac5760033&#34;&gt;如何构造复杂多样的微调指令数据：WizardLM复杂指令构造思想与实验分析工作总结 &lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/nlpxucan/WizardLM&#34;&gt;WizardLM&lt;/a&gt; git&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(List)Agent 产品 平台</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Platform/AgentList/</link>
      <pubDate>Sun, 05 Mar 2023 16:48:28 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Platform/AgentList/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;应用&#34;&gt;&#xA;  应用&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ba%94%e7%94%a8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;分类-101112&#34;&gt;&#xA;  分类 [10][11][12]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%86%e7%b1%bb-101112&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Action agents&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Function Call&lt;/li&gt;&#xA;&lt;li&gt;ReACT&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Simulation agents&#xA;生成式智能体， CAMEL，  Generative Agents&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Automomous Agent&#xA;&lt;strong&gt;AutoGPT&lt;/strong&gt;， &lt;strong&gt;BabyAGI&lt;/strong&gt;,  &lt;strong&gt;AutoGen&lt;/strong&gt;&#xA;&lt;strong&gt;MetaGPT&lt;/strong&gt;     ChatDev&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;跨模态Agents&#xA;HuggingGPT&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;hugginggpt&#34;&gt;&#xA;  HuggingGPT&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#hugginggpt&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h3 id=&#34;babyagi--aigc&#34;&gt;&#xA;  BabyAGI  [AIGC]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#babyagi--aigc&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Plan-and-execute agents&#xA;The &lt;strong&gt;planning&lt;/strong&gt; is almost always done &lt;strong&gt;by an LLM&lt;/strong&gt;.&#xA;The &lt;strong&gt;execution&lt;/strong&gt; is usually done by a &lt;strong&gt;separate agent (equipped with tools)&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;autogpt10&#34;&gt;&#xA;  AutoGPT[10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#autogpt10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;AutoGPT 的核心逻辑是一个 Prompt Loop，步骤如下&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;AutoGPT 会基于一定策略自动组装 Command Prompt，这些首次会包含用户输入的 Name, Role和Goals&lt;/li&gt;&#xA;&lt;li&gt;Command Prompt 的目标不是为了拿到最终结果，而是通过 GPT Chat API(Thinking 的过程)返回下一步的 Command (包含name和arguments, 如&lt;code&gt;browser_website(url = &amp;quot;www.baidu.com&amp;quot;)&lt;/code&gt; )&lt;/li&gt;&#xA;&lt;li&gt;这些 Command 都是可扩展的，每一种命令代表一种外部能力(比如爬虫、Google搜索，也包括GPT的能力)，通过这些 Command 调用返回的 Result 又会成为到 Command Prompt 的组成元素，&lt;/li&gt;&#xA;&lt;li&gt;回到第 1 步往复循环，直到拿到最终结果结果（状态为“compelete”）&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;platform20&#34;&gt;&#xA;  Platform[20]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#platform20&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;字节-coze2122&#34;&gt;&#xA;  字节 Coze[21,22]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ad%97%e8%8a%82-coze2122&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;优势:  有RAG，结构化数据&lt;br&gt;&#xA;劣势:  只能发布到飞书，微信&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)SFT 数据组合 *</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/Task-composition/DatasetSFT/</link>
      <pubDate>Mon, 06 Feb 2023 19:04:41 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/Task-composition/DatasetSFT/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;论文地址&#xA;《HOW ABILITIES IN LARGE LANGUAGE MODELS ARE AFFECTED BY SUPERVISED FINE-TUNING DATA COM- POSITION》&lt;br&gt;&#xA;keyword: SFT 数据组合&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;问题1&#34;&gt;&#xA;  问题[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e9%97%ae%e9%a2%981&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;1、推理、编码和通用能力如何随SFT数据量而变化？&lt;br&gt;&#xA;2、在SFT中结合三种能力时是否存在性能冲突？&lt;br&gt;&#xA;3、导致性能冲突的关键因素是什么？&lt;br&gt;&#xA;4、不同的SFT策略对组合数据有什么影响？&lt;/p&gt;&#xA;&lt;h1 id=&#34;实验结果1&#34;&gt;&#xA;  实验结果[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;1、不同的能力表现出不同的扩展模式，在数据量相同的情况下，&lt;strong&gt;较大的模型通常表现出更优越的性能&lt;/strong&gt;。&lt;br&gt;&#xA;2、随着数据量的持续增加，&lt;strong&gt;数学推理和代码生成能力也在不断提高&lt;/strong&gt;，&lt;strong&gt;一般能力&lt;/strong&gt;则是在样本数达到&lt;strong&gt;一千左右&lt;/strong&gt;时才得到提升，且提升速度较慢。&lt;br&gt;&#xA;3、在&lt;strong&gt;数据量较低&lt;/strong&gt;的情况下，数据组合会带来各种能力的&lt;strong&gt;提高&lt;/strong&gt;，而在&lt;strong&gt;数据量较高&lt;/strong&gt;的情况下，能力则会发生&lt;strong&gt;冲突&lt;/strong&gt;。&lt;br&gt;&#xA;4、组成&lt;strong&gt;数据量&lt;/strong&gt;会影响&lt;strong&gt;性能&lt;/strong&gt;，而&lt;strong&gt;组成比例&lt;/strong&gt;的影响则&lt;strong&gt;微乎其微&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;【模型大小】&lt;/p&gt;&#xA;&lt;p&gt;【数据数量】&lt;/p&gt;&#xA;&lt;p&gt;【数据数量 &amp;lt;&amp;ndash;&amp;gt;  多样性】？&lt;/p&gt;&#xA;&lt;p&gt;【组成比例】&lt;/p&gt;&#xA;&lt;h1 id=&#34;问题2-在sft中结合三种能力时是否存在性能冲突kimipaper&#34;&gt;&#xA;  问题2 在SFT中结合三种能力时是否存在性能冲突？[kimi][paper]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e9%97%ae%e9%a2%982-%e5%9c%a8sft%e4%b8%ad%e7%bb%93%e5%90%88%e4%b8%89%e7%a7%8d%e8%83%bd%e5%8a%9b%e6%97%b6%e6%98%af%e5%90%a6%e5%ad%98%e5%9c%a8%e6%80%a7%e8%83%bd%e5%86%b2%e7%aa%81kimipaper&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;问题2 探讨的是在监督式微调（Supervised Fine-Tuning, SFT）中结合推理、编码和通用能力时是否存在性能冲突。&lt;/p&gt;&#xA;&lt;h3 id=&#34;结论&#34;&gt;&#xA;  结论：&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%bb%93%e8%ae%ba&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;性能冲突的存在&lt;/strong&gt;：在高资源设置下，即当SFT数据集混合使用时，不同能力领域（如数学推理、编码和通用对齐能力）之间会发生性能冲突。然而，在低资源设置下，混合数据源能够提升性能。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;性能冲突与资源量的关系&lt;/strong&gt;：随着数据量的增加，特定任务的性能可能会因为其他任务的存在而下降。这表明在数据量较大时，不同任务之间可能会相互干扰，导致性能冲突。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;模型大小对性能的影响&lt;/strong&gt;：随着模型大小的增加，在低资源设置下，数学和通用能力的性能提升更加明显。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;问题3-导致性能冲突的关键因素是什么kimipaper&#34;&gt;&#xA;  问题3 导致性能冲突的关键因素是什么？[kimi][paper]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e9%97%ae%e9%a2%983-%e5%af%bc%e8%87%b4%e6%80%a7%e8%83%bd%e5%86%b2%e7%aa%81%e7%9a%84%e5%85%b3%e9%94%ae%e5%9b%a0%e7%b4%a0%e6%98%af%e4%bb%80%e4%b9%88kimipaper&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;在SFT（监督式微调）中结合推理、编码和通用能力时，导致性能冲突的关键因素包括：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;数据组成和比例&lt;/strong&gt;：当不同能力领域的数据混合在一起进行SFT时，如果&lt;strong&gt;数据量充足&lt;/strong&gt;，来自其他领域的数据可能会被视为&lt;strong&gt;噪声&lt;/strong&gt;，从而影响特定领域的性能。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;模型大小&lt;/strong&gt;：&lt;strong&gt;较大的模型&lt;/strong&gt;在相同数据量下通常表现&lt;strong&gt;更好&lt;/strong&gt;，并且在低资源设置下对于数学和通用能力的性能增益更大。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;训练策略&lt;/strong&gt;：多任务学习虽然能够保留专业能力，但对通用能力的伤害最大；而顺序训练和混合顺序训练虽然保留了通用能力，但会丢失太多的专业能力。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;数据量与能力的关系&lt;/strong&gt;：数学推理和编码能力随着数据量的增加而持续提高，而通用能力在大约一千个样本后趋于平稳。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;任务特性差异&lt;/strong&gt;：推理和编码任务需要复杂的逻辑来分解任务指令和处理非语言和符号特征，而对齐人类意图则需要多样性和理解模糊的人类指令。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;相应的结论包括：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在&lt;strong&gt;低资源设置下&lt;/strong&gt;，混合数据源可以&lt;strong&gt;提高性能&lt;/strong&gt;，但在&lt;strong&gt;高资源设置&lt;/strong&gt;下，可能会导致&lt;strong&gt;性能下降&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;数据量&lt;/strong&gt;直接&lt;strong&gt;影响力能表现&lt;/strong&gt;，而&lt;strong&gt;数据比例&lt;/strong&gt;的影响&lt;strong&gt;不显著&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;提出的&lt;strong&gt;双阶段混合微调（DMT）策略&lt;/strong&gt;有效地减轻了多任务学习中的性能冲突和顺序训练中的灾难性遗忘，实现了通用与专业能力之间的平衡。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;这些结论强调了在SFT阶段理解和解决数据组成问题对于全面提高LLMs（大型语言模型）的能力至关重要。&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战) Lora &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Lora/PEFTLora/</link>
      <pubDate>Thu, 05 Jan 2023 12:04:14 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Lora/PEFTLora/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;lora--实战&#34;&gt;&#xA;  Lora  实战&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#lora--%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/PEFT-Lora-10dbfe21108480489a27f07aba286e4f?pvs=4&#34;&gt;(实战) Lora &lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
