<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Agent on 基于LLM的系统设计与实现</title>
    <link>https://www6v.github.io/www6vAIGC/categories/Agent/</link>
    <description>Recent content in Agent on 基于LLM的系统设计与实现</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sat, 06 Apr 2024 23:18:53 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vAIGC/categories/Agent/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Agent Planning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Planning/AgentPlanning/</link>
      <pubDate>Sat, 13 May 2023 06:57:12 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Planning/AgentPlanning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;types1&#34;&gt;&#xA;  Types[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#types1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/plans.webp&#34; alt=&#34;plans&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;任务分解&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;多计划选择&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;外部规划器辅助规划&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;反思和提炼&lt;/strong&gt;[20]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;记忆增强规划&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;任务分解&#34;&gt;&#xA;  任务分解&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bb%bb%e5%8a%a1%e5%88%86%e8%a7%a3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;ReACT 范式 [2]&#xA;把&lt;strong&gt;融合了Reasoning和Acting&lt;/strong&gt;的一种范式，推理过程是浅显易懂，仅仅&lt;strong&gt;包含thought-action-observation步骤&lt;/strong&gt;，很容易判断推理的过程的正确性，使用ReAct做决策甚至超过了强化学习.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;chain-of-thought推理-问题&#xA;事实幻想（fact hallucination）和错误传递（error propagation）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Plan-and-execute agents [2]&#xA;本质上是先计划再执行，即先把用户的问题分解成一个个的子任务，然后再执行各个子任务，最后合并输出得到结果&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;patterns&#34;&gt;&#xA;  Patterns&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#patterns&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Self-ask [2]&#xA;Self-ask是一种follow-up的使用范式，仅仅包含follow-up, immediate answer步骤，至于follow-up多少个step，完全由它自己决定，估计这就是Self-ask的名字的由来。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;《Understanding the planning of LLM agents: A survey》&lt;br&gt;&#xA;&lt;a href=&#34;https://mp.weixin.qq.com/s/1POXDVJDv3ob1HqpKjb3Mg&#34;&gt;大语言模型智能体规划能力综述: 分类、任务分解、选择、反思、记忆增强 &lt;/a&gt; 翻译&lt;br&gt;&#xA;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/693264551&#34;&gt;Agent四大范式 | 综述：全面理解Agent工作原理&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/642357544&#34;&gt;2023年新生代大模型Agents技术,ReAct,Self-Ask,Plan-and-execute,以及AutoGPT, HuggingGPT等应用&lt;/a&gt; ***  论文+代码&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;{% post_link &amp;lsquo;gptAgentReflection&amp;rsquo; %} self&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s/NhpJMmIcnF57qEuUkxD4kQ&#34;&gt;AI Agent规划能力全面拆解&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://baoyu.io/translations/ai-paper/2311.11797-igniting-language-intelligence-the-hitchhikers-guide-from-chain-of-thought-reasoning-to-language-agents&#34;&gt;引领语言智能：从思维链推理到语言智能体的探索指南 [译]&lt;/a&gt; paper&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=Mzg5NTc2OTcyOQ==&amp;amp;mid=2247488040&amp;amp;idx=1&amp;amp;sn=f404a5fc2b0380eac00564046abc77d5&#34;&gt;2023年大语言模型智能体规划技术(LLM Agent Planning)研究进展汇总&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Agent 架构 &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Overview/Agent/</link>
      <pubDate>Wed, 02 Nov 2022 10:55:27 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Overview/Agent/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;agent-架构&#34;&gt;&#xA;  Agent 架构&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agent-%e6%9e%b6%e6%9e%84&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Agent-10dbfe211084806fa87cfd37aed482ea?pvs=4&#34;&gt;(原理)Agent 架构 &lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理) Agent Guide &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Overview/AgentGuide/</link>
      <pubDate>Wed, 02 Nov 2022 10:55:27 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Overview/AgentGuide/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;agent-guide&#34;&gt;&#xA;  Agent Guide&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agent-guide&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/A-Visual-Guide-to-LLM-Agents-1ecbfe21108480eeb5f9f0f6a31e821e?pvs=4&#34;&gt;(原理) Agent Guide&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理&amp;实战)AutoGen &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Multi-agent/AgentAutogen/</link>
      <pubDate>Mon, 05 Jun 2023 21:37:46 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Multi-agent/AgentAutogen/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;autogen&#34;&gt;&#xA;  AutoGen&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#autogen&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/AutoGen-1debfe211084807b9163d8c6a0162307?pvs=4&#34;&gt;(原理&amp;amp;实战)AutoGen&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战)Agent Tuning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentTuning/</link>
      <pubDate>Fri, 07 Apr 2023 16:56:18 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentTuning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;基于微调的agent---function-call12&#34;&gt;&#xA;  基于微调的Agent - Function Call[1][2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%9f%ba%e4%ba%8e%e5%be%ae%e8%b0%83%e7%9a%84agent---function-call12&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基座模型&lt;br&gt;&#xA;internLM&lt;/li&gt;&#xA;&lt;li&gt;微调框架&lt;br&gt;&#xA;xtuner&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/dirs.JPG&#34; alt=&#34;dirs&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/xtuner-agent.png&#34; alt=&#34;xtuner-agent&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;agent-tuning3&#34;&gt;&#xA;  Agent Tuning[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agent-tuning3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基座模型&lt;br&gt;&#xA;Yi-6B&lt;/li&gt;&#xA;&lt;li&gt;Datasets&lt;/li&gt;&#xA;&lt;li&gt;微调框架&lt;br&gt;&#xA;LLama-Factory&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;环境准备&#34;&gt;&#xA;  环境准备&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%8e%af%e5%a2%83%e5%87%86%e5%a4%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# source code&#xA;git clone -b v0.7.1  &amp;lt;https://github.com/hiyouga/LLaMA-Factory.git&amp;gt;&#xA;git switch -c v0.7.1&#xA;cd LLaMA-Factory&#xA;&#xA;# package 安装&#xA;conda create -n llama_factory python=3.10&#xA;conda activate llama_factory&#xA;pip install llmtuner==0.5.1&#xA;&#xA;# 环境变量&#xA;export CUDA_VISIBLE_DEVICES=0 # 使用第一块 GPU&#xA;export USE_MODELSCOPE_HUB=1 # 使用魔搭社区下载渠道&#xA;&#xA;# 阿里云必须加这句，不然页面会报异常&#xA;$ export GRADIO_ROOT_PATH=/${JUPYTER_NAME}/proxy/7860/&#xA;&#xA;# 启动&#xA;python -m llmtuner.webui.interface&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;训练流程&#34;&gt;&#xA;  训练流程&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ad%e7%bb%83%e6%b5%81%e7%a8%8b&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# flash-attn 安装&#xA;pip install flash-attn --no-build-isolation&#xA;&#xA;pip install modelscope -U&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;训练脚本&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# 训练轮数 1.0&#xA;&#xA;CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\&#xA;    --stage sft \\&#xA;    --do_train True \\&#xA;    --model_name_or_path 01ai/Yi-6B \\&#xA;    --finetuning_type lora \\&#xA;    --template default \\&#xA;    --flash_attn True \\&#xA;    --dataset_dir data \\&#xA;    --dataset glaive_toolcall,alpaca_gpt4_en,alpaca_gpt4_zh,oaast_sft_zh \\&#xA;    --cutoff_len 1024 \\&#xA;    --learning_rate 5e-05 \\&#xA;    --num_train_epochs 1.0 \\&#xA;    --max_samples 8000 \\&#xA;    --per_device_train_batch_size 4 \\&#xA;    --gradient_accumulation_steps 4 \\&#xA;    --lr_scheduler_type cosine \\&#xA;    --max_grad_norm 1.0 \\&#xA;    --logging_steps 5 \\&#xA;    --save_steps 100 \\&#xA;    --warmup_steps 0 \\&#xA;    --lora_rank 8 \\&#xA;    --lora_dropout 0.1 \\&#xA;    --lora_target all \\&#xA;    --output_dir saves/Yi-6B/lora/yi-agent-6b \\&#xA;    --fp16 True \\&#xA;    --plot_loss True&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;训练配置&#xA;&lt;img src=&#34;./images/agentTuningUI.png&#34; alt=&#34;agentTuningUI.png&#34; /&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Agent 分类[有趣|有用]</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Overview/AgentCategory/</link>
      <pubDate>Thu, 06 Apr 2023 23:18:53 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Overview/AgentCategory/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;有趣的ai更像人的ai&#34;&gt;&#xA;  有趣的AI：更像人的AI&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9c%89%e8%b6%a3%e7%9a%84ai%e6%9b%b4%e5%83%8f%e4%ba%ba%e7%9a%84ai&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;好看的皮囊--多模态&#34;&gt;&#xA;  好看的皮囊  多模态&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a5%bd%e7%9c%8b%e7%9a%84%e7%9a%ae%e5%9b%8a--%e5%a4%9a%e6%a8%a1%e6%80%81&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;多模态&lt;strong&gt;理解能力&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;多模态数据端到端预训练的模型&#xA;Gemini&lt;/li&gt;&#xA;&lt;li&gt;工程化&#xA;projection layer&lt;/li&gt;&#xA;&lt;li&gt;直接用文本去粘接 encoder、decoder 和文本大模型&lt;/li&gt;&#xA;&lt;li&gt;eg【自己动手做出Gemini演示视频的效果】&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;多模态&lt;strong&gt;生成能力&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;视频生成&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Live2D，3D 模型&lt;/li&gt;&#xA;&lt;li&gt;DeepFake&#xA;录制一个真人视频， 把视频中的人脸换成指定的人脸照片&lt;/li&gt;&#xA;&lt;li&gt;Image Animation&#xA;给定一张照片，随后根据这张照片生成一系列的对应视频&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Video Diffusion&lt;/strong&gt;&#xA;对物理世界的建模&#xA;成本最高&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;有趣的灵魂&#34;&gt;&#xA;  有趣的灵魂&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9c%89%e8%b6%a3%e7%9a%84%e7%81%b5%e9%ad%82&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;个性&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基于prompt&#xA;完整地刻画出一个人物的历史、个性、记忆和性格&#xA;长文本&lt;/li&gt;&#xA;&lt;li&gt;基于微调的 agent&#xA;&lt;ul&gt;&#xA;&lt;li&gt;更关键的还是数据&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;对话性语料&lt;/strong&gt; &amp;amp; &lt;strong&gt;事实性语料&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;第一步，我们先用对话性语料去微调他的个性和说话风格&lt;/li&gt;&#xA;&lt;li&gt;第二步，再去把事实性语料进行数据清洗后，基于各种角度提问，生成这个人物第一人称口吻的回答，这叫做&lt;strong&gt;数据增强&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;慢思考&lt;/strong&gt;与记忆&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;组件&#xA;&lt;strong&gt;记忆、情感&lt;/strong&gt;、任务规划、工具&lt;/li&gt;&#xA;&lt;li&gt;长期记忆&#xA;&lt;ul&gt;&#xA;&lt;li&gt;事实性的记忆&#xA;&lt;ul&gt;&#xA;&lt;li&gt;总结&#xA;文本总结  MemGPT&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;RAG 和信息压缩&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;长上下文  &lt;strong&gt;长上下文&lt;/strong&gt;&#xA;结合持久化 KV Cache&#xA;成本还是太高&#xA;【eg.  文本总结 + RAG】&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;程序性的记忆&#xA;&lt;ul&gt;&#xA;&lt;li&gt;few-shot&lt;/li&gt;&#xA;&lt;li&gt;微调&#xA;短期来看仍然是效果最好的路线&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;有用的ai更像工具的ai&#34;&gt;&#xA;  有用的AI：更像工具的AI&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9c%89%e7%94%a8%e7%9a%84ai%e6%9b%b4%e5%83%8f%e5%b7%a5%e5%85%b7%e7%9a%84ai&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;大模型基础能力&#34;&gt;&#xA;  大模型基础能力&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%a7%e6%a8%a1%e5%9e%8b%e5%9f%ba%e7%a1%80%e8%83%bd%e5%8a%9b&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;复杂任务的规划和分解&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;遵循复杂指令&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;自主使用工具&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;减少幻觉&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;1p-3p-产品法则&#34;&gt;&#xA;  1P-3P 产品法则&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#1p-3p-%e4%ba%a7%e5%93%81%e6%b3%95%e5%88%99&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;分类&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Work|实战)Plan&amp;Execute,ReWOO</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Planning/AgentPlanAndExecute/</link>
      <pubDate>Thu, 02 Mar 2023 09:31:47 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Planning/AgentPlanAndExecute/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;plan-and-execute-0&#34;&gt;&#xA;  Plan-and-execute [0]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#plan-and-execute-0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;原理&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Figure 2 - 基于prompt [1]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;代码&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;plan [2]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Planning Step&lt;/li&gt;&#xA;&lt;li&gt;Re-Plan Step&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;问题&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;冗余的提示和重复的执行 -&amp;gt; ReWOO&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;rewoo-0&#34;&gt;&#xA;  ReWOO [0]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rewoo-0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;原理&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Abstract [10]&#xA;增强语言模型（ALM）将大型语言模型（LLM）的推理能力与允许知识检索和执行操作的工具相结合。现有的ALM系统以交错的方式触发LLM的思考过程，同时从这些工具中获取观察结果。&lt;strong&gt;具体而言，LLM推理过程中会调用外部工具，然后在获取工具响应后停止，基于之前的响应令牌来决定下一步的操作。这种范式虽然直观且易于实现，但常常由于冗余的提示和重复的执行而导致计算复杂度极高&lt;/strong&gt;。本研究首次解决了这些挑战，提出了一种模块化的范式ReWOO（无观察推理），&lt;strong&gt;将推理过程与外部观察结果分离，从而显著减少了令牌的消耗&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;Figure 1 [10]&#xA;Planner里有格式化的#E&lt;/li&gt;&#xA;&lt;li&gt;Figure 2  [10]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;代码 [11]&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Executor-tool_execution() -&amp;gt; 状态机&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;问题&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;是否可以并行？-&amp;gt; LLMCompiler&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;llmcompiler&#34;&gt;&#xA;  LLMCompiler&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llmcompiler&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;原理&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Abstract [20]&#xA;LLM的多函数调用能力催生了基于LLM的软件开发，使其能够解决更复杂的问题。然而，当前的多函数调用方法通常需要&lt;strong&gt;针对每个函数进行顺序推理和执行，这可能导致较高的延迟、成本以及不准确的行为&lt;/strong&gt;。为了解决这个问题，我们引入了LLMCompiler，它可以&lt;strong&gt;并行执行函数，以高效地编排多函数调用&lt;/strong&gt;。LLMCompiler&lt;strong&gt;借鉴了经典编译器的原理&lt;/strong&gt;，在LLM中使用&lt;strong&gt;三个组件&lt;/strong&gt;来简化并行函数调用：（i）LLM规划器，制定执行策略和依赖关系；（ii）任务获取单元，分派和更新函数调用任务；（iii）执行器，以并行方式执行这些任务。通过LLMCompiler，用户可以指定工具以及可选的上下文示例，LLMCompiler会自动计算函数调用的优化编排。重要的是，LLMCompiler可以与LLaMA-2等开源模型以及OpenAI的GPT模型一起使用。&lt;/li&gt;&#xA;&lt;li&gt;Figure 2  [20]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;代码 [21]&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Planner&lt;/li&gt;&#xA;&lt;li&gt;Task Fetching Unit&lt;/li&gt;&#xA;&lt;li&gt;Joiner&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;plan-and-execute&#34;&gt;&#xA;  Plan-and-execute&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#plan-and-execute&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;0&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1vJ4m1s7Zn/&#34;&gt;LangGraph：Plan-Execute Agents 实战&lt;/a&gt; V&#xA;&lt;a href=&#34;https://blog.langchain.dev/planning-agents/&#34;&gt;Plan-and-Execute Agents&lt;/a&gt; ***&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.04091.pdf&#34;&gt;Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought&#xA;Reasoning by Large Language Models&lt;/a&gt;  Figure 2&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langgraph/blob/main/examples/plan-and-execute/plan-and-execute.ipynb&#34;&gt;plan-and-execute&lt;/a&gt;    git&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;rewoo&#34;&gt;&#xA;  ReWOO&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rewoo&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;10&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.18323.pdf&#34;&gt;ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langgraph/blob/main/examples/rewoo/rewoo.ipynb&#34;&gt;Reasoning without Observation&lt;/a&gt; git&lt;br&gt;&#xA;&lt;a href=&#34;https://www.bilibili.com/video/BV1Au4m1N7ix/&#34;&gt;ReWoo Agent框架代码实现&lt;/a&gt; V&lt;br&gt;&#xA;1xx.  &lt;a href=&#34;https://zhuanlan.zhihu.com/p/671491031&#34;&gt;ReWOO: 高效增强语言模型中解偶观测和推理&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;llmcompiler-1&#34;&gt;&#xA;  LLMCompiler&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llmcompiler-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;20&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2312.04511v1.pdf&#34;&gt;An LLM Compiler for Parallel Function Calling&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langgraph/blob/main/examples/llm-compiler/LLMCompiler.ipynb&#34;&gt;LLMCompiler&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(Survey) Agent 优化 &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/AgentOpt/</link>
      <pubDate>Sun, 01 Jan 2023 10:33:11 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/AgentOpt/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;agent-优化&#34;&gt;&#xA;  Agent 优化&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agent-%e4%bc%98%e5%8c%96&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/A-Survey-on-the-Optimization-of-Large-Language-Model-based-Agents-1e2bfe21108480219731da1c3a4a0c17?pvs=4&#34;&gt;(Survey) Agent 优化&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战)Agent</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/AgentPractice/</link>
      <pubDate>Sun, 01 Jan 2023 10:33:11 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/AgentPractice/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;assistant-api-3&#34;&gt;&#xA;  Assistant API [3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#assistant-api-3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;assistant-api功能介绍&#34;&gt;&#xA;  Assistant API功能介绍&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#assistant-api%e5%8a%9f%e8%83%bd%e4%bb%8b%e7%bb%8d&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;从功能实现层面来说，Assistant API是截至目前最完整、性能最强大的AI应用开发API，具体功能如下：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;首先，Assistant API前所未有的能够&lt;strong&gt;调用OpenAI各模型的各项能力&lt;/strong&gt;，包括可以调用Chat系列模型（即GPT系列模型）完成文本对话、调用DALL·E 3进行绘图、调用GPT-4-vision进行图像识别、以及调用Text-to-Speech模型进行语音转文字等，并且支持在一轮对话中调用不同模型；&lt;/li&gt;&#xA;&lt;li&gt;其次，Assistant API还&lt;strong&gt;内置了代码解释器功能（Code interpreter）和海量文本信息提取功能（Knowledge retrieval）&lt;strong&gt;同时也一如既往支持借助&lt;/strong&gt;Function calling&lt;/strong&gt;进行模型功能层面拓展，此外，非常重要的是，Assistant API还支持在一轮对话中调用多个工具；&lt;/li&gt;&#xA;&lt;li&gt;其三，此外对于开发者非常友好的一点是，Assistant API最小运行单元为持久化的线程对象（persistent Threads），因此在实际运行Assistant API时，不仅能可以精确控制每一步的执行过程，同时persistent Threads也会保留每轮对话的核心信息，并且当超出模型接收信息最大上下文限制时能够自动删除早期信息，从而实现对模型短期记忆的合理管理；&lt;/li&gt;&#xA;&lt;li&gt;其四，Assistant API还能够直&lt;strong&gt;接连接OpenAI在线文档库&lt;/strong&gt;，即如果用户将外部文档保存在OpenAI云空间内，则可以在调用Assistant API时实时访问文档库中的任意文件，甚至可以在不同线程中调用不同的文档。而在借助Assistant API的Knowledge retrieval功能，则可以让大模型实时获取这些文件信息，并且合理管理短期记忆；&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;实战&#34;&gt;&#xA;  实战&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;lagent--agentlego4&#34;&gt;&#xA;  Lagent &amp;amp; AgentLego[4]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#lagent--agentlego4&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Assistant API详解与Agent开发实战-九天Hector&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/InternLM/Tutorial/tree/camp2/agent&#34;&gt;Lagent &amp;amp; AgentLego 智能体应用搭建&lt;/a&gt;&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/InternLM/Tutorial/blob/camp2/agent/lagent.md&#34;&gt;Lagent：轻量级智能体框架&lt;/a&gt;&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/InternLM/Tutorial/blob/camp2/agent/agentlego.md&#34;&gt;AgentLego：组装智能体“乐高”&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://qwenlm.github.io/zh/blog/qwen-agent-2405/&#34;&gt;使用Qwen-Agent将上下文记忆扩展到百万量级&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Agent Challenge</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/AgentChallenge/</link>
      <pubDate>Sat, 13 May 2023 07:17:37 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/AgentChallenge/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;问题和局限性-4&#34;&gt;&#xA;  问题和局限性 [4]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e9%97%ae%e9%a2%98%e5%92%8c%e5%b1%80%e9%99%90%e6%80%a7-4&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;记忆召回问题&lt;br&gt;&#xA;只是做简单的 embedding 相似性召回，很容易发现召回的结果不是很好&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;错误累积问题&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;探索效率问题&lt;br&gt;&#xA;中途引入人工的判断干预和反馈输入&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;任务终止与结果验证&lt;br&gt;&#xA;模型 agent 的工作如何终止也是一个挑战&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;挑战-8&#34;&gt;&#xA;  挑战 [8]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%8c%91%e6%88%98-8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;如何让-agent-选择合适的工具&#34;&gt;&#xA;  如何让 agent 选择合适的工具&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a6%82%e4%bd%95%e8%ae%a9-agent-%e9%80%89%e6%8b%a9%e5%90%88%e9%80%82%e7%9a%84%e5%b7%a5%e5%85%b7&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Toolformer - fine tune&lt;/li&gt;&#xA;&lt;li&gt;Gorilla - retrieval，fine tune&lt;br&gt;&#xA;【solution: SFT or RL】&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;不必要的工具使用&#34;&gt;&#xA;  不必要的工具使用&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%b8%8d%e5%bf%85%e8%a6%81%e7%9a%84%e5%b7%a5%e5%85%b7%e4%bd%bf%e7%94%a8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;“Human Input”也写成一种工具，让模型来主动发起对人类的提问&lt;br&gt;&#xA;&lt;a href=&#34;https://python.langchain.com/docs/integrations/tools/human_tools&#34;&gt;Human as a tool&lt;/a&gt;&lt;br&gt;&#xA;【solution: human-in-the-loop】&lt;/p&gt;&#xA;&lt;h3 id=&#34;agent-返回的格式不稳定&#34;&gt;&#xA;  Agent 返回的格式不稳定&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agent-%e8%bf%94%e5%9b%9e%e7%9a%84%e6%a0%bc%e5%bc%8f%e4%b8%8d%e7%a8%b3%e5%ae%9a&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;这里常见的做法是让 LLM &lt;strong&gt;按照 json 这类常见的 schema 来返回&lt;/strong&gt;，一般稳定性会高一些（相比“Action:”这种）。&lt;br&gt;&#xA;此外自动修复重试也很实用，可以利用 LangChain 里的 &lt;strong&gt;output parsers&lt;/strong&gt; 来帮助完成。&lt;br&gt;&#xA;【solution: json output】&lt;/p&gt;&#xA;&lt;h3 id=&#34;记住之前的操作避免重复&#34;&gt;&#xA;  记住之前的操作，避免重复&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%b0%e4%bd%8f%e4%b9%8b%e5%89%8d%e7%9a%84%e6%93%8d%e4%bd%9c%e9%81%bf%e5%85%8d%e9%87%8d%e5%a4%8d&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;AutoGPT - retrieval 结合近期操作记录&lt;br&gt;&#xA;【solution: memory】&lt;/p&gt;</description>
    </item>
    <item>
      <title>Agent 12-Factor &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/Agent12Factor/</link>
      <pubDate>Sat, 06 Apr 2024 23:18:53 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/Agent12Factor/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;agent-12-factor&#34;&gt;&#xA;  Agent 12-Factor&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agent-12-factor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/LLM-App-12factor-1e4bfe21108480faae9bf48f6b4af95e?pvs=4&#34;&gt;Agent 12-Factor&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Work)[SFT]Gorilla</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentToolGorilla/</link>
      <pubDate>Sat, 08 Apr 2023 07:58:38 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentToolGorilla/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;&lt;a href=&#34;https://ar5iv.labs.arxiv.org/html/2305.15334&#34;&gt;Gorilla: Large Language Model Connected with Massive APIs&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/ShishirPatil/gorilla&#34;&gt;gorilla&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;方法论1&#34;&gt;&#xA;  方法论[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%96%b9%e6%b3%95%e8%ae%ba1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;数据集收集&#34;&gt;&#xA;  数据集收集&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%95%b0%e6%8d%ae%e9%9b%86%e6%94%b6%e9%9b%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;API文档&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;HuggingFace平台托管和提供了约203,681个模型。然而，其中许多模型的文档质量较差，缺乏依赖项，模型卡中没有信息等问题。&lt;/li&gt;&#xA;&lt;li&gt;为了筛选出质量较好的模型，从每个领域选择了前20个模型。考虑了多模态数据领域的7个领域，CV领域的8个领域，NLP领域的12个领域，音频领域的5个领域，表格数据领域的2个领域和强化学习领域的2个领域。&lt;/li&gt;&#xA;&lt;li&gt;经过筛选，从HuggingFace获得了总共925个模型。从TensorFlow Hub获得了801个模型，并从Torch Hub获得了95个模型。&lt;/li&gt;&#xA;&lt;li&gt;这些模型的信息被转换为&lt;strong&gt;JSON对象&lt;/strong&gt;，其中包含了领域（domain）、框架（framework）、功能（functionality）、API名称（api_name）、API调用（api_call）、API参数（api_arguments）、环境要求（environment_requirements）、示例代码（example_code）、性能（performance）和描述（description）等字段。&lt;/li&gt;&#xA;&lt;li&gt;选择这些字段是为了将其泛化到机器学习领域之外的其他领域，包括RESTful API调用。&#xA;&lt;strong&gt;总而言之&lt;/strong&gt;，通过筛选和整理，从HuggingFace、TensorFlow Hub和Torch Hub等平台获取了&lt;strong&gt;总共1,645个模型&lt;/strong&gt;的信息，并将其以&lt;strong&gt;JSON对象&lt;/strong&gt;的形式进行了记录和描述。这些信息包括模型的领域、框架、功能、API调用示例、性能等，以便在机器学习和其他领域中使用和参考。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;指令生成 （Instruction Generation ）&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在&lt;strong&gt;self-instruct&lt;/strong&gt;范例[42]的指导下，使用GPT-4生成了合成指令数据。&lt;/li&gt;&#xA;&lt;li&gt;提供了三个上下文示例和一个参考API文档，要求模型生成调用API的真实世界用例。&lt;/li&gt;&#xA;&lt;li&gt;明确指示模型在创建指令时不要使用任何API名称或提示。&lt;/li&gt;&#xA;&lt;li&gt;为每个三个模型中心构建了六个示例（&lt;strong&gt;指令-API对&lt;/strong&gt;），共计18个点，这些数据是手动生成或修改的。&lt;/li&gt;&#xA;&lt;li&gt;对于&lt;strong&gt;1,645个&lt;/strong&gt;API数据点中的每一个，从相应的六个指令示例中随机选择3个，生成&lt;strong&gt;总共10个指令-API对&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;强调只需要使用GPT-4生成指令，可以与开源替代方案（如LLaMA、Alpaca等）进行交换。&#xA;&lt;strong&gt;总而言之&lt;/strong&gt;，通过使用GPT-4生成指令，并结合上下文示例和参考API文档，在每个模型中心构建了六个示例，共计18个点。这些示例被用于&lt;strong&gt;生成1,645个API数据点中的每一个的指令-API对，生成总共10个对应关系&lt;/strong&gt;。与开源替代方案相比，GPT-4的指令生成功能被应用在这个过程中。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;gorilla&#34;&gt;&#xA;  Gorilla&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gorilla&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;带有约束的API调用（API Call with Constraints）&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;API调用通常具有固有的&lt;strong&gt;约束&lt;/strong&gt;，这些约束要求LLM不仅理解API调用的功能，还要&lt;strong&gt;根据不同的约束参数对调用进行分类&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;机器学习API调用中常见的约束集是参数大小和准确性的下限。这些约束要求LLM能够根据提示理解和回答问题，例如根据提示选择参数少于10M的图像分类模型，并且至少保持70%的ImageNet准确率。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;对LLM来说，理解和推理出请求中嵌入的各种约束是一个巨大的挑战&lt;/strong&gt;。LLM需要细致地理解用户的功能描述，并能够正确地处理伴随这些调用的复杂约束。&lt;/li&gt;&#xA;&lt;li&gt;这个挑战凸显了在实际API调用中对LLM的复杂要求。仅仅理解API调用的基本功能是不够的，&lt;strong&gt;模型还必须能够应对伴随这些调用的约束，如参数大小和准确性要求&lt;/strong&gt;。&#xA;总而言之，在机器学习API调用中，LLM面临着理解和处理约束的挑战。除了理解API调用的基本功能外，LLM还需要能够识别和满足伴随调用的约束要求，如参数大小和准确性的下限。这需要模型具备更细致的理解和推理能力，以满足实际API调用的复杂需求。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/640697382&#34;&gt;Gorilla：与大规模API相连的大型语言模型&lt;/a&gt; ***&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://apposcmf8kb5033.pc.xiaoe-tech.com/live_pc/l_64a7d5afe4b09d7237a04b5b&#34;&gt;Gorilla：链接海量API的大型语言模型&lt;/a&gt; V&#xA;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/632583909&#34;&gt;大猩猩（Gorilla）🦍，连接大量 API 的大型语言模型，能成为未来AI应用的核心么？&lt;/a&gt; ***&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://gorilla.cs.berkeley.edu/&#34;&gt;Gorilla: Large Language Model Connected with Massive APIs&lt;/a&gt;&#xA;1xx. &lt;a href=&#34;https://gorilla.cs.berkeley.edu/blog.html&#34;&gt;Gorilla blog&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Work)Agent-Tools</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentTool/</link>
      <pubDate>Fri, 27 Jan 2023 16:32:24 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentTool/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2304.08354.pdf&#34;&gt;Tool Learning with Foundation Models&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/thunlp/ToolLearningPapers&#34;&gt;ToolLearningPapers&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;分类1&#34;&gt;&#xA;  分类[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%86%e7%b1%bb1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;tool-augmented-vs-tool-oriented-kimi-总结&#34;&gt;&#xA;  Tool-augmented vs. Tool-oriented [kimi 总结]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tool-augmented-vs-tool-oriented-kimi-%e6%80%bb%e7%bb%93&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Tool-augmented Learning（工具增强学习）:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;这种学习方式指的是在基础模型（如大型预训练语言模型）的基础上，&lt;strong&gt;通过引入外部工具来增强模型的能力&lt;/strong&gt;。这些工具可以是任何可以被模型通过某种接口调用的系统或服务，例如搜索引擎、数据库、API等。&lt;/li&gt;&#xA;&lt;li&gt;工具增强学习的核心在于模型利用这些工具来获取额外的信息或执行特定的任务，从而弥补模型自身知识和能力的不足。&lt;/li&gt;&#xA;&lt;li&gt;例如，&lt;strong&gt;一个语言模型可能通过调用天气API来获取最新的天气信息，或者通过搜索引擎来找到相关问题的答案&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Tool-oriented Learning（面向工具的学习）:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;面向工具的学习则更多地关注于模型如何学习和理解如何使用这些工具。这不仅仅是调用工具API那么简单，而是&lt;strong&gt;涉及到模型对工具的深入理解和策略性使用&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;在这种学习模式下，模型可能需要&lt;strong&gt;学习如何组合使用多个工具&lt;/strong&gt;，或者在复杂任务中动态调整对工具的使用策略，以实现更高效的问题解决。&lt;/li&gt;&#xA;&lt;li&gt;例如，模型可能需要学习如何在&lt;strong&gt;规划一次旅行&lt;/strong&gt;时，先后调用地图API、航班搜索API和酒店预订API，同时还要根据用户反馈和环境变化动态调整计划。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;总的来说，Tool-augmented Learning 强调的是通过外部工具来扩展模型的能力，而 Tool-oriented Learning 则更侧重于模型对工具使用的学习和优化。两者都是工具学习（Tool Learning）的重要组成部分，但在实际应用中可能会有不同的实现方式和关注点。&lt;/p&gt;&#xA;&lt;h3 id=&#34;tool-augmented-learning&#34;&gt;&#xA;  Tool-augmented Learning&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tool-augmented-learning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Toolformer&lt;br&gt;&#xA;{% post_link &amp;lsquo;gptAgentToolformer&amp;rsquo; %}&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;tool-oriented-learning&#34;&gt;&#xA;  Tool-oriented Learning&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tool-oriented-learning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ToolMaker[10]&lt;/li&gt;&#xA;&lt;li&gt;CREATOR[11]&lt;/li&gt;&#xA;&lt;li&gt;ToolLLM [12]&lt;/li&gt;&#xA;&lt;li&gt;Visual ChatGPT[13]&lt;/li&gt;&#xA;&lt;li&gt;HuggingGPT[13]&lt;/li&gt;&#xA;&lt;li&gt;Gorilla&#xA;{% post_link &amp;lsquo;gptAgentToolGorilla&amp;rsquo; %}&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/624459759&#34;&gt;大模型工具学习权威综述，BMTools 背后的论文！&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://blog.csdn.net/xixiaoyaoww/article/details/130278978&#34;&gt;清华发布工具学习框架，让ChatGPT操控地图、股票查询，贾维斯已来？&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Agent  Memory &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Memory/AgentMemory/</link>
      <pubDate>Mon, 05 Jun 2023 11:40:10 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Memory/AgentMemory/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;memory&#34;&gt;&#xA;  Memory&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#memory&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Memory-1d5bfe211084808580b3ffc0a435ee18?pvs=4&#34;&gt;Memory&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reflection Agent *</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Reflection/AgentReflection/</link>
      <pubDate>Fri, 07 Apr 2023 02:26:20 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Reflection/AgentReflection/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;react&#34;&gt;&#xA;  ReAct&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#react&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&lt;br&gt;&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2210.03629&#34;&gt;ReAct: Synergizing Reasoning and Acting in Language Models&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/ysymyth/ReAct&#34;&gt;ReAct&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Project page&lt;br&gt;&#xA;&lt;a href=&#34;https://react-lm.github.io/&#34;&gt;ReAct: Synergizing Reasoning and Acting in Language Models&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;实现1&#34;&gt;&#xA;  实现[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e7%8e%b01&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/hotpotqa.png&#34; alt=&#34;hotpotqa&#34; /&gt;&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; json&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;folder &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;./prompts/&amp;#39;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;prompt_file &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;prompts_naive.json&amp;#39;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(folder &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; prompt_file, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; f:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    prompt_dict &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; json&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(f)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;webthink_examples &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; prompt_dict[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;webthink_simple6&amp;#39;&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;instruction &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types: &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;(2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;(3) Finish[answer], which returns the answer and finishes the task.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Here are some examples.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;webthink_prompt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; instruction &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; webthink_examples&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;reflexion&#34;&gt;&#xA;  Reflexion&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#reflexion&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;论文-1&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2303.11366&#34;&gt;Reflexion: Language Agents with Verbal Reinforcement Learning&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(List)Agent 产品 平台</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Platform/AgentList/</link>
      <pubDate>Sun, 05 Mar 2023 16:48:28 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Platform/AgentList/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;应用&#34;&gt;&#xA;  应用&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ba%94%e7%94%a8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;分类-101112&#34;&gt;&#xA;  分类 [10][11][12]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%86%e7%b1%bb-101112&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Action agents&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Function Call&lt;/li&gt;&#xA;&lt;li&gt;ReACT&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Simulation agents&#xA;生成式智能体， CAMEL，  Generative Agents&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Automomous Agent&#xA;&lt;strong&gt;AutoGPT&lt;/strong&gt;， &lt;strong&gt;BabyAGI&lt;/strong&gt;,  &lt;strong&gt;AutoGen&lt;/strong&gt;&#xA;&lt;strong&gt;MetaGPT&lt;/strong&gt;     ChatDev&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;跨模态Agents&#xA;HuggingGPT&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;hugginggpt&#34;&gt;&#xA;  HuggingGPT&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#hugginggpt&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h3 id=&#34;babyagi--aigc&#34;&gt;&#xA;  BabyAGI  [AIGC]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#babyagi--aigc&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Plan-and-execute agents&#xA;The &lt;strong&gt;planning&lt;/strong&gt; is almost always done &lt;strong&gt;by an LLM&lt;/strong&gt;.&#xA;The &lt;strong&gt;execution&lt;/strong&gt; is usually done by a &lt;strong&gt;separate agent (equipped with tools)&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;autogpt10&#34;&gt;&#xA;  AutoGPT[10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#autogpt10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;AutoGPT 的核心逻辑是一个 Prompt Loop，步骤如下&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;AutoGPT 会基于一定策略自动组装 Command Prompt，这些首次会包含用户输入的 Name, Role和Goals&lt;/li&gt;&#xA;&lt;li&gt;Command Prompt 的目标不是为了拿到最终结果，而是通过 GPT Chat API(Thinking 的过程)返回下一步的 Command (包含name和arguments, 如&lt;code&gt;browser_website(url = &amp;quot;www.baidu.com&amp;quot;)&lt;/code&gt; )&lt;/li&gt;&#xA;&lt;li&gt;这些 Command 都是可扩展的，每一种命令代表一种外部能力(比如爬虫、Google搜索，也包括GPT的能力)，通过这些 Command 调用返回的 Result 又会成为到 Command Prompt 的组成元素，&lt;/li&gt;&#xA;&lt;li&gt;回到第 1 步往复循环，直到拿到最终结果结果（状态为“compelete”）&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;platform20&#34;&gt;&#xA;  Platform[20]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#platform20&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;字节-coze2122&#34;&gt;&#xA;  字节 Coze[21,22]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ad%97%e8%8a%82-coze2122&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;优势:  有RAG，结构化数据&lt;br&gt;&#xA;劣势:  只能发布到飞书，微信&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
