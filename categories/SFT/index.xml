<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SFT on 基于LLM的系统设计与实现</title>
    <link>https://www6v.github.io/www6vAIGC/categories/SFT/</link>
    <description>Recent content in SFT on 基于LLM的系统设计与实现</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Mon, 24 Apr 2023 17:38:55 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vAIGC/categories/SFT/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(List)SFT数据集</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/DatasetSFTList/</link>
      <pubDate>Mon, 24 Apr 2023 17:38:55 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/DatasetSFTList/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;sft数据集12&#34;&gt;&#xA;  SFT数据集[1][2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#sft%e6%95%b0%e6%8d%ae%e9%9b%8612&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648402424&amp;amp;idx=1&amp;amp;sn=e2d26821b6e9a5a2871e0ddbca565c30&#34;&gt;大模型再总结及ChatSQL实践案例分享：大模型训练数据及工具的5张脑图总结及ChatSQL开源项目实现解析&lt;/a&gt;&#xA;1、通用指令微调数据&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/chaoswork/sft_datasets&#34;&gt;开源SFT数据集整理&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(原理)SFT 数据组合 *</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/Task-composition/DatasetSFT/</link>
      <pubDate>Mon, 06 Feb 2023 19:04:41 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/Data/Task-composition/DatasetSFT/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;论文地址&#xA;《HOW ABILITIES IN LARGE LANGUAGE MODELS ARE AFFECTED BY SUPERVISED FINE-TUNING DATA COM- POSITION》&lt;br&gt;&#xA;keyword: SFT 数据组合&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;问题1&#34;&gt;&#xA;  问题[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e9%97%ae%e9%a2%981&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;1、推理、编码和通用能力如何随SFT数据量而变化？&lt;br&gt;&#xA;2、在SFT中结合三种能力时是否存在性能冲突？&lt;br&gt;&#xA;3、导致性能冲突的关键因素是什么？&lt;br&gt;&#xA;4、不同的SFT策略对组合数据有什么影响？&lt;/p&gt;&#xA;&lt;h1 id=&#34;实验结果1&#34;&gt;&#xA;  实验结果[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;1、不同的能力表现出不同的扩展模式，在数据量相同的情况下，&lt;strong&gt;较大的模型通常表现出更优越的性能&lt;/strong&gt;。&lt;br&gt;&#xA;2、随着数据量的持续增加，&lt;strong&gt;数学推理和代码生成能力也在不断提高&lt;/strong&gt;，&lt;strong&gt;一般能力&lt;/strong&gt;则是在样本数达到&lt;strong&gt;一千左右&lt;/strong&gt;时才得到提升，且提升速度较慢。&lt;br&gt;&#xA;3、在&lt;strong&gt;数据量较低&lt;/strong&gt;的情况下，数据组合会带来各种能力的&lt;strong&gt;提高&lt;/strong&gt;，而在&lt;strong&gt;数据量较高&lt;/strong&gt;的情况下，能力则会发生&lt;strong&gt;冲突&lt;/strong&gt;。&lt;br&gt;&#xA;4、组成&lt;strong&gt;数据量&lt;/strong&gt;会影响&lt;strong&gt;性能&lt;/strong&gt;，而&lt;strong&gt;组成比例&lt;/strong&gt;的影响则&lt;strong&gt;微乎其微&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;【模型大小】&lt;/p&gt;&#xA;&lt;p&gt;【数据数量】&lt;/p&gt;&#xA;&lt;p&gt;【数据数量 &amp;lt;&amp;ndash;&amp;gt;  多样性】？&lt;/p&gt;&#xA;&lt;p&gt;【组成比例】&lt;/p&gt;&#xA;&lt;h1 id=&#34;问题2-在sft中结合三种能力时是否存在性能冲突kimipaper&#34;&gt;&#xA;  问题2 在SFT中结合三种能力时是否存在性能冲突？[kimi][paper]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e9%97%ae%e9%a2%982-%e5%9c%a8sft%e4%b8%ad%e7%bb%93%e5%90%88%e4%b8%89%e7%a7%8d%e8%83%bd%e5%8a%9b%e6%97%b6%e6%98%af%e5%90%a6%e5%ad%98%e5%9c%a8%e6%80%a7%e8%83%bd%e5%86%b2%e7%aa%81kimipaper&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;问题2 探讨的是在监督式微调（Supervised Fine-Tuning, SFT）中结合推理、编码和通用能力时是否存在性能冲突。&lt;/p&gt;&#xA;&lt;h3 id=&#34;结论&#34;&gt;&#xA;  结论：&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%bb%93%e8%ae%ba&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;性能冲突的存在&lt;/strong&gt;：在高资源设置下，即当SFT数据集混合使用时，不同能力领域（如数学推理、编码和通用对齐能力）之间会发生性能冲突。然而，在低资源设置下，混合数据源能够提升性能。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;性能冲突与资源量的关系&lt;/strong&gt;：随着数据量的增加，特定任务的性能可能会因为其他任务的存在而下降。这表明在数据量较大时，不同任务之间可能会相互干扰，导致性能冲突。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;模型大小对性能的影响&lt;/strong&gt;：随着模型大小的增加，在低资源设置下，数学和通用能力的性能提升更加明显。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;问题3-导致性能冲突的关键因素是什么kimipaper&#34;&gt;&#xA;  问题3 导致性能冲突的关键因素是什么？[kimi][paper]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e9%97%ae%e9%a2%983-%e5%af%bc%e8%87%b4%e6%80%a7%e8%83%bd%e5%86%b2%e7%aa%81%e7%9a%84%e5%85%b3%e9%94%ae%e5%9b%a0%e7%b4%a0%e6%98%af%e4%bb%80%e4%b9%88kimipaper&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;在SFT（监督式微调）中结合推理、编码和通用能力时，导致性能冲突的关键因素包括：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;数据组成和比例&lt;/strong&gt;：当不同能力领域的数据混合在一起进行SFT时，如果&lt;strong&gt;数据量充足&lt;/strong&gt;，来自其他领域的数据可能会被视为&lt;strong&gt;噪声&lt;/strong&gt;，从而影响特定领域的性能。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;模型大小&lt;/strong&gt;：&lt;strong&gt;较大的模型&lt;/strong&gt;在相同数据量下通常表现&lt;strong&gt;更好&lt;/strong&gt;，并且在低资源设置下对于数学和通用能力的性能增益更大。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;训练策略&lt;/strong&gt;：多任务学习虽然能够保留专业能力，但对通用能力的伤害最大；而顺序训练和混合顺序训练虽然保留了通用能力，但会丢失太多的专业能力。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;数据量与能力的关系&lt;/strong&gt;：数学推理和编码能力随着数据量的增加而持续提高，而通用能力在大约一千个样本后趋于平稳。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;任务特性差异&lt;/strong&gt;：推理和编码任务需要复杂的逻辑来分解任务指令和处理非语言和符号特征，而对齐人类意图则需要多样性和理解模糊的人类指令。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;相应的结论包括：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在&lt;strong&gt;低资源设置下&lt;/strong&gt;，混合数据源可以&lt;strong&gt;提高性能&lt;/strong&gt;，但在&lt;strong&gt;高资源设置&lt;/strong&gt;下，可能会导致&lt;strong&gt;性能下降&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;数据量&lt;/strong&gt;直接&lt;strong&gt;影响力能表现&lt;/strong&gt;，而&lt;strong&gt;数据比例&lt;/strong&gt;的影响&lt;strong&gt;不显著&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;提出的&lt;strong&gt;双阶段混合微调（DMT）策略&lt;/strong&gt;有效地减轻了多任务学习中的性能冲突和顺序训练中的灾难性遗忘，实现了通用与专业能力之间的平衡。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;这些结论强调了在SFT阶段理解和解决数据组成问题对于全面提高LLMs（大型语言模型）的能力至关重要。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
