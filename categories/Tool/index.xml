<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tool on 基于LLM的系统设计与实现</title>
    <link>https://www6v.github.io/www6vAIGC/categories/Tool/</link>
    <description>Recent content in Tool on 基于LLM的系统设计与实现</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Fri, 03 Feb 2023 22:26:11 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vAIGC/categories/Tool/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(Work)[SFT]Toolformer</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentToolformer/</link>
      <pubDate>Fri, 03 Feb 2023 22:26:11 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentToolformer/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/abs/2302.04761&#34;&gt;Toolformer: Language Models Can Teach Themselves to Use Tools&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/lucidrains/toolformer-pytorch&#34;&gt;Implementation of Toolformer&lt;/a&gt;  git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;toolformer1&#34;&gt;&#xA;  Toolformer[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#toolformer1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;🔑关键词和摘要&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Keywords: Large-scale PLMs,  Tool Learning&lt;/li&gt;&#xA;&lt;li&gt;xxx&#xA;&lt;ul&gt;&#xA;&lt;li&gt;驱动语言模型去使用简单的模型来调用外部的工具&lt;/li&gt;&#xA;&lt;li&gt;Toolformer通过语言模型的方法去决定去调用哪些API，传入哪些参数&lt;/li&gt;&#xA;&lt;li&gt;Tooformer是在自监督层面执行的，只需要对每个API的语言描述&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;⚙️研究设计和结论&#xA;&lt;ul&gt;&#xA;&lt;li&gt;方法&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Toolformer调用示例：xxx&lt;/li&gt;&#xA;&lt;li&gt;关键要素：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;模型对工具的使用应该是自监督的，这样可以省去很大的标注开销&lt;/li&gt;&#xA;&lt;li&gt;模型应该自行地去决定在何时间，用何方法来调用工具&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;方法概要：&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;受到in-context learning的启发，给定少量的人写的关于API的描述，让模型去自行生成潜在API调用的语言建模数据&lt;/li&gt;&#xA;&lt;li&gt;构建一个自监督的Loss函数，让模型来决定哪些API的调用有助于它的语言建模的预测&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;方法细节：&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;xxx&#xA;&lt;ul&gt;&#xA;&lt;li&gt;给定一个纯文本数据集，构建出一个带有API调用的数据集，然后在此数据集上做微调&lt;/li&gt;&#xA;&lt;li&gt;第一步：使用in-context learning来生成大量的潜在可能的API调用&lt;/li&gt;&#xA;&lt;li&gt;第二步：执行这些API，返回得到结果&lt;/li&gt;&#xA;&lt;li&gt;第三步：检查返回的结果是否有助于语言模型的预测，过滤掉其他的API&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;API调用采样&#xA;&lt;ul&gt;&#xA;&lt;li&gt;给每一个API来撰写提示来鼓励模型使用这些API，例如QA的提示是 xxx&lt;/li&gt;&#xA;&lt;li&gt;对于文本的每一个位置，如果这个位置是&lt;API&gt;（即API调用的开始）的概率大于一个阈值，则将此位置保留到一个集合I中&lt;/li&gt;&#xA;&lt;li&gt;对于集合I中的每一个位置，通过模型生成最多m个API调用，并且以&lt;/API&gt;结尾（如果生成的调用没有以&lt;/API&gt;结尾，直接舍去）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;API执行&#xA;&lt;ul&gt;&#xA;&lt;li&gt;去执行所有的API调用，返回文本序列&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;API过滤&#xA;&lt;ul&gt;&#xA;&lt;li&gt;构建自监督的语言模型的loss函数&lt;/li&gt;&#xA;&lt;li&gt;第一个的含义：进行API的调用，并且使用API结果的Loss&lt;/li&gt;&#xA;&lt;li&gt;第二个的含义：空字符串的Loss和调用API但不返回结果Loss的最小值&lt;/li&gt;&#xA;&lt;li&gt;这时我们希望模型使用API并且返回结果对语言建模有帮助，且帮助很明显-&amp;gt;前者的loss显著比后者小&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;微调和推理&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在经过如上操作后，就可以得到带有API调用的数据集，然后将模型在上面进行微调&lt;/li&gt;&#xA;&lt;li&gt;当模型在解码阶段输出&amp;quot;-&amp;gt;&amp;ldquo;符号时，意味着需要调用API了，调用得到返回结果然后拼接上去&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;实验&#xA;&lt;ul&gt;&#xA;&lt;li&gt;模型：GPT-J （67亿参数）&lt;/li&gt;&#xA;&lt;li&gt;原始数据：CCNet&lt;/li&gt;&#xA;&lt;li&gt;知识探测任务LAMA&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Toolformer可以大幅超过之前的方法，甚至是GPT-3等大模型&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;数学数据集&lt;/li&gt;&#xA;&lt;li&gt;问答&lt;/li&gt;&#xA;&lt;li&gt;这里即使是Toolformer也无法超越GPT-3，可见预训练规模可以囊括更多知识&lt;/li&gt;&#xA;&lt;li&gt;模型规模的影响&lt;/li&gt;&#xA;&lt;li&gt;模型的参数量到一定规模后才拥有使用工具的能力&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;📚论文贡献&#xA;&lt;ul&gt;&#xA;&lt;li&gt;优点&#xA;&lt;ul&gt;&#xA;&lt;li&gt;将语言模型使用外部工具的进行很自然的结合&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;不需要标注大量数据，使用自监督的方法进行学习&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;缺点&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;工具无法交互，也无法链式使用（每个API调用都是独立的）&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;定义的工具尚且有限，扩展工具则需要用模型标注新的数据&lt;/li&gt;&#xA;&lt;li&gt;随着基础模型zero-shot能力的增强，这种需要构建数据并且fine-tune的做法可能会比较麻烦&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;OpenBMB BMTools: &lt;a href=&#34;https://github.com/OpenBMB/BMTools&#34;&gt;https://github.com/OpenBMB/BMTools&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV18s4y1u7nJ/&#34;&gt;清华博士带你搞懂大模型自学工具使用（Toolformer)【论文速读】&lt;/a&gt; V 有思维导图&lt;br&gt;&#xA;1xx. &lt;a href=&#34;https://finisky.github.io/toolformer-summary/&#34;&gt;使LLM善假于物: Toolformer &lt;/a&gt;&lt;br&gt;&#xA;1xx. &lt;a href=&#34;https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#external-apis&#34;&gt;Prompt Engineering &lt;/a&gt;&lt;br&gt;&#xA;1xx. &lt;a href=&#34;https://nakaizura.blog.csdn.net/article/details/130817902&#34;&gt;Toolformer and Tool Learning（LLMs如何使用工具）&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
  </channel>
</rss>
