<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Agent on 基于LLM的系统设计与实现</title>
    <link>https://www6v.github.io/www6vAIGC/tags/Agent/</link>
    <description>Recent content in Agent on 基于LLM的系统设计与实现</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sat, 13 May 2023 07:17:37 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vAIGC/tags/Agent/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(实战)Agent Tuning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentTuning/</link>
      <pubDate>Fri, 07 Apr 2023 16:56:18 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentTuning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;基于微调的agent---function-call12&#34;&gt;&#xA;  基于微调的Agent - Function Call[1][2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%9f%ba%e4%ba%8e%e5%be%ae%e8%b0%83%e7%9a%84agent---function-call12&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基座模型&lt;br&gt;&#xA;internLM&lt;/li&gt;&#xA;&lt;li&gt;微调框架&lt;br&gt;&#xA;xtuner&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/dirs.JPG&#34; alt=&#34;dirs&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/xtuner-agent.png&#34; alt=&#34;xtuner-agent&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;agent-tuning3&#34;&gt;&#xA;  Agent Tuning[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agent-tuning3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基座模型&lt;br&gt;&#xA;Yi-6B&lt;/li&gt;&#xA;&lt;li&gt;Datasets&lt;/li&gt;&#xA;&lt;li&gt;微调框架&lt;br&gt;&#xA;LLama-Factory&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;环境准备&#34;&gt;&#xA;  环境准备&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%8e%af%e5%a2%83%e5%87%86%e5%a4%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# source code&#xA;git clone -b v0.7.1  &amp;lt;https://github.com/hiyouga/LLaMA-Factory.git&amp;gt;&#xA;git switch -c v0.7.1&#xA;cd LLaMA-Factory&#xA;&#xA;# package 安装&#xA;conda create -n llama_factory python=3.10&#xA;conda activate llama_factory&#xA;pip install llmtuner==0.5.1&#xA;&#xA;# 环境变量&#xA;export CUDA_VISIBLE_DEVICES=0 # 使用第一块 GPU&#xA;export USE_MODELSCOPE_HUB=1 # 使用魔搭社区下载渠道&#xA;&#xA;# 阿里云必须加这句，不然页面会报异常&#xA;$ export GRADIO_ROOT_PATH=/${JUPYTER_NAME}/proxy/7860/&#xA;&#xA;# 启动&#xA;python -m llmtuner.webui.interface&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;训练流程&#34;&gt;&#xA;  训练流程&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ad%e7%bb%83%e6%b5%81%e7%a8%8b&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# flash-attn 安装&#xA;pip install flash-attn --no-build-isolation&#xA;&#xA;pip install modelscope -U&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;训练脚本&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# 训练轮数 1.0&#xA;&#xA;CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\&#xA;    --stage sft \\&#xA;    --do_train True \\&#xA;    --model_name_or_path 01ai/Yi-6B \\&#xA;    --finetuning_type lora \\&#xA;    --template default \\&#xA;    --flash_attn True \\&#xA;    --dataset_dir data \\&#xA;    --dataset glaive_toolcall,alpaca_gpt4_en,alpaca_gpt4_zh,oaast_sft_zh \\&#xA;    --cutoff_len 1024 \\&#xA;    --learning_rate 5e-05 \\&#xA;    --num_train_epochs 1.0 \\&#xA;    --max_samples 8000 \\&#xA;    --per_device_train_batch_size 4 \\&#xA;    --gradient_accumulation_steps 4 \\&#xA;    --lr_scheduler_type cosine \\&#xA;    --max_grad_norm 1.0 \\&#xA;    --logging_steps 5 \\&#xA;    --save_steps 100 \\&#xA;    --warmup_steps 0 \\&#xA;    --lora_rank 8 \\&#xA;    --lora_dropout 0.1 \\&#xA;    --lora_target all \\&#xA;    --output_dir saves/Yi-6B/lora/yi-agent-6b \\&#xA;    --fp16 True \\&#xA;    --plot_loss True&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;训练配置&#xA;&lt;img src=&#34;./images/agentTuningUI.png&#34; alt=&#34;agentTuningUI.png&#34; /&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Work|实战)Plan&amp;Execute,ReWOO</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Planning/AgentPlanAndExecute/</link>
      <pubDate>Thu, 02 Mar 2023 09:31:47 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Planning/AgentPlanAndExecute/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;plan-and-execute-0&#34;&gt;&#xA;  Plan-and-execute [0]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#plan-and-execute-0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;原理&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Figure 2 - 基于prompt [1]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;代码&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;plan [2]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Planning Step&lt;/li&gt;&#xA;&lt;li&gt;Re-Plan Step&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;问题&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;冗余的提示和重复的执行 -&amp;gt; ReWOO&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;rewoo-0&#34;&gt;&#xA;  ReWOO [0]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rewoo-0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;原理&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Abstract [10]&#xA;增强语言模型（ALM）将大型语言模型（LLM）的推理能力与允许知识检索和执行操作的工具相结合。现有的ALM系统以交错的方式触发LLM的思考过程，同时从这些工具中获取观察结果。&lt;strong&gt;具体而言，LLM推理过程中会调用外部工具，然后在获取工具响应后停止，基于之前的响应令牌来决定下一步的操作。这种范式虽然直观且易于实现，但常常由于冗余的提示和重复的执行而导致计算复杂度极高&lt;/strong&gt;。本研究首次解决了这些挑战，提出了一种模块化的范式ReWOO（无观察推理），&lt;strong&gt;将推理过程与外部观察结果分离，从而显著减少了令牌的消耗&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;Figure 1 [10]&#xA;Planner里有格式化的#E&lt;/li&gt;&#xA;&lt;li&gt;Figure 2  [10]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;代码 [11]&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Executor-tool_execution() -&amp;gt; 状态机&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;问题&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;是否可以并行？-&amp;gt; LLMCompiler&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;llmcompiler&#34;&gt;&#xA;  LLMCompiler&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llmcompiler&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;原理&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Abstract [20]&#xA;LLM的多函数调用能力催生了基于LLM的软件开发，使其能够解决更复杂的问题。然而，当前的多函数调用方法通常需要&lt;strong&gt;针对每个函数进行顺序推理和执行，这可能导致较高的延迟、成本以及不准确的行为&lt;/strong&gt;。为了解决这个问题，我们引入了LLMCompiler，它可以&lt;strong&gt;并行执行函数，以高效地编排多函数调用&lt;/strong&gt;。LLMCompiler&lt;strong&gt;借鉴了经典编译器的原理&lt;/strong&gt;，在LLM中使用&lt;strong&gt;三个组件&lt;/strong&gt;来简化并行函数调用：（i）LLM规划器，制定执行策略和依赖关系；（ii）任务获取单元，分派和更新函数调用任务；（iii）执行器，以并行方式执行这些任务。通过LLMCompiler，用户可以指定工具以及可选的上下文示例，LLMCompiler会自动计算函数调用的优化编排。重要的是，LLMCompiler可以与LLaMA-2等开源模型以及OpenAI的GPT模型一起使用。&lt;/li&gt;&#xA;&lt;li&gt;Figure 2  [20]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;代码 [21]&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Planner&lt;/li&gt;&#xA;&lt;li&gt;Task Fetching Unit&lt;/li&gt;&#xA;&lt;li&gt;Joiner&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;plan-and-execute&#34;&gt;&#xA;  Plan-and-execute&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#plan-and-execute&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;0&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1vJ4m1s7Zn/&#34;&gt;LangGraph：Plan-Execute Agents 实战&lt;/a&gt; V&#xA;&lt;a href=&#34;https://blog.langchain.dev/planning-agents/&#34;&gt;Plan-and-Execute Agents&lt;/a&gt; ***&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.04091.pdf&#34;&gt;Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought&#xA;Reasoning by Large Language Models&lt;/a&gt;  Figure 2&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langgraph/blob/main/examples/plan-and-execute/plan-and-execute.ipynb&#34;&gt;plan-and-execute&lt;/a&gt;    git&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;rewoo&#34;&gt;&#xA;  ReWOO&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rewoo&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;10&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.18323.pdf&#34;&gt;ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langgraph/blob/main/examples/rewoo/rewoo.ipynb&#34;&gt;Reasoning without Observation&lt;/a&gt; git&lt;br&gt;&#xA;&lt;a href=&#34;https://www.bilibili.com/video/BV1Au4m1N7ix/&#34;&gt;ReWoo Agent框架代码实现&lt;/a&gt; V&lt;br&gt;&#xA;1xx.  &lt;a href=&#34;https://zhuanlan.zhihu.com/p/671491031&#34;&gt;ReWOO: 高效增强语言模型中解偶观测和推理&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;llmcompiler-1&#34;&gt;&#xA;  LLMCompiler&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llmcompiler-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;20&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2312.04511v1.pdf&#34;&gt;An LLM Compiler for Parallel Function Calling&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langgraph/blob/main/examples/llm-compiler/LLMCompiler.ipynb&#34;&gt;LLMCompiler&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(原理)Agent Challenge</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/AgentChallenge/</link>
      <pubDate>Sat, 13 May 2023 07:17:37 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Practice/AgentChallenge/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;问题和局限性-4&#34;&gt;&#xA;  问题和局限性 [4]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e9%97%ae%e9%a2%98%e5%92%8c%e5%b1%80%e9%99%90%e6%80%a7-4&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;记忆召回问题&lt;br&gt;&#xA;只是做简单的 embedding 相似性召回，很容易发现召回的结果不是很好&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;错误累积问题&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;探索效率问题&lt;br&gt;&#xA;中途引入人工的判断干预和反馈输入&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;任务终止与结果验证&lt;br&gt;&#xA;模型 agent 的工作如何终止也是一个挑战&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;挑战-8&#34;&gt;&#xA;  挑战 [8]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%8c%91%e6%88%98-8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;如何让-agent-选择合适的工具&#34;&gt;&#xA;  如何让 agent 选择合适的工具&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a6%82%e4%bd%95%e8%ae%a9-agent-%e9%80%89%e6%8b%a9%e5%90%88%e9%80%82%e7%9a%84%e5%b7%a5%e5%85%b7&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Toolformer - fine tune&lt;/li&gt;&#xA;&lt;li&gt;Gorilla - retrieval，fine tune&lt;br&gt;&#xA;【solution: SFT or RL】&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;不必要的工具使用&#34;&gt;&#xA;  不必要的工具使用&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%b8%8d%e5%bf%85%e8%a6%81%e7%9a%84%e5%b7%a5%e5%85%b7%e4%bd%bf%e7%94%a8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;“Human Input”也写成一种工具，让模型来主动发起对人类的提问&lt;br&gt;&#xA;&lt;a href=&#34;https://python.langchain.com/docs/integrations/tools/human_tools&#34;&gt;Human as a tool&lt;/a&gt;&lt;br&gt;&#xA;【solution: human-in-the-loop】&lt;/p&gt;&#xA;&lt;h3 id=&#34;agent-返回的格式不稳定&#34;&gt;&#xA;  Agent 返回的格式不稳定&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agent-%e8%bf%94%e5%9b%9e%e7%9a%84%e6%a0%bc%e5%bc%8f%e4%b8%8d%e7%a8%b3%e5%ae%9a&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;这里常见的做法是让 LLM &lt;strong&gt;按照 json 这类常见的 schema 来返回&lt;/strong&gt;，一般稳定性会高一些（相比“Action:”这种）。&lt;br&gt;&#xA;此外自动修复重试也很实用，可以利用 LangChain 里的 &lt;strong&gt;output parsers&lt;/strong&gt; 来帮助完成。&lt;br&gt;&#xA;【solution: json output】&lt;/p&gt;&#xA;&lt;h3 id=&#34;记住之前的操作避免重复&#34;&gt;&#xA;  记住之前的操作，避免重复&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%b0%e4%bd%8f%e4%b9%8b%e5%89%8d%e7%9a%84%e6%93%8d%e4%bd%9c%e9%81%bf%e5%85%8d%e9%87%8d%e5%a4%8d&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;AutoGPT - retrieval 结合近期操作记录&lt;br&gt;&#xA;【solution: memory】&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Work)[SFT]Gorilla</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentToolGorilla/</link>
      <pubDate>Sat, 08 Apr 2023 07:58:38 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentToolGorilla/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;&lt;a href=&#34;https://ar5iv.labs.arxiv.org/html/2305.15334&#34;&gt;Gorilla: Large Language Model Connected with Massive APIs&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/ShishirPatil/gorilla&#34;&gt;gorilla&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;方法论1&#34;&gt;&#xA;  方法论[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%96%b9%e6%b3%95%e8%ae%ba1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;数据集收集&#34;&gt;&#xA;  数据集收集&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%95%b0%e6%8d%ae%e9%9b%86%e6%94%b6%e9%9b%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;API文档&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;HuggingFace平台托管和提供了约203,681个模型。然而，其中许多模型的文档质量较差，缺乏依赖项，模型卡中没有信息等问题。&lt;/li&gt;&#xA;&lt;li&gt;为了筛选出质量较好的模型，从每个领域选择了前20个模型。考虑了多模态数据领域的7个领域，CV领域的8个领域，NLP领域的12个领域，音频领域的5个领域，表格数据领域的2个领域和强化学习领域的2个领域。&lt;/li&gt;&#xA;&lt;li&gt;经过筛选，从HuggingFace获得了总共925个模型。从TensorFlow Hub获得了801个模型，并从Torch Hub获得了95个模型。&lt;/li&gt;&#xA;&lt;li&gt;这些模型的信息被转换为&lt;strong&gt;JSON对象&lt;/strong&gt;，其中包含了领域（domain）、框架（framework）、功能（functionality）、API名称（api_name）、API调用（api_call）、API参数（api_arguments）、环境要求（environment_requirements）、示例代码（example_code）、性能（performance）和描述（description）等字段。&lt;/li&gt;&#xA;&lt;li&gt;选择这些字段是为了将其泛化到机器学习领域之外的其他领域，包括RESTful API调用。&#xA;&lt;strong&gt;总而言之&lt;/strong&gt;，通过筛选和整理，从HuggingFace、TensorFlow Hub和Torch Hub等平台获取了&lt;strong&gt;总共1,645个模型&lt;/strong&gt;的信息，并将其以&lt;strong&gt;JSON对象&lt;/strong&gt;的形式进行了记录和描述。这些信息包括模型的领域、框架、功能、API调用示例、性能等，以便在机器学习和其他领域中使用和参考。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;指令生成 （Instruction Generation ）&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在&lt;strong&gt;self-instruct&lt;/strong&gt;范例[42]的指导下，使用GPT-4生成了合成指令数据。&lt;/li&gt;&#xA;&lt;li&gt;提供了三个上下文示例和一个参考API文档，要求模型生成调用API的真实世界用例。&lt;/li&gt;&#xA;&lt;li&gt;明确指示模型在创建指令时不要使用任何API名称或提示。&lt;/li&gt;&#xA;&lt;li&gt;为每个三个模型中心构建了六个示例（&lt;strong&gt;指令-API对&lt;/strong&gt;），共计18个点，这些数据是手动生成或修改的。&lt;/li&gt;&#xA;&lt;li&gt;对于&lt;strong&gt;1,645个&lt;/strong&gt;API数据点中的每一个，从相应的六个指令示例中随机选择3个，生成&lt;strong&gt;总共10个指令-API对&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;强调只需要使用GPT-4生成指令，可以与开源替代方案（如LLaMA、Alpaca等）进行交换。&#xA;&lt;strong&gt;总而言之&lt;/strong&gt;，通过使用GPT-4生成指令，并结合上下文示例和参考API文档，在每个模型中心构建了六个示例，共计18个点。这些示例被用于&lt;strong&gt;生成1,645个API数据点中的每一个的指令-API对，生成总共10个对应关系&lt;/strong&gt;。与开源替代方案相比，GPT-4的指令生成功能被应用在这个过程中。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;gorilla&#34;&gt;&#xA;  Gorilla&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gorilla&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;带有约束的API调用（API Call with Constraints）&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;API调用通常具有固有的&lt;strong&gt;约束&lt;/strong&gt;，这些约束要求LLM不仅理解API调用的功能，还要&lt;strong&gt;根据不同的约束参数对调用进行分类&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;机器学习API调用中常见的约束集是参数大小和准确性的下限。这些约束要求LLM能够根据提示理解和回答问题，例如根据提示选择参数少于10M的图像分类模型，并且至少保持70%的ImageNet准确率。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;对LLM来说，理解和推理出请求中嵌入的各种约束是一个巨大的挑战&lt;/strong&gt;。LLM需要细致地理解用户的功能描述，并能够正确地处理伴随这些调用的复杂约束。&lt;/li&gt;&#xA;&lt;li&gt;这个挑战凸显了在实际API调用中对LLM的复杂要求。仅仅理解API调用的基本功能是不够的，&lt;strong&gt;模型还必须能够应对伴随这些调用的约束，如参数大小和准确性要求&lt;/strong&gt;。&#xA;总而言之，在机器学习API调用中，LLM面临着理解和处理约束的挑战。除了理解API调用的基本功能外，LLM还需要能够识别和满足伴随调用的约束要求，如参数大小和准确性的下限。这需要模型具备更细致的理解和推理能力，以满足实际API调用的复杂需求。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/640697382&#34;&gt;Gorilla：与大规模API相连的大型语言模型&lt;/a&gt; ***&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://apposcmf8kb5033.pc.xiaoe-tech.com/live_pc/l_64a7d5afe4b09d7237a04b5b&#34;&gt;Gorilla：链接海量API的大型语言模型&lt;/a&gt; V&#xA;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/632583909&#34;&gt;大猩猩（Gorilla）🦍，连接大量 API 的大型语言模型，能成为未来AI应用的核心么？&lt;/a&gt; ***&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://gorilla.cs.berkeley.edu/&#34;&gt;Gorilla: Large Language Model Connected with Massive APIs&lt;/a&gt;&#xA;1xx. &lt;a href=&#34;https://gorilla.cs.berkeley.edu/blog.html&#34;&gt;Gorilla blog&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Work)Agent-Tools</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentTool/</link>
      <pubDate>Fri, 27 Jan 2023 16:32:24 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Tool-use/AgentTool/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2304.08354.pdf&#34;&gt;Tool Learning with Foundation Models&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/thunlp/ToolLearningPapers&#34;&gt;ToolLearningPapers&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;分类1&#34;&gt;&#xA;  分类[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%86%e7%b1%bb1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;tool-augmented-vs-tool-oriented-kimi-总结&#34;&gt;&#xA;  Tool-augmented vs. Tool-oriented [kimi 总结]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tool-augmented-vs-tool-oriented-kimi-%e6%80%bb%e7%bb%93&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Tool-augmented Learning（工具增强学习）:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;这种学习方式指的是在基础模型（如大型预训练语言模型）的基础上，&lt;strong&gt;通过引入外部工具来增强模型的能力&lt;/strong&gt;。这些工具可以是任何可以被模型通过某种接口调用的系统或服务，例如搜索引擎、数据库、API等。&lt;/li&gt;&#xA;&lt;li&gt;工具增强学习的核心在于模型利用这些工具来获取额外的信息或执行特定的任务，从而弥补模型自身知识和能力的不足。&lt;/li&gt;&#xA;&lt;li&gt;例如，&lt;strong&gt;一个语言模型可能通过调用天气API来获取最新的天气信息，或者通过搜索引擎来找到相关问题的答案&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Tool-oriented Learning（面向工具的学习）:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;面向工具的学习则更多地关注于模型如何学习和理解如何使用这些工具。这不仅仅是调用工具API那么简单，而是&lt;strong&gt;涉及到模型对工具的深入理解和策略性使用&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;在这种学习模式下，模型可能需要&lt;strong&gt;学习如何组合使用多个工具&lt;/strong&gt;，或者在复杂任务中动态调整对工具的使用策略，以实现更高效的问题解决。&lt;/li&gt;&#xA;&lt;li&gt;例如，模型可能需要学习如何在&lt;strong&gt;规划一次旅行&lt;/strong&gt;时，先后调用地图API、航班搜索API和酒店预订API，同时还要根据用户反馈和环境变化动态调整计划。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;总的来说，Tool-augmented Learning 强调的是通过外部工具来扩展模型的能力，而 Tool-oriented Learning 则更侧重于模型对工具使用的学习和优化。两者都是工具学习（Tool Learning）的重要组成部分，但在实际应用中可能会有不同的实现方式和关注点。&lt;/p&gt;&#xA;&lt;h3 id=&#34;tool-augmented-learning&#34;&gt;&#xA;  Tool-augmented Learning&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tool-augmented-learning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Toolformer&lt;br&gt;&#xA;{% post_link &amp;lsquo;gptAgentToolformer&amp;rsquo; %}&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;tool-oriented-learning&#34;&gt;&#xA;  Tool-oriented Learning&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tool-oriented-learning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ToolMaker[10]&lt;/li&gt;&#xA;&lt;li&gt;CREATOR[11]&lt;/li&gt;&#xA;&lt;li&gt;ToolLLM [12]&lt;/li&gt;&#xA;&lt;li&gt;Visual ChatGPT[13]&lt;/li&gt;&#xA;&lt;li&gt;HuggingGPT[13]&lt;/li&gt;&#xA;&lt;li&gt;Gorilla&#xA;{% post_link &amp;lsquo;gptAgentToolGorilla&amp;rsquo; %}&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/624459759&#34;&gt;大模型工具学习权威综述，BMTools 背后的论文！&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://blog.csdn.net/xixiaoyaoww/article/details/130278978&#34;&gt;清华发布工具学习框架，让ChatGPT操控地图、股票查询，贾维斯已来？&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reflection Agent *</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Reflection/AgentReflection/</link>
      <pubDate>Fri, 07 Apr 2023 02:26:20 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/%E7%BB%84%E4%BB%B6/Reflection/AgentReflection/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;react&#34;&gt;&#xA;  ReAct&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#react&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&lt;br&gt;&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2210.03629&#34;&gt;ReAct: Synergizing Reasoning and Acting in Language Models&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/ysymyth/ReAct&#34;&gt;ReAct&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Project page&lt;br&gt;&#xA;&lt;a href=&#34;https://react-lm.github.io/&#34;&gt;ReAct: Synergizing Reasoning and Acting in Language Models&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;实现1&#34;&gt;&#xA;  实现[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e7%8e%b01&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/hotpotqa.png&#34; alt=&#34;hotpotqa&#34; /&gt;&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; json&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;folder &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;./prompts/&amp;#39;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;prompt_file &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;prompts_naive.json&amp;#39;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(folder &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; prompt_file, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; f:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    prompt_dict &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; json&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(f)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;webthink_examples &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; prompt_dict[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;webthink_simple6&amp;#39;&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;instruction &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types: &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;(2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;(3) Finish[answer], which returns the answer and finishes the task.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Here are some examples.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;webthink_prompt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; instruction &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; webthink_examples&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;reflexion&#34;&gt;&#xA;  Reflexion&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#reflexion&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;论文-1&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2303.11366&#34;&gt;Reflexion: Language Agents with Verbal Reinforcement Learning&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(List)Agent 产品 平台</title>
      <link>https://www6v.github.io/www6vAIGC/docs/Agent/Platform/AgentList/</link>
      <pubDate>Sun, 05 Mar 2023 16:48:28 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/Agent/Platform/AgentList/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;应用&#34;&gt;&#xA;  应用&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ba%94%e7%94%a8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;分类-101112&#34;&gt;&#xA;  分类 [10][11][12]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%86%e7%b1%bb-101112&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Action agents&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Function Call&lt;/li&gt;&#xA;&lt;li&gt;ReACT&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Simulation agents&#xA;生成式智能体， CAMEL，  Generative Agents&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Automomous Agent&#xA;&lt;strong&gt;AutoGPT&lt;/strong&gt;， &lt;strong&gt;BabyAGI&lt;/strong&gt;,  &lt;strong&gt;AutoGen&lt;/strong&gt;&#xA;&lt;strong&gt;MetaGPT&lt;/strong&gt;     ChatDev&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;跨模态Agents&#xA;HuggingGPT&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;hugginggpt&#34;&gt;&#xA;  HuggingGPT&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#hugginggpt&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h3 id=&#34;babyagi--aigc&#34;&gt;&#xA;  BabyAGI  [AIGC]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#babyagi--aigc&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Plan-and-execute agents&#xA;The &lt;strong&gt;planning&lt;/strong&gt; is almost always done &lt;strong&gt;by an LLM&lt;/strong&gt;.&#xA;The &lt;strong&gt;execution&lt;/strong&gt; is usually done by a &lt;strong&gt;separate agent (equipped with tools)&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;autogpt10&#34;&gt;&#xA;  AutoGPT[10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#autogpt10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;AutoGPT 的核心逻辑是一个 Prompt Loop，步骤如下&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;AutoGPT 会基于一定策略自动组装 Command Prompt，这些首次会包含用户输入的 Name, Role和Goals&lt;/li&gt;&#xA;&lt;li&gt;Command Prompt 的目标不是为了拿到最终结果，而是通过 GPT Chat API(Thinking 的过程)返回下一步的 Command (包含name和arguments, 如&lt;code&gt;browser_website(url = &amp;quot;www.baidu.com&amp;quot;)&lt;/code&gt; )&lt;/li&gt;&#xA;&lt;li&gt;这些 Command 都是可扩展的，每一种命令代表一种外部能力(比如爬虫、Google搜索，也包括GPT的能力)，通过这些 Command 调用返回的 Result 又会成为到 Command Prompt 的组成元素，&lt;/li&gt;&#xA;&lt;li&gt;回到第 1 步往复循环，直到拿到最终结果结果（状态为“compelete”）&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;platform20&#34;&gt;&#xA;  Platform[20]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#platform20&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;字节-coze2122&#34;&gt;&#xA;  字节 Coze[21,22]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ad%97%e8%8a%82-coze2122&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;优势:  有RAG，结构化数据&lt;br&gt;&#xA;劣势:  只能发布到飞书，微信&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
