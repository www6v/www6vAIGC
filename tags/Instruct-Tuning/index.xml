<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Instruct-Tuning on 基于LLM的系统设计与实现</title>
    <link>https://www6v.github.io/www6vAIGC/tags/Instruct-Tuning/</link>
    <description>Recent content in Instruct-Tuning on 基于LLM的系统设计与实现</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 12 Mar 2023 16:00:05 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vAIGC/tags/Instruct-Tuning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(原理)Instruct Tuning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/Instruct-Tuning/InstructTuning/</link>
      <pubDate>Fri, 06 Jan 2023 19:09:20 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/Instruct-Tuning/InstructTuning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;in-context-learning--icl--上下文学习&#34;&gt;&#xA;  In Context Learning ( ICL ) 上下文学习&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#in-context-learning--icl--%e4%b8%8a%e4%b8%8b%e6%96%87%e5%ad%a6%e4%b9%a0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/ICL.webp&#34; alt=&#34;ICL&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;in context learning&lt;/strong&gt;，大意是在&lt;strong&gt;prompt learning的基础上，将少量有标签样本融入prompt&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;上图的ICL模型可以理解成&lt;strong&gt;有监督、无训练&lt;/strong&gt;的&lt;strong&gt;小样本学习&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;但&lt;strong&gt;并非所有ICL都不训练&lt;/strong&gt;。比如下图右上角的&lt;strong&gt;FLAN&lt;/strong&gt;就是用instruction tuning&lt;strong&gt;训练参数&lt;/strong&gt;的。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/ICL-tech.webp&#34; alt=&#34;ICL-tech&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;FLAN&lt;/strong&gt;，&lt;strong&gt;既属于 in context learning，也属于 instruction learning&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;instruction-learning-1&#34;&gt;&#xA;  Instruction Learning [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#instruction-learning-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;instruct-tuning-&#34;&gt;&#xA;  Instruct Tuning-&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#instruct-tuning-&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;pre&gt;&lt;code&gt;FLANv1, FLANv2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;h3 id=&#34;instructgpt&#34;&gt;&#xA;  instructGPT&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#instructgpt&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h3 id=&#34;chatgpt&#34;&gt;&#xA;  chatGPT&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#chatgpt&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;instruction-tuning&#34;&gt;&#xA;  Instruction Tuning&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#instruction-tuning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/instructTuning.webp&#34; alt=&#34;instructTuning&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;对于已有的预训练模型，继续在多项任务（B、C、D等）上做训练，在其他任务（A）上做预测。&lt;strong&gt;虽然依然没见过任务A，但是根据对B、C、D等的训练，对A的效果有所提升；&lt;/strong&gt; [1]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Instruct Tuning 本质上也是Prompt Tuning&lt;/strong&gt; [2]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;研究了缩放对指令微调的影响 [3]&#xA;与微调指令的任务数量有关，&lt;strong&gt;任务数量越多效果越好&lt;/strong&gt;&#xA;与模型的大小有关，&lt;strong&gt;模型越大效果越好&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Prompt vs. Instruction Tuning  [4]&#xA;Prompt是去激发语言模型的&lt;strong&gt;补全能力&lt;/strong&gt;，比如给出上半句生成下半句、或者做完形填空，都还是像在做language model任务.&#xA;而Instruction Tuning则是激发语言模型的&lt;strong&gt;理解能力&lt;/strong&gt;，通过给出更明显的指令/指示，让模型去理解并做出正确的action&#xA;&lt;strong&gt;Prompt tuning&lt;/strong&gt;都是针对&lt;strong&gt;一个任务&lt;/strong&gt;的，比如做个情感分析任务的prompt tuning，精调完的模型只能用于情感分析任务，而经过&lt;strong&gt;Instruction Tuning多任务&lt;/strong&gt;精调后，可以用于其他任务的zero-shot&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Survey)Instruct Tuning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/Instruct-Tuning/InstructTuningSurvey/</link>
      <pubDate>Sun, 12 Mar 2023 16:00:05 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/Instruct-Tuning/InstructTuningSurvey/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://arxiv.org/abs/2308.10792&#34;&gt;大语言模型指令微调综述&lt;/a&gt;&#xA;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/654054370&#34;&gt;一篇关于LLM指令微调的综述&lt;/a&gt;&lt;br&gt;&#xA;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/657138921&#34;&gt;[论文]大语言模型指令调优综述&lt;/a&gt;&lt;br&gt;&#xA;1xx. &lt;a href=&#34;https://blog.csdn.net/qq_41185868/article/details/132613338&#34;&gt;Paper：《Instruction Tuning for Large Language Models: A Survey—大型语言模型的指令调优的综述》翻译与解读&lt;/a&gt;&lt;br&gt;&#xA;1xx. &lt;a href=&#34;https://github.com/xiaoya-li/Instruction-Tuning-Survey&#34;&gt;Instruction Tuning for Large Language Models: A Survey&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;p&gt;【前面大部分是Instruct-Tuning， 中间一部分是Multi-modality Instruction Tuning】&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
