<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PEFT on 基于LLM的系统设计与实现</title>
    <link>https://www6v.github.io/www6vAIGC/tags/PEFT/</link>
    <description>Recent content in PEFT on 基于LLM的系统设计与实现</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 28 Jan 2024 19:04:02 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vAIGC/tags/PEFT/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(最佳实践)SFT  &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/%E5%AE%9E%E8%B7%B5/FineTuningBestPractice/</link>
      <pubDate>Sat, 23 Dec 2023 11:11:20 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/%E5%AE%9E%E8%B7%B5/FineTuningBestPractice/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;sft-bestpractice&#34;&gt;&#xA;  SFT BestPractice&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#sft-bestpractice&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/SFT-158bfe2110848004a0d7dc9be4e2e0ac?pvs=4&#34;&gt;SFT BestPractice&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)PEFT &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/FineTuning/</link>
      <pubDate>Fri, 18 Nov 2022 23:31:46 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/FineTuning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;peft原理&#34;&gt;&#xA;  PEFT原理&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#peft%e5%8e%9f%e7%90%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/PEFT-10dbfe2110848028b9afd05f05fdbde6?pvs=4&#34;&gt;(原理)PEFT&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>P-Tuning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Soft-Prompt/PEFTPtuning/</link>
      <pubDate>Fri, 24 Mar 2023 22:17:49 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Soft-Prompt/PEFTPtuning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;p-tuning2&#34;&gt;&#xA;  P-Tuning[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#p-tuning2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;P-Tuning 的创新之处在于将提示（Prompt）转化为&lt;strong&gt;可学习的嵌入层（Embedding Layer）&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;架构&#34;&gt;&#xA;  架构&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9e%b6%e6%9e%84&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/ptuning.png&#34; alt=&#34;ptuning&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;一个关于“The capital of Britain is [MASK]” 示例：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;蓝色是上下文 “Britain”&lt;/li&gt;&#xA;&lt;li&gt;红色是目标单词 “[MASK]”，&lt;/li&gt;&#xA;&lt;li&gt;橙色区域是提示词。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;传统方式 与 P-Tuning 对比：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在（a）中，提示生成器只接收离散奖励；&lt;/li&gt;&#xA;&lt;li&gt;在（b）中，连续的&lt;strong&gt;提示嵌入（Prompt Embedding）&lt;/strong&gt; 和**提示编码器（Prompt Encoder）**以可微的方式进行 优化。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;p-tuning-v22&#34;&gt;&#xA;  P-Tuning v2[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#p-tuning-v22&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;背景&#34;&gt;&#xA;  背景&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%83%8c%e6%99%af&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;之前的方法在以下两方面有所&lt;strong&gt;限制&lt;/strong&gt;：&#xA;• 模型规模差异：在大型预训练模型中，Prompt Tuning 和&#xA;P-Tuning 能取得与全面微调相似的效果，但在参数较少&#xA;的模型上则表现不佳。&#xA;• 任务类型差异：无论是 Prompt Tuning 还是 P-Tuning，&#xA;在序列标注任务上的表现都较差。&lt;/p&gt;&#xA;&lt;h3 id=&#34;目的&#34;&gt;&#xA;  目的&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e7%9a%84&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;P-Tuning v2 旨在使提示调整（Prompt Tuning）在不同规模的预训练模型上，针对各种下游任务都能达到类似全面微调（Fine-tuning）的效果。&lt;/p&gt;&#xA;&lt;h3 id=&#34;架构-1&#34;&gt;&#xA;  架构 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9e%b6%e6%9e%84-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/ptuning-v2.png&#34; alt=&#34;ptuning-v2&#34; /&gt;&#xA;在每一层都加入了Prompts tokens 作为输入,  而不是仅仅加在输入层&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战)PEFT 概述</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/FineTuningPEFT/</link>
      <pubDate>Tue, 20 Dec 2022 11:25:22 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/FineTuningPEFT/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;huggingface--peft中的任务1&#34;&gt;&#xA;  Huggingface  PEFT中的任务[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#huggingface--peft%e4%b8%ad%e7%9a%84%e4%bb%bb%e5%8a%a11&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;class TaskType(str, enum.Enum):&#xD;&#xA;    SEQ_CLS = &amp;#34;SEQ_CLS&amp;#34;  # 3. 序列分类任务&#xD;&#xA;    SEQ_2_SEQ_LM = &amp;#34;SEQ_2_SEQ_LM&amp;#34;  # 2. 条件生成任务&#xD;&#xA;    CAUSAL_LM = &amp;#34;CAUSAL_LM&amp;#34;  #  1. 因果语言建模任务&#xD;&#xA;    TOKEN_CLS = &amp;#34;TOKEN_CLS&amp;#34;  #  4. Token 分类任务&#xD;&#xA;    QUESTION_ANS = &amp;#34;QUESTION_ANS&amp;#34;&#xD;&#xA;    FEATURE_EXTRACTION = &amp;#34;FEATURE_EXTRACTION&amp;#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;1-因果语言建模任务causal-language-modeling&#34;&gt;&#xA;  1. 因果语言建模任务（Causal Language Modeling）&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#1-%e5%9b%a0%e6%9e%9c%e8%af%ad%e8%a8%80%e5%bb%ba%e6%a8%a1%e4%bb%bb%e5%8a%a1causal-language-modeling&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;因果语言建模任务（CLM），在这种建模方法中，模型试图预测给定上下文中的下一个单词，该上下文通常包括在当前单词之前的所有单词。&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-条件生成任务conditional-generation&#34;&gt;&#xA;  2. 条件生成任务（Conditional Generation）&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#2-%e6%9d%a1%e4%bb%b6%e7%94%9f%e6%88%90%e4%bb%bb%e5%8a%a1conditional-generation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;条件生成任务（Conditional Generation），根据给定的输入（可能是文本、图片等）生成符合条件的输出。&#xA;条件生成的应用包括但不限于机器翻译、文本摘要、图像描述等。这些任务通常需要模型在输入和输出之间建立复杂的映射关系。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;因果语言建模任务  vs.  条件生成任务&#xA;因果语言建模主要关注于生成连贯、自然的文本，而条件生成关注于生成满足特定条件或任务要求的文本。这两种建模方法在某些场景下可能会互相使用和结合，以实现更复杂的自然语言处理任务。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;h3 id=&#34;3-序列分类任务sequence-classification&#34;&gt;&#xA;  3. 序列分类任务（Sequence Classification）&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#3-%e5%ba%8f%e5%88%97%e5%88%86%e7%b1%bb%e4%bb%bb%e5%8a%a1sequence-classification&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;序列分类（Sequence Classification），对整个句子进行分类。如: 获取评论的情绪，检测电子邮件是否为垃圾邮件，确定句子在语法上是否正确或两个句子在逻辑上是否相关等&lt;/p&gt;&#xA;&lt;h3 id=&#34;4-token-分类任务token-classification&#34;&gt;&#xA;  4. Token 分类任务（Token Classification）&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#4-token-%e5%88%86%e7%b1%bb%e4%bb%bb%e5%8a%a1token-classification&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Token 分类任务（Token Classification），对句子中的每个词进行分类。如: 识别句子的语法成分（名词、动词、形容词）或命名实体（人、地点、组织）。&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战)PEFT P-Tuning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Soft-Prompt/PEFTPtuningPractice/</link>
      <pubDate>Sun, 28 Jan 2024 19:04:02 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Soft-Prompt/PEFTPtuningPractice/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h3 id=&#34;最佳实践1&#34;&gt;&#xA;  最佳实践[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b51&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;要看losss, 也要看&lt;strong&gt;业务的loss&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;生成模型常用的评价方法&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;BLEU 能评估&lt;/strong&gt;流畅度**&lt;/li&gt;&#xA;&lt;li&gt;结果都是流畅的前提下，ROUGE 反应参照句中多少内容被生成的句子包含（召回）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;垂直模型&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;stf之后失去通用能力&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;要有&lt;strong&gt;通用能力&lt;/strong&gt;, 需要&lt;strong&gt;pre-train和STF中都融入通用的语料&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;每个模型的学习率lr不一样&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;chatglm的学习率&#xA;LR=2e-2&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;学习率&#34;&gt;&#xA;  学习率&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ad%a6%e4%b9%a0%e7%8e%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;改的&lt;strong&gt;特别大&lt;/strong&gt;&#xA;模型训练的时候会&lt;strong&gt;震荡&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;改的&lt;strong&gt;特别小&lt;/strong&gt;&#xA;模型训练的时候会&lt;strong&gt;收敛非常慢&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;《13-基于 ChatGLM2的 Fine-tuning 实战》 AI 大模型全栈工程师培养计划  2期&#xA;&lt;a href=&#34;https://github.com/www6v/fine-tuning-lab/blob/agiclass-v1/chatglm/train_pt2.sh&#34;&gt;train_pt2.sh&lt;/a&gt; git   基于法律文本的chatglm的p-tuning&#xA;&lt;a href=&#34;https://github.com/www6v/fine-tuning-lab/blob/agiclass-v1/chatglm2/train_pt2.sh&#34;&gt;train_pt2.sh&lt;/a&gt; git   基于法律文本的chatglm-2的P-tuning v2&#xA;&lt;a href=&#34;https://github.com/www6v/fullStackLLM/blob/master/08-fine-tuning/peft/index.ipynb&#34;&gt;课件&lt;/a&gt;&#xA;bili有相关的总结的视频&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(原理|实战) QLoRA *</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Lora/PEFTQLora/</link>
      <pubDate>Fri, 12 Jan 2024 10:36:44 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Lora/PEFTQLora/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;aside&gt;&#xA;💡&#xA;&lt;ul&gt;&#xA;&lt;li&gt;QAT&#xA;&lt;ul&gt;&#xA;&lt;li&gt;4 bit NormalFloat(NF4) 量化&lt;/li&gt;&#xA;&lt;li&gt;双量化&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/aside&gt;&#xA;&lt;h1 id=&#34;技术原理-1&#34;&gt;&#xA;  技术原理 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%8a%80%e6%9c%af%e5%8e%9f%e7%90%86-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;使用一种新颖的高精度技术将预训练模型量化为 4 bit，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。&lt;br&gt;&#xA;QLoRA提出了两种技术实现高保真 4 bit微调——4 bit NormalFloat(NF4) 量化和双量化。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;4bit NormalFloat（NF4）：对于正态分布权重而言，一种信息理论上最优的新数据类型，该数据类型对正态分布数据产生比 4 bit整数和 4bit 浮点数更好的实证结果。&lt;/li&gt;&#xA;&lt;li&gt;双量化：对第一次量化后的那些常量再进行一次量化，减少存储空间。&lt;/li&gt;&#xA;&lt;li&gt;分页优化器: 使用此功能为优化器状态（Optimizer）分配分页内存，然后在 GPU 内存不足时将其自动卸载到 CPU 内存，并在优化器更新步骤需要时将其加载回 GPU 内存。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/qlora.png&#34; alt=&#34;qlora.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;实验证明，无论是使用16bit、8bit还是4bit的适配器方法，都能够复制16bit全参数微调的基准性能。这说明，尽管量化过程中会存在性能损失，但通过适配器微调，完全可以恢复这些性能。&lt;/p&gt;&#xA;&lt;h1 id=&#34;总结&#34;&gt;&#xA;  总结&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%80%bb%e7%bb%93&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;QLoRA [189] quantizes the weights of LLMs into 4-bit and subsequently employs LoRA [224] in BF16 for each 4-bit weight matrix to fine-tune the quantized model. QLoRA allows for the efficient fine-tuning of a 65B parameter LLM on one GPU with only 30GB of memory.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战) Lora &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Lora/PEFTLora/</link>
      <pubDate>Thu, 05 Jan 2023 12:04:14 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Lora/PEFTLora/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;lora--实战&#34;&gt;&#xA;  Lora  实战&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#lora--%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/PEFT-Lora-10dbfe21108480489a27f07aba286e4f?pvs=4&#34;&gt;(实战) Lora &lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
