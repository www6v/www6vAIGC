<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Prompt-Tuning on 基于LLM的系统设计与实现</title>
    <link>https://www6v.github.io/www6vAIGC/tags/Prompt-Tuning/</link>
    <description>Recent content in Prompt-Tuning on 基于LLM的系统设计与实现</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 25 Jan 2023 19:11:28 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vAIGC/tags/Prompt-Tuning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(原理)Prompt Tuning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Soft-Prompt/PromptTuning/</link>
      <pubDate>Fri, 06 Jan 2023 19:06:15 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Soft-Prompt/PromptTuning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;npl范式-1&#34;&gt;&#xA;  NPL范式 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#npl%e8%8c%83%e5%bc%8f-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/npl4Paragiam.jpg&#34; alt=&#34;npl4Paragiam&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;prompt-tuning-2&#34;&gt;&#xA;  Prompt Tuning [2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#prompt-tuning-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;🔔 Prompt Tuning&#xA;&lt;ul&gt;&#xA;&lt;li&gt;🔗 文章：The Power of Scale for Parameter-Efficient Prompt Tuning (EMNLP 2021) &lt;a href=&#34;https://aclanthology.org/2021.emnlp-main.243/&#34;&gt;https://aclanthology.org/2021.emnlp-main.243/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;🔑关键词和摘要&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Keywords: Large-scale PLMs, Parameter-efficient Tuning, Prompt Tuning&lt;/li&gt;&#xA;&lt;li&gt;摘要&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Prompt变成可学习的向量，固定PLM，微调Prompt来适配下游任务&lt;/li&gt;&#xA;&lt;li&gt;PLM参数规模越大，Prompt Tuning的性能和全参数微调越接近&lt;/li&gt;&#xA;&lt;li&gt;这种基于&lt;strong&gt;Soft Prompt&lt;/strong&gt;的Prompt Tuning方法可以看作是&lt;strong&gt;Prefix Tuning的简化版本&lt;/strong&gt;（只加在输入上）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;⚙️研究设计和结论&#xA;&lt;ul&gt;&#xA;&lt;li&gt;方法&#xA;&lt;ul&gt;&#xA;&lt;li&gt;模型示意图：xxx&lt;/li&gt;&#xA;&lt;li&gt;模型基本思路：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;经典分类：P(Y | X; θ)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Hard Prompt: P(Y | [P;X] ; θ)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Soft Prompt: P(Y | [P;X] ; θ; Δ)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Pre-Training&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Fine-Tuning&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Prompt Tuning&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;实现细节：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;模型参数量&#xA;&lt;ul&gt;&#xA;&lt;li&gt;参数量：T5 ~ T5-XXL(10B)&lt;/li&gt;&#xA;&lt;li&gt;预训练：LM Adaptation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Prompt长度：xxx&#xA;&lt;ul&gt;&#xA;&lt;li&gt;1、5、20、100、150&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;初始化方法：xxx&#xA;&lt;ul&gt;&#xA;&lt;li&gt;随机初始化&lt;/li&gt;&#xA;&lt;li&gt;使用预设文本的词向量初始化，类似于设计hard prompt，然后将hard prompt转化为soft prompt&lt;/li&gt;&#xA;&lt;li&gt;使用类别词向量初始化，类似于提供选项&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;实验&#xA;&lt;ul&gt;&#xA;&lt;li&gt;数据集：SuperGLUE&lt;/li&gt;&#xA;&lt;li&gt;xxx&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Prompt的规模越大，性能相对而言会越好&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;xxx&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基于语义信息的初始化比随机初始化要好&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;xxx&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LM Adaptation 对性能提升显著&lt;/li&gt;&#xA;&lt;li&gt;Prompt Tuning还是需要大模型有较好的文本生成能力&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;xxx&#xA;&lt;ul&gt;&#xA;&lt;li&gt;模型参数规模越大，Prompt Tuning效果越好&lt;/li&gt;&#xA;&lt;li&gt;10B参数时与全参数微调性能接近&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;📚论文贡献&#xA;&lt;ul&gt;&#xA;&lt;li&gt;优点（计算友好）&#xA;&lt;ul&gt;&#xA;&lt;li&gt;大模型的&lt;strong&gt;微调新范式&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;一个中心模型服务多个下游任务&lt;/strong&gt;，&lt;strong&gt;节省参数存储量&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;无需优化模型参数&lt;/strong&gt;，节省优化器的计算量和存储量&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;只在输入层进行操作&lt;/strong&gt;，适合多任务场景下的计算合并&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;缺点（性能和收敛性存在问题）&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Prompt Tuning的&lt;strong&gt;收敛速度很慢&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Prompt Tuning的模型&lt;strong&gt;性能不稳定&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Few-shot场景上表现不佳&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;prompt-tuning3&#34;&gt;&#xA;  Prompt Tuning[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#prompt-tuning3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/promptTuning.JPG&#34; alt=&#34;promptTuning&#34; /&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战)PromptTuning</title>
      <link>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Soft-Prompt/PromptTuningPractice/</link>
      <pubDate>Wed, 25 Jan 2023 19:11:28 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/FineTuning/PEFT/Soft-Prompt/PromptTuningPractice/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/646748939&#34;&gt;大模型参数高效微调技术实战（二）-Prompt Tuning&lt;/a&gt;&#xA;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/635686756&#34;&gt;大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/www6v/llm-action/blob/main/train/peft/clm/peft_prompt_tuning_clm.ipynb&#34;&gt;peft_prompt_tuning_clm.ipynb&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
