<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RAG on 基于LLM的系统设计与实现</title>
    <link>https://www6v.github.io/www6vAIGC/tags/RAG/</link>
    <description>Recent content in RAG on 基于LLM的系统设计与实现</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Fri, 22 Dec 2023 17:34:50 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vAIGC/tags/RAG/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(原理) RAG Pattern *</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/RAGPattern/</link>
      <pubDate>Fri, 22 Dec 2023 17:34:50 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/RAGPattern/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;7-种-rag-模式&#34;&gt;&#xA;  7 种 RAG 模式&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#7-%e7%a7%8d-rag-%e6%a8%a1%e5%bc%8f&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Naive RAG 是最基础的架构，包含简单的文档检索、处理和生成响应的流程&lt;/li&gt;&#xA;&lt;li&gt;Retrieve-and-rerank 在基础 RAG 上增加了重排序步骤，可以优化检索结果的相关性&lt;/li&gt;&#xA;&lt;li&gt;Multimodal RAG 能够处理图像等多种类型的数据，不仅限于文本&lt;/li&gt;&#xA;&lt;li&gt;Graph RAG 利用图数据库增强知识连接，可以更好地理解文档间的关系&lt;/li&gt;&#xA;&lt;li&gt;Hybrid RAG 结合了多种技术的优势，包含图结构和传统检索方法&lt;/li&gt;&#xA;&lt;li&gt;Agentic RAG Router 使用 AI Agent 来路由和处理查询，可以选择最适合的处理路径&lt;/li&gt;&#xA;&lt;li&gt;Agentic RAG Multi-Agent 使用多个专门的 AI Agent 协同工作，可以调用不同的工具（如向量搜索、网页搜索、Slack、Gmail 等）&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/v16n31qs.bmp&#34; alt=&#34;v16n31qs.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;核心组件&#34;&gt;&#xA;  核心组件&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%a0%b8%e5%bf%83%e7%bb%84%e4%bb%b6&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;嵌入模型：将文本转换为向量表示&lt;/li&gt;&#xA;&lt;li&gt;生成模型：负责最终的内容生成&lt;/li&gt;&#xA;&lt;li&gt;重排序模型：优化检索结果的相关性&lt;/li&gt;&#xA;&lt;li&gt;向量数据库：存储和检索向量化的内容&lt;/li&gt;&#xA;&lt;li&gt;提示模板：规范化的查询处理模板&lt;/li&gt;&#xA;&lt;li&gt;AI Agent：智能决策和任务协调&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/v8Dd4mQmaemxpOyrrI7o6g&#34;&gt;RAG 架构图解：从基础到高级的7种模式&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/shao__meng/status/1866626166079230355?s=46&amp;amp;t=1AUvwyftFbcog4yHzgnysw&#34;&gt;RAG 架构图解：从基础到高级的7种模式&lt;/a&gt;   x&lt;/p&gt;</description>
    </item>
    <item>
      <title>GraphRAG &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/KG-RAG/graphRAG/</link>
      <pubDate>Fri, 22 Dec 2023 17:34:50 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/KG-RAG/graphRAG/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;graphrag&#34;&gt;&#xA;  GraphRAG&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#graphrag&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/GrpahRAG-210bfe211084800d978cdcebbfc1f337?source=copy_link&#34;&gt;GraphRAG&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)Agentic RAG &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/Agentic-RAG/AgenticRAG/</link>
      <pubDate>Sun, 25 Jun 2023 10:39:17 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/Agentic-RAG/AgenticRAG/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;agentic-rag&#34;&gt;&#xA;  Agentic RAG&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#agentic-rag&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Agentic-RAG-1d5bfe211084804b95a6e661868813af?pvs=4&#34;&gt;(原理|实战)Agentic RAG&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理) Index &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGIndex/RAGIndex/</link>
      <pubDate>Sun, 21 May 2023 17:09:42 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGIndex/RAGIndex/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;index&#34;&gt;&#xA;  Index&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#index&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Index-1f0bfe21108480d19597f733f0f3f518?pvs=4&#34;&gt; (原理) Index&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战) Chunk &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGIndex/RAGChunk/</link>
      <pubDate>Sun, 21 May 2023 17:09:42 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGIndex/RAGChunk/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;chunk&#34;&gt;&#xA;  Chunk&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#chunk&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Index-Chunk-109bfe21108480558752d5f4e9a72dd6?pvs=4&#34;&gt; (原理|实战) Chunk&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG Framework</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/framework/RAGFramework/</link>
      <pubDate>Tue, 09 May 2023 16:34:11 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/framework/RAGFramework/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;框架-0&#34;&gt;&#xA;  框架 [0]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%a1%86%e6%9e%b6-0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/infiniflow/ragflow/tree/main&#34;&gt;&lt;strong&gt;ragflow&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/tree/master&#34;&gt;&lt;strong&gt;QAnything&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/chatchat-space/Langchain-Chatchat/releases/tag/v0.2.8&#34;&gt;&lt;strong&gt;langchainchat&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/labring/FastGPT&#34;&gt;&lt;strong&gt;FastGPT&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain/&#34;&gt;&lt;strong&gt;LangChain&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/run-llama/llama_index/&#34;&gt;&lt;strong&gt;LlamaIndex&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/langchain4j/langchain4j&#34;&gt;langchain4j&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Azure/GPT-RAG&#34;&gt;GPT-RAG&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Unstructured-IO/unstructured&#34;&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/StanGirard/quivr&#34;&gt;Quivr&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/langgenius/dify&#34;&gt;&lt;strong&gt;Dify&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/weaviate/Verba&#34;&gt;Verba&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/danswer-ai/danswer&#34;&gt;danswer&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol start=&#34;0&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648407281&amp;amp;idx=2&amp;amp;sn=f39b46cad1787123b485d76dff33bc93&#34;&gt;大模型RAG问答研发真实图鉴：一周出Demo，半年用不好，缝补之路漫漫 &lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s/ZoI4Dscm9f9m5-q4Dq4bag&#34;&gt;大模型RAG问答开源框架的两个风向:兼看大模型安全的学术评测&lt;/a&gt;&#xA;RAGFlow - 引入文档理解及溯源机制&#xA;QAnything - 优化embeddding+召回侧方向的&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://llamahub.ai/&#34;&gt;LlamaHub&lt;/a&gt;&lt;br&gt;&#xA;Mix and match our Data Loaders and Agent Tools to build custom RAG apps or use our LlamaPacks as a starting point for your retrieval use cases.&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s/XRfSqYwvuGB6sDJzRm0QVA&#34;&gt;FlashRAG：可能是最全的、最快搭建RAG的开源框架 &lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)Query Rewrite</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPreRetrieval/QueryRewrite/</link>
      <pubDate>Thu, 20 Apr 2023 22:51:05 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPreRetrieval/QueryRewrite/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;query-rewrite&#34;&gt;&#xA;  Query rewrite&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#query-rewrite&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;query-rewrite-12&#34;&gt;&#xA;  query rewrite [1][2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#query-rewrite-12&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.14283.pdf&#34;&gt;论文&lt;/a&gt;&lt;strong&gt;使用LLM重写用户查询&lt;/strong&gt;，而不是直接使用原始用户查询进行检索。&#xA;因为对于LLM 而言，&lt;strong&gt;原始查询不可能总是最佳检索结果&lt;/strong&gt;，可以让LLM重写查询。&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain/blob/master/cookbook/rewrite.ipynb&#34;&gt;Repo&lt;/a&gt; git&#xA;【问题的多样化】&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;transformation-多样性&#34;&gt;&#xA;  Transformation-多样性&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#transformation-%e5%a4%9a%e6%a0%b7%e6%80%a7&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;step-back&#34;&gt;&#xA;  Step Back&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#step-back&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;step-back问答回退策略-3&#34;&gt;&#xA;  Step Back问答回退策略 [3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#step-back%e9%97%ae%e7%ad%94%e5%9b%9e%e9%80%80%e7%ad%96%e7%95%a5-3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Step Back问答回退，首先提示LLM提出一个&lt;strong&gt;关于高级概念或原则的通用后退问题&lt;/strong&gt;，并检索有关它们的相关事实，使用此基础来帮助回答用户问题。&lt;/p&gt;&#xA;&lt;h3 id=&#34;step-back-prompting-12&#34;&gt;&#xA;  Step-back Prompting [1][2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#step-back-prompting-12&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.06117.pdf&#34;&gt;论文&lt;/a&gt;使用退一步提示，&lt;strong&gt;使用LLM生成&amp;quot;后退&amp;quot;(Step back prompting)问题&lt;/strong&gt;。&#xA;使用检索时，&amp;ldquo;后退&amp;quot;问题和原始问题都会被用来进行检索，然后这两个结果都会被用来作为语言模型回复的基础。&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain/blob/master/cookbook/stepback-qa.ipynb&#34;&gt;Repo&lt;/a&gt; git&#xA;【问题的抽象化】&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;transformation-抽象化&#34;&gt;&#xA;  Transformation-抽象化&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#transformation-%e6%8a%bd%e8%b1%a1%e5%8c%96&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;hyde&#34;&gt;&#xA;  HyDE&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#hyde&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;hyde混合策略3&#34;&gt;&#xA;  HyDE混合策略[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#hyde%e6%b7%b7%e5%90%88%e7%ad%96%e7%95%a53&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;LLM将&lt;strong&gt;问题&lt;/strong&gt;转换为回答问题的&lt;strong&gt;假设文档&lt;/strong&gt;。&lt;strong&gt;使用嵌入的假设文档检索真实文档&lt;/strong&gt;，前提是doc-doc相似性搜索可以产生更多相关匹配。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;HyDE&#xA;At a high level, HyDE is an embedding technique that takes queries, &lt;strong&gt;generates a hypothetical answer&lt;/strong&gt;, and then embeds that generated document and uses that as the final example.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;transformation-具体化&#34;&gt;&#xA;  Transformation-具体化&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#transformation-%e5%85%b7%e4%bd%93%e5%8c%96&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648406156&amp;amp;idx=1&amp;amp;sn=d91a4df105c4fc4c9523f7141bc1c24d&#34;&gt;知识图谱用于细粒度大模型幻觉评估：兼论Langchain-RAG问答中的问题改写范式 &lt;/a&gt;&#xA;RAG:  rewrite , Step back, fusion&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)Query Transformation</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPreRetrieval/QueryTransformation/</link>
      <pubDate>Thu, 20 Apr 2023 22:51:05 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPreRetrieval/QueryTransformation/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;query-transformation&#34;&gt;&#xA;  Query Transformation&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#query-transformation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;multi-query多查询策略3&#34;&gt;&#xA;  Multi Query多查询策略[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#multi-query%e5%a4%9a%e6%9f%a5%e8%af%a2%e7%ad%96%e7%95%a53&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;该方法&lt;strong&gt;从多个角度重写用户问题&lt;/strong&gt;，为每个重写的问题检索文档，返回所有查询的唯一文档。&lt;/p&gt;&#xA;&lt;h3 id=&#34;decomposition问题分解策略3&#34;&gt;&#xA;  Decomposition问题分解策略[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#decomposition%e9%97%ae%e9%a2%98%e5%88%86%e8%a7%a3%e7%ad%96%e7%95%a53&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Answer recursively迭代式回答&#xA;在问题分解的基础上，逐步迭代出答案，&lt;strong&gt;将上一步问题的答案，与下一步骤的答案进行拼接&lt;/strong&gt;，送入大模型进行问答&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Answer individually&#xA;也可以&lt;strong&gt;让每个subquery分别进行处理&lt;/strong&gt;，然后得到答案，然后再拼接成一个QA pairspprompt最终形成答案。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;xxx&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;xxx&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/pK2BRLrWpEKKIPFhUtGvcg&#34;&gt;一文详看Langchain框架中的RAG多阶段优化策略：从问题转换到查询路由再到生成优化&lt;/a&gt; ***   原理paper，代码示例&lt;br&gt;&#xA;[Multi Query多查询策略， Decomposition问题]， RAG-Fusion， Step Back， HyDE混合&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/langchain-ai/rag-from-scratch&#34;&gt;rag-from-scratch Repo&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(Survey)多模态 RAG &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/Multimodal-RAG/RAGMultimodal/</link>
      <pubDate>Tue, 14 Mar 2023 13:55:59 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/Multimodal-RAG/RAGMultimodal/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;多模态-rag&#34;&gt;&#xA;  多模态 RAG&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%9a%e6%a8%a1%e6%80%81-rag&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/A-Survey-on-Multimodal-Retrieval-Augmented-Generation-1ebbfe211084808ab147d95e07c9cfd4?pvs=4&#34;&gt;(Survey)多模态 RAG&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)RAG OpenAI案例</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E6%A1%88%E4%BE%8B/RAGOpenAI/</link>
      <pubDate>Tue, 27 Dec 2022 11:11:05 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E6%A1%88%E4%BE%8B/RAGOpenAI/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;openai-rag-案例3&#34;&gt;&#xA;  OpenAI RAG 案例[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#openai-rag-%e6%a1%88%e4%be%8b3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/openai-rag.jpg&#34; alt=&#34;openai-rag&#34; /&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;retrieval with consine similarity&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;HyDE retrieval&lt;/strong&gt; [5]&#xA;Fine-tune Embeddings&#xA;&lt;strong&gt;Chunk/embedding experiments&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Reranking&lt;/strong&gt; [6][8]&#xA;Classification step&lt;/li&gt;&#xA;&lt;li&gt;Prompt engineering&#xA;&lt;strong&gt;Tool use&lt;/strong&gt;&#xA;&lt;strong&gt;Query expansion&lt;/strong&gt;[5]&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;query-transformations5&#34;&gt;&#xA;  Query Transformations[5]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#query-transformations5&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Query expansion&lt;/strong&gt;&#xA;Multi-query retriever&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;HyDE&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Step back prompting&#xA;[抽象prompting]&lt;/li&gt;&#xA;&lt;li&gt;Rewrite-Retrieve-Read&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;query-construction-4&#34;&gt;&#xA;  Query Construction [4]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#query-construction-4&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/structured_data_stacks.jpg&#34; alt=&#34;structured_data_stacks&#34; /&gt;&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Data source&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Text-to-metadata-filter&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Vectorstores&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/?ref=blog.langchain.dev#constructing-from-scratch-with-lcel&#34;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Text-to-SQL&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;SQL DB&lt;/td&gt;&#xA;          &lt;td&gt;&lt;a href=&#34;https://python.langchain.com/docs/use_cases/qa_structured/sql?ref=blog.langchain.dev&#34;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href=&#34;https://blog.langchain.dev/llms-and-sql/&#34;&gt;&lt;strong&gt;blog&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href=&#34;https://blog.langchain.dev/incorporating-domain-specific-knowledge-in-sql-llm-solutions/&#34;&gt;&lt;strong&gt;blog&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Text-to-metadata-filter [7]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;A &lt;strong&gt;self-querying&lt;/strong&gt; retriever is one that, as the name suggests, has the  ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a &lt;strong&gt;structured query&lt;/strong&gt; and then applies that structured query to its underlying  VectorStore. This allows the retriever to not only use the user-input  query for semantic similarity comparison with the contents of stored  documents but to also &lt;strong&gt;extract filters from the user query on the  metadata of stored documents and to execute those filters&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(综述)RAG &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAG/</link>
      <pubDate>Wed, 02 Nov 2022 09:57:59 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAG/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;rag综述&#34;&gt;&#xA;  RAG综述&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag%e7%bb%bc%e8%bf%b0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/RAG-108bfe21108480be9c7ee46ff02a1ad6?pvs=4&#34;&gt;(综述)RAG&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(框架)RAG Langchain-Chatchat</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/framework/RAGchatchat/</link>
      <pubDate>Wed, 31 May 2023 11:31:35 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/framework/RAGchatchat/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;langchain-chatchat-架构&#34;&gt;&#xA;  Langchain-Chatchat 架构&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#langchain-chatchat-%e6%9e%b6%e6%9e%84&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/langchain&amp;#43;chatglm.jpg&#34; alt=&#34;langchain&amp;#43;chatglm&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;组件&#xA;&lt;ul&gt;&#xA;&lt;li&gt;本地知识库&lt;/li&gt;&#xA;&lt;li&gt;Embedding 模型&lt;/li&gt;&#xA;&lt;li&gt;向量数据库&lt;/li&gt;&#xA;&lt;li&gt;Prompt Template&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;langchain-chatchat&#34;&gt;&#xA;  Langchain-Chatchat&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#langchain-chatchat&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;部署&#xA;&lt;ul&gt;&#xA;&lt;li&gt;windows 10 [5]&#xA;部署本地， 没显存，卡&lt;/li&gt;&#xA;&lt;li&gt;Linux [2]&#xA;部署   32C125G ，没显存， 推理很慢&lt;/li&gt;&#xA;&lt;li&gt;Docker&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/chatchat-space/Langchain-Chatchat&#34;&gt;Langchain-Chatchat &lt;/a&gt; master&#xA;Langchain 与 ChatGLM 等语言模型的本地知识库问答&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/chatchat-space/Langchain-Chatchat/tree/v0.2.4&#34;&gt;Langchain-Chatchat&lt;/a&gt;  v0.2.4&lt;br&gt;&#xA;&lt;a href=&#34;https://gitee.com/deepeye/langchain-ChatGLM&#34;&gt;langchain-ChatGLM&lt;/a&gt;  gitee&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/www6v/Langchain-Chatchat-Colab&#34;&gt;Colab for Langchain-Chatchat&lt;/a&gt;   linux 可以部署  v0.2.6&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/649055955&#34;&gt;langChain-ChatGLM 尝试，踩坑记录&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/651189680&#34;&gt;Langchain-Chatchat + 阿里通义千问Qwen 保姆级教程 | 次世代知识管理解决方案&lt;/a&gt;    Langchain-Chatchat + 通义千问&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_43094965/article/details/133044128&#34;&gt;win10 安装 Langchain-Chatchat 避坑指南（2023年9月18日v0.2.4版本，包含全部下载内容！）&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(原理|实战)RAG Fusion</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPostRetrieval/RAGFusion/</link>
      <pubDate>Sun, 14 May 2023 18:23:41 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPostRetrieval/RAGFusion/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h3 id=&#34;rag-fusion多查询结果融合策略&#34;&gt;&#xA;  RAG-Fusion多查询结果融合策略&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag-fusion%e5%a4%9a%e6%9f%a5%e8%af%a2%e7%bb%93%e6%9e%9c%e8%9e%8d%e5%90%88%e7%ad%96%e7%95%a5&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;将多个召回查询的结果进行&lt;strong&gt;合并&lt;/strong&gt;[3]&lt;/p&gt;&#xA;&lt;p&gt;其思想在于通过生成多个用户查询和重新排序结果来解决RAG固有的约束；利用倒数排序融合（RRF）和自定义向量评分加权，生成全面准确的结果。[2]&lt;/p&gt;&#xA;&lt;h3 id=&#34;代码&#34;&gt;&#xA;  代码&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bb%a3%e7%a0%81&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_fusion.ipynb&#34;&gt;RAG Fusion&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1&#34;&gt;Forget RAG, the Future is RAG-Fusion&lt;/a&gt;  失效&#xA;&lt;a href=&#34;https://mp.weixin.qq.com/s/N7HgjsqgCVf2i-xy05qZtA&#34;&gt;使用RAG-Fusion和RRF让RAG在意图搜索方面更进一步&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/NFjn8pUsQaSx85nhBphORA&#34;&gt;再谈大模型RAG问答中的三个现实问题：兼看RAG-Fusion多query融合策略、回答引文生成策略及相关数据集概述&lt;/a&gt; &lt;br&gt;&#xA;二、基于大模型生成能力自动生成引文&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/pK2BRLrWpEKKIPFhUtGvcg&#34;&gt;一文详看Langchain框架中的RAG多阶段优化策略：从问题转换到查询路由再到生成优化&lt;/a&gt; ***   原理paper，代码示例&lt;br&gt;&#xA;Multi Query多查询策略， Decomposition问题， [RAG-Fusion]， Step Back， HyDE混合&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/langchain-ai/rag-from-scratch&#34;&gt;rag-from-scratch Repo&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(原理|实战)Query Routing</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPreRetrieval/RAGRouting/</link>
      <pubDate>Sun, 14 May 2023 16:28:20 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPreRetrieval/RAGRouting/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;类型1&#34;&gt;&#xA;  类型[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%b1%bb%e5%9e%8b1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;LLM Routers&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LLM Completion Routers&lt;/li&gt;&#xA;&lt;li&gt;LLM Function Calling Routers&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Semantic Routers&lt;/strong&gt; [2]&lt;/li&gt;&#xA;&lt;li&gt;Zero Shot Classification Routers&lt;/li&gt;&#xA;&lt;li&gt;Language Classification Routers&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/overview.png&#34; alt=&#34;overview&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;logical-and-semantic-routing3&#34;&gt;&#xA;  Logical and Semantic routing[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#logical-and-semantic-routing3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;logical-routing&#34;&gt;&#xA;  Logical routing&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#logical-routing&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/logical-router.png&#34; alt=&#34;Logical routing&#34; /&gt;&lt;/p&gt;&#xA;&lt;details &gt;&lt;summary&gt;code&lt;/summary&gt;&#xA;  &lt;div class=&#34;markdown-inner&#34;&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; typing &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Literal&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langchain_core.prompts &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; ChatPromptTemplate&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langchain_core.pydantic_v1 &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; BaseModel, Field&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langchain_openai &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; ChatOpenAI&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Data model&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;RouteQuery&lt;/span&gt;(BaseModel):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Route a user query to the most relevant datasource.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    datasource: Literal[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;python_docs&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;js_docs&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;golang_docs&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Field(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        description&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Given a user question choose which datasource would be most relevant for answering their question&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    )&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# LLM with function call &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;llm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ChatOpenAI(model&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;gpt-3.5-turbo-0125&amp;#34;&lt;/span&gt;, temperature&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;structured_llm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;with_structured_output(RouteQuery)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Prompt &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;system &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;You are an expert at routing a user question to the appropriate data source.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Based on the programming language the question is referring to, route it to the relevant data source.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;prompt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ChatPromptTemplate&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_messages(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;system&amp;#34;&lt;/span&gt;, system),&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;human&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{question}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;),&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Define router &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;router &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; prompt &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; structured_llm&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;choose_route&lt;/span&gt;(result):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;python_docs&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;datasource&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lower():&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;### Logic here &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;chain for python_docs&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;js_docs&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;datasource&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lower():&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;### Logic here &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;chain for js_docs&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;### Logic here &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;golang_docs&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; langchain_core.runnables &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; RunnableLambda&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;full_chain &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; router &lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; RunnableLambda(choose_route)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;full_chain&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;invoke({&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;question&amp;#34;&lt;/span&gt;: question})&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;&#xA;&lt;/details&gt;&#xA;&lt;h3 id=&#34;semantic-routing&#34;&gt;&#xA;  Semantic routing&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#semantic-routing&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/semantic-router.png&#34; alt=&#34;Semantic routing&#34; /&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Modular RAG &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAGModularRAG/</link>
      <pubDate>Fri, 21 Apr 2023 19:22:24 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAGModularRAG/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;modular-rag&#34;&gt;&#xA;  Modular RAG&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#modular-rag&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Modular-RAG-108bfe21108480468c35c5b45d991778?pvs=4&#34;&gt;(原理)Modular RAG&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)RAG Baichuan案例</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E6%A1%88%E4%BE%8B/RAGBaichuan/</link>
      <pubDate>Tue, 18 Apr 2023 14:26:22 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E6%A1%88%E4%BE%8B/RAGBaichuan/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;baichuan-rag1&#34;&gt;&#xA;  Baichuan RAG[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#baichuan-rag1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;借鉴了Meta的CoVe技术&lt;/li&gt;&#xA;&lt;li&gt;自研的TSF（Think-Step Further)技术&#xA;猜测其本质应该是对Step-back prompting方法的改良&lt;/li&gt;&#xA;&lt;li&gt;自研了Baichuan-Text-Embedding向量模型&lt;/li&gt;&#xA;&lt;li&gt;混合检索&#xA;向量检索与稀疏检索并行的&lt;/li&gt;&#xA;&lt;li&gt;self-Critique&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;总结2&#34;&gt;&#xA;  总结[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%80%bb%e7%bb%932&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;**多轮问答等场景的召回和传统搜索引擎的召回分布还不太一样。**百川借助子问题检索效果更高的特点，对原始复杂问题进行拆解、拓展来解决复杂问题检索质量偏差的问题。&lt;/li&gt;&#xA;&lt;li&gt;**对于没见过的语料直接用向量检索的结果可能不太理想。**百川在大量语料上利用无监督方法训练embedding模型来优化效果。而行业大模型更倾向于私有的数据，要提升私有数据的训练效果还得继续在私有化数据上训练效果会更佳。&lt;/li&gt;&#xA;&lt;li&gt;**Query拓展 + 多路召回 + Rerank + self-Critique可能是现阶段比较好的一种RAG方式，但是其也会带来更多成本。**总体思路有点像ReAct[3]系列的进阶版本，其在搜索侧和答案修正侧都做了更多的一些工作来优化实际效果。其缺点是需要多次调用大模型，会带来额外的成本，真实线上是否采用这种策略还有待验证。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648407638&amp;amp;idx=1&amp;amp;sn=5c167b4a11bc483f5790ef1e0340d670&#34;&gt;大模型RAG问答行业最佳案例及微调、推理双阶段实现模式：基于模块化(Modular)RAG自定义RAG Flow&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/675770700&#34;&gt;百川智能RAG方案总结：搜索出生的百川智能大模型RAG爬坑之路&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/658469464&#34;&gt;LLM/百川Baichuan2-53B搜索增强-开放API&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;amp;mid=2650901201&amp;amp;idx=1&amp;amp;sn=3a9bd61403fb4b024ec5d8c128990495&#34;&gt;大模型+搜索构建完整技术栈，百川智能用搜索增强给企业定制化下了一剂「猛药」&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://blog.csdn.net/qq_27590277/article/details/135421245&#34;&gt;百川智能RAG方案总结：搜索出生的百川智能大模型RAG爬坑之路&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战)多模态 RAG</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/Multimodal-RAG/RAGMultimodalPractice/</link>
      <pubDate>Tue, 14 Mar 2023 13:55:59 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/Multimodal-RAG/RAGMultimodalPractice/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;多模态rag-多向量检索器-1011&#34;&gt;&#xA;  多模态RAG-多向量检索器 [10][11]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%9a%e6%a8%a1%e6%80%81rag-%e5%a4%9a%e5%90%91%e9%87%8f%e6%a3%80%e7%b4%a2%e5%99%a8-1011&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;semi-structured-tables--text-rag-20&#34;&gt;&#xA;  semi-structured (tables + text) RAG [20]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#semi-structured-tables--text-rag-20&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/1.png&#34; alt=&#34;1.png&#34; /&gt;&#xA;分析pdf中表格&lt;/p&gt;&#xA;&lt;h3 id=&#34;multi-modal-text--tables--images-rag--13&#34;&gt;&#xA;  multi-modal (text + tables + images) RAG  [13]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#multi-modal-text--tables--images-rag--13&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/2.png&#34; alt=&#34;2.png&#34; /&gt;&#xA;分析PDF中图片&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Option 1&lt;/strong&gt;  [基于CLIP] [23]   &lt;em&gt;[30][32][33]&lt;/em&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Use multimodal embeddings &lt;strong&gt;(such as &lt;a href=&#34;https://openai.com/research/clip&#34;&gt;CLIP&lt;/a&gt;)&lt;/strong&gt; to embed images and text&lt;/li&gt;&#xA;&lt;li&gt;Retrieve both using similarity search&lt;/li&gt;&#xA;&lt;li&gt;Pass &lt;strong&gt;raw images and text chunks&lt;/strong&gt; to a multimodal LLM for answer synthesis&lt;br&gt;&#xA;{选项1：对文本和表格生成summary，然后应用多模态embedding模型把文本/表格summary、原始图片转化成embedding存入多向量检索器。对话时，根据query召回原始文本/表格/图像。然后将其喂给多模态LLM生成应答结果。}[10]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Option 2&lt;/strong&gt;   [21]&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Use a multimodal LLM (such as &lt;a href=&#34;https://openai.com/research/gpt-4v-system-card&#34;&gt;GPT4-V&lt;/a&gt;, &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;LLaVA&lt;/a&gt;, or &lt;a href=&#34;https://www.adept.ai/blog/fuyu-8b&#34;&gt;FUYU-8b&lt;/a&gt;) to produce &lt;strong&gt;text summaries from images&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Embed and retrieve text&lt;/li&gt;&#xA;&lt;li&gt;Pass text chunks to an LLM for answer synthesis&lt;br&gt;&#xA;【将图片转成摘要，和其他文本信息整合在文本粒度进行检索】[12]&lt;br&gt;&#xA;{选项2：首先应用多模态大模型（GPT4-V、LLaVA、FUYU-8b）生成图片summary。然后对文本/表格/图片summary进行向量化存入多向量检索器中。当生成应答的多模态大模型不具备时，可根据query召回原始文本/表格+图片summary。}[10]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Option 3 [24]    &lt;em&gt;[31][34]&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)Self-Reflective RAG</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/Agentic-RAG/RAGSelfReflective/</link>
      <pubDate>Thu, 02 Mar 2023 17:12:22 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/Agentic-RAG/RAGSelfReflective/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;cognitive-architecture-2&#34;&gt;&#xA;  Cognitive Architecture [2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#cognitive-architecture-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Cognitive architectures for RAG [1]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;crag&#34;&gt;&#xA;  CRAG&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#crag&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.15884.pdf&#34;&gt;Corrective Retrieval Augmented Generation&lt;/a&gt; Figure 2&lt;/p&gt;&#xA;&lt;h3 id=&#34;实现10&#34;&gt;&#xA;  实现[10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e7%8e%b010&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Corrective-RAG (CRAG) is a strategy for RAG that incorporates &lt;strong&gt;self-reflection / self-grading&lt;/strong&gt; on retrieved documents.&lt;/p&gt;&#xA;&lt;p&gt;In the paper here, a few steps are taken:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If at least one document exceeds the threshold for relevance, then it proceeds to generation&lt;/li&gt;&#xA;&lt;li&gt;Before generation, it performs knowledge refinement&lt;/li&gt;&#xA;&lt;li&gt;This partitions the document into &amp;ldquo;knowledge strips&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;It grades each strip, and filters our irrelevant ones&lt;/li&gt;&#xA;&lt;li&gt;If all documents fall below the relevance threshold or if the grader is unsure, then the framework seeks an additional datasource&lt;/li&gt;&#xA;&lt;li&gt;It will use web search to supplement retrieval&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;We will implement some of these ideas from scratch using LangGraph:&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战)RAG</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/RAGPractice/</link>
      <pubDate>Sat, 31 Dec 2022 07:42:52 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/RAGPractice/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;data-processing17&#34;&gt;&#xA;  Data processing[17]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#data-processing17&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;长文本   变成   QA pair&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;规则匹配&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;利用LLM抽取&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;人工处理&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;医疗问答rag20&#34;&gt;&#xA;  医疗问答RAG[20]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8c%bb%e7%96%97%e9%97%ae%e7%ad%94rag20&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;架构&#34;&gt;&#xA;  架构&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9e%b6%e6%9e%84&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/arch.JPG&#34; alt=&#34;arch&#34; /&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;chuck&#34;&gt;&#xA;  chuck&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#chuck&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;段落&lt;/strong&gt;&#xA;句子&#xA;token&lt;/p&gt;&#xA;&lt;h3 id=&#34;数据格式&#34;&gt;&#xA;  数据格式&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%95%b0%e6%8d%ae%e6%a0%bc%e5%bc%8f&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;{&amp;ldquo;id&amp;rdquo;: xxx, &amp;ldquo;病情描述&amp;rdquo;: &amp;ldquo;xxx&amp;rdquo;,  &amp;ldquo;治疗方案&amp;rdquo;: &amp;ldquo;xxx&amp;rdquo; }&lt;/p&gt;&#xA;&lt;h3 id=&#34;改写query&#34;&gt;&#xA;  改写query&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%94%b9%e5%86%99query&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;HyDE&lt;/li&gt;&#xA;&lt;li&gt;RAG Fusion -&amp;gt; Generate Similar query&#xA;用户的查询不精准，要扩充query, 用大模型改写&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;召回模型&#34;&gt;&#xA;  召回模型&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%ac%e5%9b%9e%e6%a8%a1%e5%9e%8b&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;bert模型&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;sbert&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;2个bert模型&lt;/strong&gt;，共享参数，s1,s2向量化后做&lt;strong&gt;相似度&lt;/strong&gt;计算&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;速度快&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;相似度&#xA;欧式距离&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;在百万语料上训练&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;语料格式&lt;/strong&gt;&lt;br&gt;&#xA;[s1][s2] 0 - 无关&#xA;[s1][s2] 1-类似&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;根据query, 召回id和value整条记录&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;排序模型&#34;&gt;&#xA;  排序模型&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%8e%92%e5%ba%8f%e6%a8%a1%e5%9e%8b&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;bert模型&#xA;&lt;ul&gt;&#xA;&lt;li&gt;1个bert模型&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;速度慢&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;格式&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;query[sep]s2  -&amp;gt; 经过softmax，产生2分类，0-1&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;也要训练&#xA;&lt;ul&gt;&#xA;&lt;li&gt;同&lt;strong&gt;召回模型训练方式&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;索引方式&#34;&gt;&#xA;  索引方式&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%b4%a2%e5%bc%95%e6%96%b9%e5%bc%8f&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;树索引&lt;/li&gt;&#xA;&lt;li&gt;知识图谱的索引&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;大模型&#34;&gt;&#xA;  大模型&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%a7%e6%a8%a1%e5%9e%8b&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;综合归纳的作用&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;xxx&#34;&gt;&#xA;  xxx&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#xxx&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;17&#34;&gt;&#xA;&lt;li&gt;&amp;laquo;大模型结合 RAG 构建客服场景自动问答系统&amp;raquo;  NVIDIA大模型日系列活动&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;医疗问答&#34;&gt;&#xA;  医疗问答&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8c%bb%e7%96%97%e9%97%ae%e7%ad%94&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;20&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1fW421P7u6?p=5&#34;&gt;基于百万语料的医疗RAG项目&lt;/a&gt; v&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(框架) Qanything</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/framework/RAGQanything/</link>
      <pubDate>Mon, 19 Jun 2023 10:14:53 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/framework/RAGQanything/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;qanything&#34;&gt;&#xA;  QAnything&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#qanything&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;arch1&#34;&gt;&#xA;  Arch[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#arch1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/netease-youdao/QAnything/raw/master/docs/images/qanything_arch.png&#34; alt=&#34;Arch&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;索引（indexing）&#xA;通过Embedding为每一个文本块生成一个向量表示，用于计算&lt;strong&gt;文本向量&lt;/strong&gt;和&lt;strong&gt;问题向量&lt;/strong&gt;之间的&lt;strong&gt;相似度&lt;/strong&gt;。创建索引将原始文本块和Embedding向量以键值对的形式存储，以便将来进行快速和频繁的搜索。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;检索（Retrieval）&#xA;使用Embedding模型将用户输入问题转换为向量，计算问题的Embedding向量和语料库中文本块Embedding向量之间的相似度，选择&lt;strong&gt;相似度最高的前K个文档块&lt;/strong&gt;作为当前问题的增强上下文信息。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;生成（Generation）&#xA;将检索得到的前K个文本块和用户问题一起送进大模型，让大模型基于给定的文本块来回答用户的问题。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;1st-retrievalembedding1&#34;&gt;&#xA;  1st Retrieval（embedding）[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#1st-retrievalembedding1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Bcembedding模型 [3]&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;中英双语和跨语种能力&lt;/li&gt;&#xA;&lt;li&gt;多领域覆盖&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Embedding 可以给出一个得分，但是这个得分描述的更多的是&lt;strong&gt;相似性&lt;/strong&gt;。Embedding本质上是一个&lt;strong&gt;双编码器&lt;/strong&gt;，两个文本在模型内部没有任何信息交互。只在最后计算两个向量的余弦相似度时才进行唯一一次交互。所以Embedding检索只能把&lt;strong&gt;最相似的&lt;/strong&gt;文本片段给你，&lt;strong&gt;没有&lt;/strong&gt;能力来判断候选文本和query之间的&lt;strong&gt;相关性&lt;/strong&gt;。但是&lt;strong&gt;相似又不等于相关&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;【embedding -&amp;gt; 相似性】&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/embedding.png&#34; alt=&#34;embedding&#34; /&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;2nd-retrievalrerank1&#34;&gt;&#xA;  2nd Retrieval（rerank）[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#2nd-retrievalrerank1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Rerank [3]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Rerank本质是一个&lt;strong&gt;Cross-Encoder&lt;/strong&gt;的模型。Cross-Encoder能让两个文本片段一开始就在BERT模型各层中通过self-attention进行交互。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/reranker.png&#34; alt=&#34;reranker&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;【rerank -&amp;gt; 相关性】&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;qanything-1&#34;&gt;&#xA;  QAnything&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#qanything-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything&#34;&gt;QAnything Repo&lt;/a&gt; git&lt;/li&gt;&#xA;&lt;li&gt;xxx&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1HF4m1w7rY/&#34;&gt;有道QAnything背后的故事：关于RAG的一点经验分享&lt;/a&gt; V&lt;br&gt;&#xA;&lt;a href=&#34;https://mp.weixin.qq.com/s/FUex1Q984-IhQ-FoLZTf5Q&#34;&gt;有道QAnything背后的故事&amp;mdash;关于RAG的一点经验分享&lt;/a&gt;   文字版&lt;br&gt;&#xA;[公众号有其他文章]&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&amp;amp;mid=2247489671&amp;amp;idx=1&amp;amp;sn=564a232c3c7919c70a7a1cf5efa77628&#34;&gt;前沿重器[45] RAG开源项目Qanything源码阅读1-概述+服务&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Work)RAG 故障点 &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/RAGFailure/</link>
      <pubDate>Tue, 09 May 2023 18:28:50 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/RAGFailure/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;rag-故障点&#34;&gt;&#xA;  RAG 故障点&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag-%e6%95%85%e9%9a%9c%e7%82%b9&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/RAG-1e2bfe21108480e4a0c7ec8ece4f18da?pvs=4&#34;&gt;RAG 故障点&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG 优化 *</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/RAGOptimize/</link>
      <pubDate>Tue, 09 May 2023 18:28:50 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/RAGOptimize/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;朴素rag-embedding&#34;&gt;&#xA;  朴素RAG Embedding&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9c%b4%e7%b4%a0rag-embedding&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;embedding-召回方案及局限性1&#34;&gt;&#xA;  Embedding 召回方案及局限性[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#embedding-%e5%8f%ac%e5%9b%9e%e6%96%b9%e6%a1%88%e5%8f%8a%e5%b1%80%e9%99%90%e6%80%a71&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;召回&lt;strong&gt;精度低&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;粒度过粗&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;不支持条件查询/统计&lt;/li&gt;&#xA;&lt;li&gt;不能替代信息提取&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;解决方案&#34;&gt;&#xA;  解决方案&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;问题理解——准确识别&lt;strong&gt;用户意图&lt;/strong&gt;(传统NLP)  [2]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;基于&lt;strong&gt;关键词Embedding&lt;/strong&gt;的入库和搜索 [2]&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;关键词提取&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;实现信息抽取（Information Extraction，IE）&#xA;&lt;ul&gt;&#xA;&lt;li&gt;实体关系三元组抽取(RE, Relation Extraction )&lt;/li&gt;&#xA;&lt;li&gt;命名实体识别(NER, Name-Entity Recognition)&lt;/li&gt;&#xA;&lt;li&gt;事件抽取(EE, Event Extraction)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;基于 LLM 提取 [不推荐]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;结果不准确、开销也大&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;传统 NLP 方法提取&lt;/strong&gt;[推荐]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;名词短语提取与整合&lt;/li&gt;&#xA;&lt;li&gt;依存分析&lt;/li&gt;&#xA;&lt;li&gt;成分句法分析&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;总结&#xA;从&lt;strong&gt;完整语句的 Embedding&lt;/strong&gt;，切换为&lt;strong&gt;关键词 Embedding&lt;/strong&gt;：&lt;/li&gt;&#xA;&lt;li&gt;优势&#xA;&lt;ul&gt;&#xA;&lt;li&gt;相比传统 Embedding，大幅提升&lt;strong&gt;召回精准度&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;使用传统 NLP 在专项问题处理上，相比 LLM 提供更好的精度和性能。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;知识库存储选型&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Vector Store&#xA;&lt;ul&gt;&#xA;&lt;li&gt;分片:  区分&lt;strong&gt;层级结构&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Relational Database&lt;/li&gt;&#xA;&lt;li&gt;Graph Database&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;图数据检索&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;行业问答3&#34;&gt;&#xA;  行业问答[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%a1%8c%e4%b8%9a%e9%97%ae%e7%ad%943&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;挑战&#34;&gt;&#xA;  挑战&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%8c%91%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;版面复杂多样&lt;/li&gt;&#xA;&lt;li&gt;文本分块&#xA;&lt;strong&gt;存在知识点被分割、不完整的情况&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;多因素影响内容召回效果&#xA;&lt;ul&gt;&#xA;&lt;li&gt;例如：文档内容相似度高(专业文档细分领域、版本迭代等)；&lt;/li&gt;&#xA;&lt;li&gt;通用的&lt;strong&gt;向量相似度算法&lt;/strong&gt;效果不好(问题与问题匹配 VS问题与答案匹配)；&lt;/li&gt;&#xA;&lt;li&gt;召回率受文档库增大而降低&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;优化&#34;&gt;&#xA;  优化&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bc%98%e5%8c%96&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;向量化上的优化&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Advanced RAG &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAGPerformance/</link>
      <pubDate>Wed, 07 Dec 2022 09:44:24 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAGPerformance/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;advanced-rag&#34;&gt;&#xA;  Advanced RAG&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#advanced-rag&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Advanced-RAG-108bfe2110848030b150e8c09baa7232?pvs=4&#34;&gt;(原理)Advanced RAG&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(框架)RAGflow &#43;</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/framework/RAGRAGflow/</link>
      <pubDate>Mon, 19 Jun 2023 10:25:33 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/%E5%AE%9E%E6%88%98/framework/RAGRAGflow/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;ragflow&#34;&gt;&#xA;  RAGflow&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#ragflow&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/RagFlow-1b5bfe21108480618819d7abf6c7704f?pvs=4&#34;&gt;RAGflow&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG 评估</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAGEval/</link>
      <pubDate>Wed, 07 Jun 2023 17:02:42 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAGEval/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://www.bilibili.com/video/BV1Jz421Q7Lw/&#34;&gt;如何利用RAGAs评估RAG系统的好坏&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/blackinkkkxi/RAG_langchain/blob/main/learn/evaluation/RAGAS-langchian.ipynb&#34;&gt;使用LangChain和RAGAS对RAG系统进行自动有效评估&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://www.bilibili.com/video/BV1aZ421W7DB/&#34;&gt;一次搞懂RAG评估，三个角度LangChain，LlamaIndex，RAGAS&lt;/a&gt;&lt;br&gt;&#xA;&lt;a href=&#34;https://techdiylife.github.io/blog/blog.html?category1=c02&amp;amp;blogid=0053&#34;&gt;RAG评估资料大全 &lt;/a&gt;&lt;br&gt;&#xA;RAG评估指标：两种视角理解评估指标&lt;br&gt;&#xA;&lt;a href=&#34;https://docs.smith.langchain.com/old/cookbook/testing-examples/rag_eval&#34;&gt;RAG Evaluation&lt;/a&gt;&lt;br&gt;&#xA;&lt;a href=&#34;https://docs.smith.langchain.com/old/cookbook/testing-examples/ragas&#34;&gt;RAG evaluation with RAGAS&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648404511&amp;amp;idx=2&amp;amp;sn=fefb78c1d920cb5b437f2e3da9935637&#34;&gt;再看大模型RAG检索增强如何评估：RAGAS开源自动化评估框架&lt;/a&gt;&lt;br&gt;&#xA;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648404476&amp;amp;idx=2&amp;amp;sn=d07b27dc9162ab0aaec3108004e4cfbe&#34;&gt;大模型RAG检索增强问答如何评估：噪声、拒答、反事实、信息整合四大能力评测任务探索 &lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)RAG Rerank</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPostRetrieval/RAGRerank/</link>
      <pubDate>Sun, 14 May 2023 18:23:41 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/phase/phaseRAGPostRetrieval/RAGRerank/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;reranker-22&#34;&gt;&#xA;  Reranker [22]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#reranker-22&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;A reranking model — also known as a &lt;strong&gt;cross-encoder&lt;/strong&gt; — is a type of model that,&lt;strong&gt;given a query and document pair, will output a similarity score.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f0d2f75571bb58eecf2520a23d300a5fc5b1e2c-2440x1100.png&amp;amp;w=3840&amp;amp;q=65&#34; alt=&#34;Rerankers&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;产品&#34;&gt;&#xA;  产品&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%ba%a7%e5%93%81&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;bge-ranker-20&#34;&gt;&#xA;  BGE Ranker [20]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#bge-ranker-20&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;交叉编码器&lt;/strong&gt;将对查询和答案实时计算相关性分数，这比**向量模型(即双编码器)**更准确，但比向量模型更耗时。 因此，它可以用来对嵌入模型返回的前k个文档重新排序。 我们在多语言数据上训练了交叉编码器，数据格式与向量模型相同，因此您可以根据我们的示例 轻松地对其进行微调。&lt;/p&gt;&#xA;&lt;h3 id=&#34;bce24&#34;&gt;&#xA;  BCE[24]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#bce24&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;中文效果比BGE好[老刘说nlp]&lt;/p&gt;&#xA;&lt;h3 id=&#34;优秀的组合-21&#34;&gt;&#xA;  优秀的组合 [21]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%bc%98%e7%a7%80%e7%9a%84%e7%bb%84%e5%90%88-21&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;OpenAI + CohereRerank&#xA;Voyage + big-reranker-large&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol start=&#34;20&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md&#34;&gt;BGE Reranker&lt;/a&gt;&#xA;&lt;a href=&#34;https://www.bilibili.com/video/BV1sQ4y137Ft/&#34;&gt;transformers二次开发——bge-reranker模型微调流程&lt;/a&gt; V&#xA;&lt;a href=&#34;https://mp.weixin.qq.com/s/XnkQFCdbvjox1Y06IbIlYw&#34;&gt;RAG 再添新利器！智源开源最强检索排序模型 BGE Re-Ranker v2.0 &lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG KG</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/KG-RAG/RAGKG/</link>
      <pubDate>Mon, 19 Jun 2023 14:24:23 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Pattern/KG-RAG/RAGKG/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h3 id=&#34;vectorkg-rag1516&#34;&gt;&#xA;  Vector+KG RAG[15][16]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#vectorkg-rag1516&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h3 id=&#34;rag-多跳问题&#34;&gt;&#xA;  RAG 多跳问题&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag-%e5%a4%9a%e8%b7%b3%e9%97%ae%e9%a2%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;rag-多跳问题-1&#34;&gt;&#xA;  RAG 多跳问题&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag-%e5%a4%9a%e8%b7%b3%e9%97%ae%e9%a2%98-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://neo4j.com/developer-blog/knowledge-graphs-llms-multi-hop-question-answering/&#34;&gt;Knowledge Graphs &amp;amp; LLMs: Multi-Hop Question Answering&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://cloud.tencent.com.cn/developer/article/2409038&#34;&gt;知识图谱和 LLM：多跳问答-腾讯云开发者社区-腾讯云&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/qq_41185868/article/details/138514051&#34;&gt;LLMs之KG-RAG：KG-RAG/GraphRAG(基于知识图谱的RAG系统)的简介(可以解决多跳问题/同时支持结构化和非结构化数据查询)、经验技巧、案例应用之详细攻略-CSDN博客&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://blog.csdn.net/qq_36931982/article/details/139118215&#34;&gt;MultiHop-RAG：多跳查询的基准检索增强生成_rag多跳查询-CSDN博客&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;llmkg--知识图谱&#34;&gt;&#xA;  LLM+KG  知识图谱&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llmkg--%e7%9f%a5%e8%af%86%e5%9b%be%e8%b0%b1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;15&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://neo4j.com/developer-blog/unstructured-knowledge-graph-neo4j-langchain/&#34;&gt;Enhanced QA Integrating Unstructured Knowledge Graph Using Neo4j and LangChain&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.langchain.dev/using-a-knowledge-graph-to-implement-a-devops-rag-application/&#34;&gt;Using a Knowledge Graph to implement a DevOps RAG application&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s/buV1j4DtDiVavtGCJIsedQ&#34;&gt;大模型辅助图谱构建的4个策略对比：兼看大模型与知识图谱结合的3个综述 &lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG vs SFT</title>
      <link>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAGSFT/</link>
      <pubDate>Wed, 07 Jun 2023 17:02:42 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAIGC/docs/RAG/Overview/RAGSFT/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;rag-vs-ft1&#34;&gt;&#xA;  RAG vs FT[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag-vs-ft1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;rag-vs-微调场景&#34;&gt;&#xA;  RAG vs 微调[场景]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag-vs-%e5%be%ae%e8%b0%83%e5%9c%ba%e6%99%af&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;动态数据：RAG&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;模型能力定制：微调&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;幻觉：RAG &amp;gt; 微调&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;可解释性：RAG&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;成本：RAG&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;依赖通用能力：RAG&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;​       微调会有灾难性的遗忘&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;延迟：微调&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;​       rag的流程长&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;智能设备：微调&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;【rag和微调可以一起使用】&lt;/p&gt;&#xA;&lt;h3 id=&#34;应用-case&#34;&gt;&#xA;  应用 Case&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ba%94%e7%94%a8-case&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;A: 投资理财规划师   &lt;strong&gt;[用RAG]&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;处理动态数据：RAG&lt;/li&gt;&#xA;&lt;li&gt;很强的对话能力：RAG&lt;/li&gt;&#xA;&lt;li&gt;金融能力：微调 ×&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;B: 金融信息抽取Bot  &lt;strong&gt;[用微调]&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;很强的抽取能力：微调  [特定的能力]&lt;/li&gt;&#xA;&lt;li&gt;金融能力&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;C: 销售机器人   &lt;strong&gt;[用RAG+微调]&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;多轮对话/动态：RAG&lt;/li&gt;&#xA;&lt;li&gt;销售技巧/语气：微调&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;rag-vs-ft-2&#34;&gt;&#xA;  RAG vs FT [2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rag-vs-ft-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/rag-vs-ft.jpg&#34; alt=&#34;rag-vs-ft.jpg&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;todo:  有中文翻译的图片&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1xM4m117FP/&#34;&gt;大模型项目选择RAG还是微调：三个案例&lt;/a&gt;  v&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1vJ4m1M7qG/&#34;&gt;大模型项目选择RAG还是微调：八个判断依据&lt;/a&gt; v&lt;/p&gt;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;#1《Retrieval-Augmented Generation for Large Language Models: A Survey》&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
